LayerName,LayerType,BenchmarkName,RealTime(ms),Theoretical Flops
batchnorm,BatchNorm,BATCHNORM_FWD_INFERENCE4496119707191152535<CUDNN_BATC...,3.901800,708837376
activation,Relu,ACTIVATION_FWD13811507302503856802<CUDNN_ACTIVATION_R...,4.192482,1063256064
batchnorm1,BatchNorm,BATCHNORM_FWD_INFERENCE7006336951657907319<CUDNN_BATC...,1.966776,354418688
activation1,Relu,ACTIVATION_FWD16209224632504742210<CUDNN_ACTIVATION_R...,1.989087,531628032
convolution2,Conv,CONV_FWD_013695261584197046314<CUDNN_CONVOLUTION_FWD_...,3.820225,51036291072
batchnorm2,BatchNorm,BATCHNORM_FWD_INFERENCE16838400792959797723<CUDNN_BAT...,0.945655,177209344
activation2,Relu,ACTIVATION_FWD7523049824865877230<CUDNN_ACTIVATION_RE...,0.973919,265814016
batchnorm3,BatchNorm,BATCHNORM_FWD_INFERENCE2272772328970393420<CUDNN_BATC...,0.453412,88604672
activation3,Relu,ACTIVATION_FWD11404165245588578937<CUDNN_ACTIVATION_R...,0.485841,132907008
batchnorm4,BatchNorm,BATCHNORM_FWD_INFERENCE5722518830201903470<CUDNN_BATC...,0.244080,44302336
activation4,Relu,ACTIVATION_FWD8096921656341651302<CUDNN_ACTIVATION_RE...,0.244321,66453504
batchnorm5,BatchNorm,BATCHNORM_FWD_INFERENCE309343917831893305<CUDNN_BATCH...,0.135997,22151168
activation5,Relu,ACTIVATION_FWD9620316750620623884<CUDNN_ACTIVATION_RE...,0.124845,33226752
batchnorm6,BatchNorm,BATCHNORM_FWD_INFERENCE10920519682909109743<CUDNN_BAT...,0.222808,37748736
activation6,Relu,ACTIVATION_FWD1604481517759940826<CUDNN_ACTIVATION_RE...,0.207995,56623104
batchnorm7,BatchNorm,BATCHNORM_FWD_INFERENCE10920519682909109743<CUDNN_BAT...,0.222808,37748736
activation7,Relu,ACTIVATION_FWD1604481517759940826<CUDNN_ACTIVATION_RE...,0.207995,56623104
,,Total,20.767816,59432435712
