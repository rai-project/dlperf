
namespace LAYER_CUDNN_CONV_FWD__5483414137840417464 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      3 /* Input1 */, \
      800 /* Input2 */, \
      800 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5483414137840417464(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 800} /* Input2 */, 
      {"input[3]", 800} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5483414137840417464(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5483414137840417464(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5483414137840417464(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5483414137840417464(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5483414137840417464(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5483414137840417464(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5483414137840417464);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5483414137840417464);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5483414137840417464);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__1450987090372482704 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      401 /* Input2 */, \
      401 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1450987090372482704(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 401} /* Input2 */, 
      {"input[3]", 401} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1450987090372482704(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1450987090372482704(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1450987090372482704(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1450987090372482704(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1450987090372482704(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1450987090372482704(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1450987090372482704);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1450987090372482704);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1450987090372482704);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__1281690589873307883 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      401 /* Input2 */, \
      401 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1281690589873307883(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 401} /* Input2 */, 
      {"input[3]", 401} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1281690589873307883(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1281690589873307883(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1281690589873307883(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1281690589873307883(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1281690589873307883(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1281690589873307883(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1281690589873307883);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1281690589873307883);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1281690589873307883);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12922391288831387533 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      401 /* Input2 */, \
      401 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12922391288831387533(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 401} /* Input2 */, 
      {"input[3]", 401} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12922391288831387533(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12922391288831387533(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12922391288831387533(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12922391288831387533(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12922391288831387533(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12922391288831387533(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12922391288831387533);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12922391288831387533);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12922391288831387533);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__5648513578061539754 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      128 /* Input1 */, \
      401 /* Input2 */, \
      401 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5648513578061539754(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 401} /* Input2 */, 
      {"input[3]", 401} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5648513578061539754(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5648513578061539754(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5648513578061539754(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5648513578061539754(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5648513578061539754(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5648513578061539754(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5648513578061539754);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5648513578061539754);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5648513578061539754);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__14987857242337335380 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      128 /* Input1 */, \
      401 /* Input2 */, \
      401 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__14987857242337335380(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 401} /* Input2 */, 
      {"input[3]", 401} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__14987857242337335380(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__14987857242337335380(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__14987857242337335380(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__14987857242337335380(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__14987857242337335380(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__14987857242337335380(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__14987857242337335380);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__14987857242337335380);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__14987857242337335380);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11672337044011801873 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      128 /* Input1 */, \
      200 /* Input2 */, \
      200 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11672337044011801873(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 200} /* Input2 */, 
      {"input[3]", 200} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11672337044011801873(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11672337044011801873(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11672337044011801873(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11672337044011801873(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11672337044011801873(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11672337044011801873(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11672337044011801873);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11672337044011801873);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11672337044011801873);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12937296503356461428 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      128 /* Input1 */, \
      200 /* Input2 */, \
      200 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12937296503356461428(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 200} /* Input2 */, 
      {"input[3]", 200} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12937296503356461428(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12937296503356461428(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12937296503356461428(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12937296503356461428(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12937296503356461428(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12937296503356461428(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12937296503356461428);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12937296503356461428);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12937296503356461428);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8590430140272131569 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      200 /* Input2 */, \
      200 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8590430140272131569(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 200} /* Input2 */, 
      {"input[3]", 200} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8590430140272131569(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8590430140272131569(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8590430140272131569(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8590430140272131569(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8590430140272131569(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8590430140272131569(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8590430140272131569);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8590430140272131569);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8590430140272131569);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__150914675167651632 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      200 /* Input2 */, \
      200 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__150914675167651632(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 200} /* Input2 */, 
      {"input[3]", 200} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__150914675167651632(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__150914675167651632(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__150914675167651632(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__150914675167651632(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__150914675167651632(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__150914675167651632(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__150914675167651632);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__150914675167651632);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__150914675167651632);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__572981656370721099 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      200 /* Input2 */, \
      200 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__572981656370721099(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 200} /* Input2 */, 
      {"input[3]", 200} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__572981656370721099(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__572981656370721099(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__572981656370721099(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__572981656370721099(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__572981656370721099(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__572981656370721099(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__572981656370721099);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__572981656370721099);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__572981656370721099);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__17014728314582939701 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      200 /* Input2 */, \
      200 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17014728314582939701(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 200} /* Input2 */, 
      {"input[3]", 200} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17014728314582939701(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17014728314582939701(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17014728314582939701(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17014728314582939701(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17014728314582939701(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17014728314582939701(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17014728314582939701);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17014728314582939701);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17014728314582939701);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__10273408007883520234 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      200 /* Input2 */, \
      200 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10273408007883520234(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 200} /* Input2 */, 
      {"input[3]", 200} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10273408007883520234(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10273408007883520234(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10273408007883520234(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10273408007883520234(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10273408007883520234(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10273408007883520234(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10273408007883520234);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10273408007883520234);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10273408007883520234);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2959315207773715624 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      200 /* Input2 */, \
      200 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2959315207773715624(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 200} /* Input2 */, 
      {"input[3]", 200} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2959315207773715624(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2959315207773715624(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2959315207773715624(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2959315207773715624(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2959315207773715624(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2959315207773715624(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2959315207773715624);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2959315207773715624);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2959315207773715624);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__9585747137689001534 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      200 /* Input2 */, \
      200 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9585747137689001534(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 200} /* Input2 */, 
      {"input[3]", 200} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9585747137689001534(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9585747137689001534(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9585747137689001534(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9585747137689001534(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9585747137689001534(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9585747137689001534(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9585747137689001534);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9585747137689001534);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9585747137689001534);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4200461661441999254 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      512 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4200461661441999254(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4200461661441999254(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4200461661441999254(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4200461661441999254(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4200461661441999254(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4200461661441999254(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4200461661441999254(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4200461661441999254);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4200461661441999254);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4200461661441999254);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__393687152679589448 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      128 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__393687152679589448(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__393687152679589448(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__393687152679589448(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__393687152679589448(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__393687152679589448(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__393687152679589448(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__393687152679589448(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__393687152679589448);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__393687152679589448);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__393687152679589448);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__12099585352111730990 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      128 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12099585352111730990(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12099585352111730990(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12099585352111730990(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12099585352111730990(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12099585352111730990(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12099585352111730990(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12099585352111730990(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12099585352111730990);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12099585352111730990);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12099585352111730990);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6138121043116353470 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      128 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6138121043116353470(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6138121043116353470(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6138121043116353470(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6138121043116353470(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6138121043116353470(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6138121043116353470(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6138121043116353470(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6138121043116353470);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6138121043116353470);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6138121043116353470);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10325219894246243566 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      512 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10325219894246243566(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10325219894246243566(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10325219894246243566(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10325219894246243566(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10325219894246243566(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10325219894246243566(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10325219894246243566(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10325219894246243566);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10325219894246243566);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10325219894246243566);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__16715485664703554457 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      512 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      1024 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16715485664703554457(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16715485664703554457(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16715485664703554457(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16715485664703554457(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16715485664703554457(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16715485664703554457(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16715485664703554457(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16715485664703554457);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16715485664703554457);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16715485664703554457);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__9105095390211712792 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      9 /* PadHeight */, \
      9 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      9 /* DilationWidth */, \
      9 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9105095390211712792(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 9} /* PadHeight */, 
      {"pad_width", 9} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 9} /* DilationWidth */, 
      {"dilation_width", 9} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9105095390211712792(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9105095390211712792(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9105095390211712792(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9105095390211712792(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9105095390211712792(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9105095390211712792(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9105095390211712792);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9105095390211712792);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9105095390211712792);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__7402795222581005264 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7402795222581005264(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7402795222581005264(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7402795222581005264(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7402795222581005264(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7402795222581005264(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7402795222581005264(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7402795222581005264(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7402795222581005264);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7402795222581005264);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7402795222581005264);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11031039547586859208 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      2 /* DilationWidth */, \
      2 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11031039547586859208(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 2} /* DilationWidth */, 
      {"dilation_width", 2} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11031039547586859208(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11031039547586859208(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11031039547586859208(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11031039547586859208(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11031039547586859208(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11031039547586859208(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11031039547586859208);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11031039547586859208);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11031039547586859208);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4696420654802048879 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      1024 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4696420654802048879(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4696420654802048879(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4696420654802048879(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4696420654802048879(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4696420654802048879(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4696420654802048879(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4696420654802048879(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4696420654802048879);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4696420654802048879);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4696420654802048879);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8903221941647550432 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      5 /* PadHeight */, \
      5 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      5 /* DilationWidth */, \
      5 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8903221941647550432(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 5} /* PadHeight */, 
      {"pad_width", 5} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 5} /* DilationWidth */, 
      {"dilation_width", 5} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8903221941647550432(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8903221941647550432(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8903221941647550432(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8903221941647550432(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8903221941647550432(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8903221941647550432(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8903221941647550432);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8903221941647550432);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8903221941647550432);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__8151922992267252174 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8151922992267252174(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8151922992267252174(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8151922992267252174(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8151922992267252174(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8151922992267252174(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8151922992267252174(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8151922992267252174(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8151922992267252174);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8151922992267252174);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8151922992267252174);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__17456371258936887239 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      1024 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17456371258936887239(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17456371258936887239(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17456371258936887239(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17456371258936887239(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17456371258936887239(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17456371258936887239(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17456371258936887239(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17456371258936887239);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17456371258936887239);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17456371258936887239);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__12321531049862307345 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      1024 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12321531049862307345(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12321531049862307345(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12321531049862307345(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12321531049862307345(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12321531049862307345(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12321531049862307345(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12321531049862307345(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12321531049862307345);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12321531049862307345);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12321531049862307345);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6357533878887008385 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      1024 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6357533878887008385(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6357533878887008385(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6357533878887008385(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6357533878887008385(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6357533878887008385(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6357533878887008385(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6357533878887008385(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6357533878887008385);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6357533878887008385);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6357533878887008385);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8584361682468920400 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      512 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      5 /* PadHeight */, \
      5 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      5 /* DilationWidth */, \
      5 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8584361682468920400(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 5} /* PadHeight */, 
      {"pad_width", 5} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 5} /* DilationWidth */, 
      {"dilation_width", 5} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8584361682468920400(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8584361682468920400(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8584361682468920400(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8584361682468920400(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8584361682468920400(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8584361682468920400(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8584361682468920400);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8584361682468920400);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8584361682468920400);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4916437551457201187 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      1024 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      2048 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4916437551457201187(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 2048} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4916437551457201187(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4916437551457201187(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4916437551457201187(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4916437551457201187(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4916437551457201187(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4916437551457201187(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4916437551457201187);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4916437551457201187);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4916437551457201187);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8283406541106240680 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      512 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      9 /* PadHeight */, \
      9 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      9 /* DilationWidth */, \
      9 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8283406541106240680(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 9} /* PadHeight */, 
      {"pad_width", 9} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 9} /* DilationWidth */, 
      {"dilation_width", 9} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8283406541106240680(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8283406541106240680(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8283406541106240680(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8283406541106240680(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8283406541106240680(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8283406541106240680(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8283406541106240680);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8283406541106240680);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8283406541106240680);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__9975130275254721858 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      2048 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9975130275254721858(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9975130275254721858(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9975130275254721858(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9975130275254721858(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9975130275254721858(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9975130275254721858(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9975130275254721858(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9975130275254721858);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9975130275254721858);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9975130275254721858);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__15590199560045644816 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      512 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      17 /* PadHeight */, \
      17 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      17 /* DilationWidth */, \
      17 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15590199560045644816(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 17} /* PadHeight */, 
      {"pad_width", 17} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 17} /* DilationWidth */, 
      {"dilation_width", 17} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15590199560045644816(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15590199560045644816(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15590199560045644816(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15590199560045644816(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15590199560045644816(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15590199560045644816(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15590199560045644816);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15590199560045644816);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15590199560045644816);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__7248924370962380176 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      512 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7248924370962380176(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7248924370962380176(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7248924370962380176(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7248924370962380176(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7248924370962380176(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7248924370962380176(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7248924370962380176(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7248924370962380176);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7248924370962380176);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7248924370962380176);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__9970183001732024226 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      512 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      2048 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9970183001732024226(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 2048} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9970183001732024226(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9970183001732024226(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9970183001732024226(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9970183001732024226(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9970183001732024226(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9970183001732024226(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9970183001732024226);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9970183001732024226);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9970183001732024226);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__8694876889654889426 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      2048 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8694876889654889426(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8694876889654889426(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8694876889654889426(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8694876889654889426(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8694876889654889426(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8694876889654889426(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8694876889654889426(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8694876889654889426);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8694876889654889426);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8694876889654889426);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__7266417581424125919 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      2048 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      304 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      6 /* PadHeight */, \
      6 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      6 /* DilationWidth */, \
      6 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7266417581424125919(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 304} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 6} /* PadHeight */, 
      {"pad_width", 6} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 6} /* DilationWidth */, 
      {"dilation_width", 6} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7266417581424125919(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7266417581424125919(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7266417581424125919(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7266417581424125919(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7266417581424125919(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7266417581424125919(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7266417581424125919);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7266417581424125919);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7266417581424125919);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6233775122211103303 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      2048 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      304 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      12 /* PadHeight */, \
      12 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      12 /* DilationWidth */, \
      12 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6233775122211103303(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 304} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 12} /* PadHeight */, 
      {"pad_width", 12} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 12} /* DilationWidth */, 
      {"dilation_width", 12} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6233775122211103303(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6233775122211103303(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6233775122211103303(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6233775122211103303(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6233775122211103303(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6233775122211103303(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6233775122211103303);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6233775122211103303);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6233775122211103303);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10365513986194798311 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      2048 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      304 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      18 /* PadHeight */, \
      18 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      18 /* DilationWidth */, \
      18 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10365513986194798311(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 304} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 18} /* PadHeight */, 
      {"pad_width", 18} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 18} /* DilationWidth */, 
      {"dilation_width", 18} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10365513986194798311(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10365513986194798311(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10365513986194798311(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10365513986194798311(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10365513986194798311(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10365513986194798311(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10365513986194798311);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10365513986194798311);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10365513986194798311);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__13073471456554292191 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      2048 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      304 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      24 /* PadHeight */, \
      24 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      24 /* DilationWidth */, \
      24 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13073471456554292191(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 304} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 24} /* PadHeight */, 
      {"pad_width", 24} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 24} /* DilationWidth */, 
      {"dilation_width", 24} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13073471456554292191(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13073471456554292191(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13073471456554292191(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13073471456554292191(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13073471456554292191(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13073471456554292191(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13073471456554292191);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13073471456554292191);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13073471456554292191);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__2804395488945963112 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      2048 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__2804395488945963112(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__2804395488945963112(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__2804395488945963112(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__2804395488945963112(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__2804395488945963112(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__2804395488945963112(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__2804395488945963112(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__2804395488945963112);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__2804395488945963112);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__2804395488945963112);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__18048485655459953979 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      57 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18048485655459953979(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__18048485655459953979(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18048485655459953979(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18048485655459953979(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18048485655459953979(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18048485655459953979(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18048485655459953979(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__18048485655459953979);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18048485655459953979);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18048485655459953979);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__278157200631412768 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__278157200631412768(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__278157200631412768(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__278157200631412768(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__278157200631412768(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__278157200631412768(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__278157200631412768(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__278157200631412768(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__278157200631412768);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__278157200631412768);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__278157200631412768);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11221556613953088006 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      57 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11221556613953088006(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11221556613953088006(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11221556613953088006(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11221556613953088006(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11221556613953088006(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11221556613953088006(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11221556613953088006(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11221556613953088006);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11221556613953088006);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11221556613953088006);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__18283792071765400025 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      192 /* Input1 */, \
      56 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18283792071765400025(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18283792071765400025(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18283792071765400025(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18283792071765400025(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18283792071765400025(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__18283792071765400025(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18283792071765400025(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18283792071765400025);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18283792071765400025);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__18283792071765400025);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__8944441227645970471 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      192 /* Input1 */, \
      56 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__8944441227645970471(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__8944441227645970471(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__8944441227645970471(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__8944441227645970471(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__8944441227645970471(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__8944441227645970471(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__8944441227645970471(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__8944441227645970471);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__8944441227645970471);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__8944441227645970471);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__9089984983275446371 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9089984983275446371(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9089984983275446371(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9089984983275446371(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9089984983275446371(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9089984983275446371(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9089984983275446371(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9089984983275446371(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9089984983275446371);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9089984983275446371);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9089984983275446371);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8978159188323800738 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8978159188323800738(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8978159188323800738(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8978159188323800738(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8978159188323800738(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8978159188323800738(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8978159188323800738(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8978159188323800738(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8978159188323800738);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8978159188323800738);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8978159188323800738);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__4722560631828075752 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      96 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4722560631828075752(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4722560631828075752(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4722560631828075752(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4722560631828075752(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4722560631828075752(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4722560631828075752(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4722560631828075752(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4722560631828075752);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4722560631828075752);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4722560631828075752);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__16363006243484398478 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      96 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16363006243484398478(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16363006243484398478(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16363006243484398478(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16363006243484398478(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16363006243484398478(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16363006243484398478(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16363006243484398478(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16363006243484398478);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16363006243484398478);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16363006243484398478);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__2758706421516135872 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      128 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2758706421516135872(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2758706421516135872(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2758706421516135872(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2758706421516135872(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2758706421516135872(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2758706421516135872(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2758706421516135872(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2758706421516135872);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2758706421516135872);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2758706421516135872);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14943752604183265983 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      16 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14943752604183265983(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14943752604183265983(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14943752604183265983(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14943752604183265983(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14943752604183265983(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14943752604183265983(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14943752604183265983(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14943752604183265983);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14943752604183265983);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14943752604183265983);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__3355524136780200957 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      16 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3355524136780200957(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3355524136780200957(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3355524136780200957(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3355524136780200957(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3355524136780200957(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3355524136780200957(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3355524136780200957(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3355524136780200957);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3355524136780200957);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3355524136780200957);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__18127569558886495492 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      16 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      32 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18127569558886495492(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__18127569558886495492(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18127569558886495492(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18127569558886495492(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18127569558886495492(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18127569558886495492(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18127569558886495492(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__18127569558886495492);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18127569558886495492);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18127569558886495492);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__441120040434963590 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__441120040434963590(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__441120040434963590(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__441120040434963590(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__441120040434963590(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__441120040434963590(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__441120040434963590(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__441120040434963590(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__441120040434963590);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__441120040434963590);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__441120040434963590);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14251938285816836589 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14251938285816836589(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14251938285816836589(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14251938285816836589(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14251938285816836589(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14251938285816836589(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14251938285816836589(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14251938285816836589(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14251938285816836589);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14251938285816836589);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14251938285816836589);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__16567457548810529467 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      32 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16567457548810529467(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16567457548810529467(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16567457548810529467(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16567457548810529467(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16567457548810529467(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16567457548810529467(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16567457548810529467(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16567457548810529467);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16567457548810529467);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16567457548810529467);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__1565702409220339304 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      128 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1565702409220339304(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1565702409220339304(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1565702409220339304(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1565702409220339304(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1565702409220339304(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1565702409220339304(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1565702409220339304(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1565702409220339304);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1565702409220339304);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1565702409220339304);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__7481702161992970198 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        4 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7481702161992970198(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7481702161992970198(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7481702161992970198(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7481702161992970198(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7481702161992970198(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7481702161992970198(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7481702161992970198(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7481702161992970198);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7481702161992970198);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7481702161992970198);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__7398565864096887441 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        4 /* Input0 */, \
      128 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7398565864096887441(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7398565864096887441(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7398565864096887441(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7398565864096887441(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7398565864096887441(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7398565864096887441(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7398565864096887441(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7398565864096887441);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7398565864096887441);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7398565864096887441);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__18359540421217132727 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        4 /* Input0 */, \
      128 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18359540421217132727(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__18359540421217132727(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18359540421217132727(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18359540421217132727(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18359540421217132727(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18359540421217132727(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18359540421217132727(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__18359540421217132727);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18359540421217132727);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18359540421217132727);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__14368498292884470313 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        4 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14368498292884470313(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14368498292884470313(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14368498292884470313(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14368498292884470313(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14368498292884470313(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14368498292884470313(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14368498292884470313(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14368498292884470313);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14368498292884470313);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14368498292884470313);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8884533835296886597 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        4 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8884533835296886597(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8884533835296886597(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8884533835296886597(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8884533835296886597(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8884533835296886597(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8884533835296886597(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8884533835296886597(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8884533835296886597);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8884533835296886597);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8884533835296886597);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__11893855395638851050 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        4 /* Input0 */, \
      32 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11893855395638851050(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11893855395638851050(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11893855395638851050(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11893855395638851050(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11893855395638851050(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11893855395638851050(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11893855395638851050(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11893855395638851050);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11893855395638851050);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11893855395638851050);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14509906446812585052 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        4 /* Input0 */, \
      32 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      96 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14509906446812585052(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14509906446812585052(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14509906446812585052(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14509906446812585052(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14509906446812585052(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14509906446812585052(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14509906446812585052(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14509906446812585052);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14509906446812585052);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14509906446812585052);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__103019240659274681 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        4 /* Input0 */, \
      96 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__103019240659274681(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__103019240659274681(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__103019240659274681(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__103019240659274681(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__103019240659274681(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__103019240659274681(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__103019240659274681(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__103019240659274681);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__103019240659274681);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__103019240659274681);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__13301969162504448558 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        4 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__13301969162504448558(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__13301969162504448558(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__13301969162504448558(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__13301969162504448558(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__13301969162504448558(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__13301969162504448558(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__13301969162504448558(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__13301969162504448558);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__13301969162504448558);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__13301969162504448558);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__13880981363830365899 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        4 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13880981363830365899(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13880981363830365899(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13880981363830365899(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13880981363830365899(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13880981363830365899(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13880981363830365899(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13880981363830365899(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13880981363830365899);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13880981363830365899);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13880981363830365899);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__4108984445956742096 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        4 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4108984445956742096(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4108984445956742096(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4108984445956742096(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4108984445956742096(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4108984445956742096(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4108984445956742096(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4108984445956742096(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4108984445956742096);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4108984445956742096);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4108984445956742096);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__10675329811004606358 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      128 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__10675329811004606358(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__10675329811004606358(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__10675329811004606358(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__10675329811004606358(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__10675329811004606358(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__10675329811004606358(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__10675329811004606358(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__10675329811004606358);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__10675329811004606358);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__10675329811004606358);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__9659383004367529164 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9659383004367529164(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9659383004367529164(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9659383004367529164(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9659383004367529164(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9659383004367529164(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9659383004367529164(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9659383004367529164(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9659383004367529164);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9659383004367529164);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9659383004367529164);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__13784437887767935174 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        64 /* Input0 */, \
      192 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13784437887767935174(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13784437887767935174(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13784437887767935174(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13784437887767935174(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13784437887767935174(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13784437887767935174(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13784437887767935174(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13784437887767935174);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13784437887767935174);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13784437887767935174);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__1819044840243697232 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1819044840243697232(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1819044840243697232(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1819044840243697232(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1819044840243697232(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1819044840243697232(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1819044840243697232(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1819044840243697232(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1819044840243697232);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1819044840243697232);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1819044840243697232);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9647956321851399330 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      96 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9647956321851399330(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9647956321851399330(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9647956321851399330(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9647956321851399330(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9647956321851399330(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9647956321851399330(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9647956321851399330(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9647956321851399330);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9647956321851399330);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9647956321851399330);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12673431412319924031 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      96 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      208 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12673431412319924031(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 208} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12673431412319924031(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12673431412319924031(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12673431412319924031(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12673431412319924031(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12673431412319924031(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12673431412319924031(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12673431412319924031);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12673431412319924031);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12673431412319924031);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__4573706414288985097 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      208 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4573706414288985097(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 208} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4573706414288985097(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4573706414288985097(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4573706414288985097(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4573706414288985097(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4573706414288985097(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4573706414288985097(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4573706414288985097);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4573706414288985097);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4573706414288985097);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12305101108046529101 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      16 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12305101108046529101(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12305101108046529101(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12305101108046529101(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12305101108046529101(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12305101108046529101(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12305101108046529101(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12305101108046529101(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12305101108046529101);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12305101108046529101);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12305101108046529101);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__16932617173870638007 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      16 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16932617173870638007(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16932617173870638007(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16932617173870638007(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16932617173870638007(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16932617173870638007(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16932617173870638007(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16932617173870638007(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16932617173870638007);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16932617173870638007);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16932617173870638007);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5491292824644893300 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      16 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      48 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5491292824644893300(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5491292824644893300(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5491292824644893300(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5491292824644893300(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5491292824644893300(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5491292824644893300(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5491292824644893300(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5491292824644893300);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5491292824644893300);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5491292824644893300);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9694079633906640499 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      48 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9694079633906640499(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9694079633906640499(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9694079633906640499(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9694079633906640499(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9694079633906640499(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9694079633906640499(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9694079633906640499(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9694079633906640499);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9694079633906640499);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9694079633906640499);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__7185901968926577780 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__7185901968926577780(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__7185901968926577780(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__7185901968926577780(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__7185901968926577780(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__7185901968926577780(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__7185901968926577780(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__7185901968926577780(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__7185901968926577780);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__7185901968926577780);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__7185901968926577780);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__1981657212003295377 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1981657212003295377(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1981657212003295377(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1981657212003295377(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1981657212003295377(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1981657212003295377(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1981657212003295377(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1981657212003295377(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1981657212003295377);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1981657212003295377);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1981657212003295377);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__13659305283677211851 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      64 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13659305283677211851(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13659305283677211851(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13659305283677211851(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13659305283677211851(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13659305283677211851(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13659305283677211851(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13659305283677211851(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13659305283677211851);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13659305283677211851);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13659305283677211851);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14410813508407374226 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        64 /* Input0 */, \
      192 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14410813508407374226(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14410813508407374226(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14410813508407374226(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14410813508407374226(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14410813508407374226(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14410813508407374226(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14410813508407374226(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14410813508407374226);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14410813508407374226);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14410813508407374226);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__11424629922304609125 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        256 /* Input0 */, \
      160 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11424629922304609125(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11424629922304609125(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11424629922304609125(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11424629922304609125(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11424629922304609125(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11424629922304609125(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11424629922304609125(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11424629922304609125);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11424629922304609125);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11424629922304609125);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__7451835566427819488 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        64 /* Input0 */, \
      192 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      112 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7451835566427819488(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 112} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7451835566427819488(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7451835566427819488(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7451835566427819488(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7451835566427819488(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7451835566427819488(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7451835566427819488(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7451835566427819488);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7451835566427819488);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7451835566427819488);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__10888242974007726434 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        64 /* Input0 */, \
      112 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10888242974007726434(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10888242974007726434(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10888242974007726434(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10888242974007726434(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10888242974007726434(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10888242974007726434(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10888242974007726434(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10888242974007726434);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10888242974007726434);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10888242974007726434);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__9084283697223661649 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        64 /* Input0 */, \
      112 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      224 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9084283697223661649(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 224} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9084283697223661649(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9084283697223661649(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9084283697223661649(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9084283697223661649(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9084283697223661649(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9084283697223661649(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9084283697223661649);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9084283697223661649);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9084283697223661649);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__18420307919446021579 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        64 /* Input0 */, \
      224 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18420307919446021579(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18420307919446021579(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18420307919446021579(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18420307919446021579(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18420307919446021579(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__18420307919446021579(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18420307919446021579(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18420307919446021579);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18420307919446021579);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__18420307919446021579);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__17881823977575224988 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        64 /* Input0 */, \
      192 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      24 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17881823977575224988(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 24} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17881823977575224988(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17881823977575224988(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17881823977575224988(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17881823977575224988(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17881823977575224988(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17881823977575224988(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17881823977575224988);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17881823977575224988);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17881823977575224988);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__10912126794173573126 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        64 /* Input0 */, \
      24 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10912126794173573126(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10912126794173573126(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10912126794173573126(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10912126794173573126(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10912126794173573126(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10912126794173573126(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10912126794173573126(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10912126794173573126);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10912126794173573126);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10912126794173573126);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__17979387756531152753 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        64 /* Input0 */, \
      24 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      64 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17979387756531152753(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17979387756531152753(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17979387756531152753(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17979387756531152753(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17979387756531152753(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17979387756531152753(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17979387756531152753(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17979387756531152753);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17979387756531152753);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17979387756531152753);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__4526175716958243128 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        64 /* Input0 */, \
      192 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__4526175716958243128(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__4526175716958243128(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__4526175716958243128(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__4526175716958243128(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__4526175716958243128(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__4526175716958243128(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__4526175716958243128(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__4526175716958243128);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__4526175716958243128);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__4526175716958243128);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5109621497058672093 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        64 /* Input0 */, \
      192 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5109621497058672093(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5109621497058672093(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5109621497058672093(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5109621497058672093(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5109621497058672093(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5109621497058672093(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5109621497058672093(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5109621497058672093);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5109621497058672093);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5109621497058672093);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__4696422894390204735 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        64 /* Input0 */, \
      64 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4696422894390204735(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4696422894390204735(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4696422894390204735(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4696422894390204735(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4696422894390204735(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4696422894390204735(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4696422894390204735(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4696422894390204735);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4696422894390204735);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4696422894390204735);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__7218462432593342260 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1024 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7218462432593342260(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7218462432593342260(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7218462432593342260(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7218462432593342260(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7218462432593342260(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7218462432593342260(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7218462432593342260(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7218462432593342260);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7218462432593342260);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7218462432593342260);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__13861555791611592547 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        256 /* Input0 */, \
      160 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13861555791611592547(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13861555791611592547(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13861555791611592547(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13861555791611592547(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13861555791611592547(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13861555791611592547(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13861555791611592547(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13861555791611592547);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13861555791611592547);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13861555791611592547);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__8714444222819656883 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        256 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8714444222819656883(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8714444222819656883(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8714444222819656883(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8714444222819656883(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8714444222819656883(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8714444222819656883(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8714444222819656883(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8714444222819656883);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8714444222819656883);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8714444222819656883);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8030653797742509741 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        256 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8030653797742509741(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8030653797742509741(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8030653797742509741(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8030653797742509741(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8030653797742509741(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8030653797742509741(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8030653797742509741(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8030653797742509741);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8030653797742509741);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8030653797742509741);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__12548368700237972563 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        256 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12548368700237972563(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12548368700237972563(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12548368700237972563(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12548368700237972563(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12548368700237972563(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12548368700237972563(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12548368700237972563(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12548368700237972563);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12548368700237972563);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12548368700237972563);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__15702179921545340223 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        256 /* Input0 */, \
      160 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      24 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15702179921545340223(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 24} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15702179921545340223(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15702179921545340223(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15702179921545340223(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15702179921545340223(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15702179921545340223(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15702179921545340223(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15702179921545340223);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15702179921545340223);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15702179921545340223);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__17368053915572632779 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        256 /* Input0 */, \
      24 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17368053915572632779(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17368053915572632779(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17368053915572632779(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17368053915572632779(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17368053915572632779(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17368053915572632779(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17368053915572632779(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17368053915572632779);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17368053915572632779);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17368053915572632779);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11523459527500293052 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        256 /* Input0 */, \
      24 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      64 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11523459527500293052(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11523459527500293052(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11523459527500293052(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11523459527500293052(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11523459527500293052(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11523459527500293052(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11523459527500293052(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11523459527500293052);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11523459527500293052);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11523459527500293052);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__2238409475648109211 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        256 /* Input0 */, \
      160 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__2238409475648109211(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__2238409475648109211(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__2238409475648109211(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__2238409475648109211(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__2238409475648109211(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__2238409475648109211(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__2238409475648109211(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__2238409475648109211);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__2238409475648109211);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__2238409475648109211);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__7433574873688280702 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        256 /* Input0 */, \
      160 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7433574873688280702(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7433574873688280702(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7433574873688280702(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7433574873688280702(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7433574873688280702(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7433574873688280702(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7433574873688280702(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7433574873688280702);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7433574873688280702);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7433574873688280702);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__2829697870017777138 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        256 /* Input0 */, \
      64 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2829697870017777138(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2829697870017777138(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2829697870017777138(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2829697870017777138(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2829697870017777138(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2829697870017777138(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2829697870017777138(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2829697870017777138);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2829697870017777138);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2829697870017777138);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__13549981734977722898 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1024 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      112 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13549981734977722898(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 112} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13549981734977722898(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13549981734977722898(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13549981734977722898(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13549981734977722898(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13549981734977722898(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13549981734977722898(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13549981734977722898);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13549981734977722898);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13549981734977722898);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__5612002379057013320 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        4096 /* Input0 */, \
      112 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5612002379057013320(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5612002379057013320(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5612002379057013320(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5612002379057013320(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5612002379057013320(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5612002379057013320(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5612002379057013320(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5612002379057013320);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5612002379057013320);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5612002379057013320);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__15481375148636093484 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1024 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      144 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15481375148636093484(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 144} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15481375148636093484(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15481375148636093484(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15481375148636093484(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15481375148636093484(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15481375148636093484(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15481375148636093484(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15481375148636093484);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15481375148636093484);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15481375148636093484);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__2311857639289536478 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1024 /* Input0 */, \
      144 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2311857639289536478(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2311857639289536478(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2311857639289536478(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2311857639289536478(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2311857639289536478(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2311857639289536478(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2311857639289536478(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2311857639289536478);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2311857639289536478);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2311857639289536478);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5805644535422787395 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1024 /* Input0 */, \
      144 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      288 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5805644535422787395(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 288} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5805644535422787395(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5805644535422787395(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5805644535422787395(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5805644535422787395(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5805644535422787395(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5805644535422787395(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5805644535422787395);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5805644535422787395);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5805644535422787395);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__15687145761702745239 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1024 /* Input0 */, \
      288 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15687145761702745239(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 288} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15687145761702745239(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15687145761702745239(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15687145761702745239(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15687145761702745239(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15687145761702745239(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15687145761702745239(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15687145761702745239);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15687145761702745239);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15687145761702745239);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2767214238919195553 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1024 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2767214238919195553(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2767214238919195553(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2767214238919195553(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2767214238919195553(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2767214238919195553(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2767214238919195553(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2767214238919195553(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2767214238919195553);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2767214238919195553);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2767214238919195553);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__12073776034013751375 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1024 /* Input0 */, \
      32 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12073776034013751375(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12073776034013751375(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12073776034013751375(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12073776034013751375(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12073776034013751375(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12073776034013751375(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12073776034013751375(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12073776034013751375);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12073776034013751375);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12073776034013751375);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14511335993161070392 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1024 /* Input0 */, \
      32 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      64 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14511335993161070392(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14511335993161070392(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14511335993161070392(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14511335993161070392(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14511335993161070392(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14511335993161070392(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14511335993161070392(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14511335993161070392);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14511335993161070392);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14511335993161070392);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__16551076879279726282 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        1024 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__16551076879279726282(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__16551076879279726282(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__16551076879279726282(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__16551076879279726282(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__16551076879279726282(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__16551076879279726282(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__16551076879279726282(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__16551076879279726282);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__16551076879279726282);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__16551076879279726282);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11351337989592328751 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1024 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11351337989592328751(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11351337989592328751(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11351337989592328751(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11351337989592328751(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11351337989592328751(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11351337989592328751(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11351337989592328751(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11351337989592328751);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11351337989592328751);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11351337989592328751);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__4289633252889740917 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1024 /* Input0 */, \
      64 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4289633252889740917(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4289633252889740917(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4289633252889740917(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4289633252889740917(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4289633252889740917(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4289633252889740917(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4289633252889740917(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4289633252889740917);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4289633252889740917);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4289633252889740917);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11975635359135607606 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        4096 /* Input0 */, \
      112 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11975635359135607606(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11975635359135607606(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11975635359135607606(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11975635359135607606(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11975635359135607606(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11975635359135607606(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11975635359135607606(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11975635359135607606);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11975635359135607606);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11975635359135607606);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__3578243887978211176 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        16384 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3578243887978211176(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3578243887978211176(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3578243887978211176(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3578243887978211176(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3578243887978211176(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3578243887978211176(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3578243887978211176(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3578243887978211176);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3578243887978211176);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3578243887978211176);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__3841713278929955612 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        4096 /* Input0 */, \
      112 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3841713278929955612(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3841713278929955612(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3841713278929955612(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3841713278929955612(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3841713278929955612(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3841713278929955612(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3841713278929955612(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3841713278929955612);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3841713278929955612);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3841713278929955612);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__2458817798348893314 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        4096 /* Input0 */, \
      160 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2458817798348893314(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2458817798348893314(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2458817798348893314(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2458817798348893314(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2458817798348893314(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2458817798348893314(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2458817798348893314(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2458817798348893314);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2458817798348893314);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2458817798348893314);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__13011425189416604842 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        4096 /* Input0 */, \
      160 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      320 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13011425189416604842(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 320} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13011425189416604842(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13011425189416604842(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13011425189416604842(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13011425189416604842(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13011425189416604842(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13011425189416604842(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13011425189416604842);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13011425189416604842);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13011425189416604842);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__15824537773460006402 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        4096 /* Input0 */, \
      320 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15824537773460006402(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15824537773460006402(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15824537773460006402(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15824537773460006402(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15824537773460006402(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15824537773460006402(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15824537773460006402(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15824537773460006402);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15824537773460006402);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15824537773460006402);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__1129252902536427229 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        4096 /* Input0 */, \
      112 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1129252902536427229(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1129252902536427229(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1129252902536427229(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1129252902536427229(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1129252902536427229(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1129252902536427229(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1129252902536427229(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1129252902536427229);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1129252902536427229);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1129252902536427229);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__567294650979353647 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        4096 /* Input0 */, \
      32 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__567294650979353647(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__567294650979353647(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__567294650979353647(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__567294650979353647(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__567294650979353647(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__567294650979353647(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__567294650979353647(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__567294650979353647);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__567294650979353647);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__567294650979353647);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14863044695081904709 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        4096 /* Input0 */, \
      32 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      128 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14863044695081904709(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14863044695081904709(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14863044695081904709(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14863044695081904709(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14863044695081904709(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14863044695081904709(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14863044695081904709(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14863044695081904709);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14863044695081904709);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14863044695081904709);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__14735157318031987638 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        4096 /* Input0 */, \
      112 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__14735157318031987638(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__14735157318031987638(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__14735157318031987638(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__14735157318031987638(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__14735157318031987638(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__14735157318031987638(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__14735157318031987638(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__14735157318031987638);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__14735157318031987638);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__14735157318031987638);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__1383596300876944974 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        4096 /* Input0 */, \
      112 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1383596300876944974(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1383596300876944974(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1383596300876944974(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1383596300876944974(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1383596300876944974(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1383596300876944974(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1383596300876944974(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1383596300876944974);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1383596300876944974);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1383596300876944974);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__14158443639363289940 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        4096 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14158443639363289940(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14158443639363289940(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14158443639363289940(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14158443639363289940(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14158443639363289940(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14158443639363289940(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14158443639363289940(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14158443639363289940);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14158443639363289940);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14158443639363289940);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__12696883437894309526 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        16384 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__12696883437894309526(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__12696883437894309526(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__12696883437894309526(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__12696883437894309526(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__12696883437894309526(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__12696883437894309526(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__12696883437894309526(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__12696883437894309526);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__12696883437894309526);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__12696883437894309526);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12144473031666270479 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        16384 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12144473031666270479(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12144473031666270479(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12144473031666270479(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12144473031666270479(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12144473031666270479(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12144473031666270479(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12144473031666270479(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12144473031666270479);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12144473031666270479);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12144473031666270479);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__6353787560842308132 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        65536 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6353787560842308132(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6353787560842308132(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6353787560842308132(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6353787560842308132(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6353787560842308132(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6353787560842308132(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6353787560842308132(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6353787560842308132);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6353787560842308132);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6353787560842308132);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4317637952032227621 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        16384 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4317637952032227621(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4317637952032227621(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4317637952032227621(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4317637952032227621(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4317637952032227621(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4317637952032227621(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4317637952032227621(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4317637952032227621);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4317637952032227621);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4317637952032227621);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__8354935821930940231 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        16384 /* Input0 */, \
      160 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8354935821930940231(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8354935821930940231(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8354935821930940231(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8354935821930940231(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8354935821930940231(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8354935821930940231(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8354935821930940231(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8354935821930940231);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8354935821930940231);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8354935821930940231);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__16536116508441460591 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        16384 /* Input0 */, \
      160 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      320 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16536116508441460591(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 320} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16536116508441460591(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16536116508441460591(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16536116508441460591(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16536116508441460591(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16536116508441460591(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16536116508441460591(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16536116508441460591);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16536116508441460591);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16536116508441460591);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9975971652142009799 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        16384 /* Input0 */, \
      320 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9975971652142009799(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9975971652142009799(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9975971652142009799(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9975971652142009799(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9975971652142009799(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9975971652142009799(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9975971652142009799(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9975971652142009799);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9975971652142009799);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9975971652142009799);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__76627715148881124 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        16384 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__76627715148881124(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__76627715148881124(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__76627715148881124(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__76627715148881124(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__76627715148881124(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__76627715148881124(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__76627715148881124(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__76627715148881124);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__76627715148881124);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__76627715148881124);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__6211210889337876458 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        16384 /* Input0 */, \
      32 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6211210889337876458(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6211210889337876458(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6211210889337876458(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6211210889337876458(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6211210889337876458(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6211210889337876458(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6211210889337876458(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6211210889337876458);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6211210889337876458);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6211210889337876458);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11504986725474845056 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        16384 /* Input0 */, \
      32 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      128 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11504986725474845056(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11504986725474845056(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11504986725474845056(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11504986725474845056(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11504986725474845056(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11504986725474845056(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11504986725474845056(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11504986725474845056);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11504986725474845056);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11504986725474845056);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__14034938869668805007 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        16384 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__14034938869668805007(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__14034938869668805007(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__14034938869668805007(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__14034938869668805007(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__14034938869668805007(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__14034938869668805007(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__14034938869668805007(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__14034938869668805007);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__14034938869668805007);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__14034938869668805007);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2128039374656748663 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        16384 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2128039374656748663(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2128039374656748663(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2128039374656748663(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2128039374656748663(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2128039374656748663(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2128039374656748663(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2128039374656748663(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2128039374656748663);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2128039374656748663);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2128039374656748663);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__10777445300294150289 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        16384 /* Input0 */, \
      128 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10777445300294150289(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10777445300294150289(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10777445300294150289(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10777445300294150289(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10777445300294150289(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10777445300294150289(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10777445300294150289(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10777445300294150289);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10777445300294150289);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10777445300294150289);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4669294775958113474 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        65536 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      384 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4669294775958113474(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4669294775958113474(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4669294775958113474(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4669294775958113474(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4669294775958113474(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4669294775958113474(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4669294775958113474(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4669294775958113474);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4669294775958113474);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4669294775958113474);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__4580162803117873438 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        262144 /* Input0 */, \
      384 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4580162803117873438(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 262144} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4580162803117873438(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4580162803117873438(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4580162803117873438(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4580162803117873438(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4580162803117873438(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4580162803117873438(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4580162803117873438);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4580162803117873438);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4580162803117873438);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4324963748548897634 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        65536 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4324963748548897634(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4324963748548897634(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4324963748548897634(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4324963748548897634(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4324963748548897634(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4324963748548897634(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4324963748548897634(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4324963748548897634);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4324963748548897634);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4324963748548897634);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__3399597021902881404 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        65536 /* Input0 */, \
      192 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3399597021902881404(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3399597021902881404(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3399597021902881404(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3399597021902881404(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3399597021902881404(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3399597021902881404(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3399597021902881404(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3399597021902881404);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3399597021902881404);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3399597021902881404);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14664554858885838842 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        65536 /* Input0 */, \
      192 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      384 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14664554858885838842(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14664554858885838842(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14664554858885838842(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14664554858885838842(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14664554858885838842(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14664554858885838842(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14664554858885838842(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14664554858885838842);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14664554858885838842);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14664554858885838842);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__6746287482177165116 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        65536 /* Input0 */, \
      384 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6746287482177165116(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6746287482177165116(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6746287482177165116(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6746287482177165116(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6746287482177165116(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6746287482177165116(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6746287482177165116(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6746287482177165116);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6746287482177165116);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6746287482177165116);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__7614576295550957963 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        65536 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      48 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7614576295550957963(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7614576295550957963(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7614576295550957963(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7614576295550957963(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7614576295550957963(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7614576295550957963(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7614576295550957963(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7614576295550957963);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7614576295550957963);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7614576295550957963);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__16882651569745791293 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        65536 /* Input0 */, \
      48 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16882651569745791293(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16882651569745791293(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16882651569745791293(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16882651569745791293(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16882651569745791293(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16882651569745791293(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16882651569745791293(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16882651569745791293);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16882651569745791293);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16882651569745791293);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2580687081091458903 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        65536 /* Input0 */, \
      48 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      128 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2580687081091458903(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2580687081091458903(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2580687081091458903(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2580687081091458903(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2580687081091458903(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2580687081091458903(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2580687081091458903(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2580687081091458903);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2580687081091458903);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2580687081091458903);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__15686369502144060378 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        65536 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__15686369502144060378(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__15686369502144060378(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__15686369502144060378(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__15686369502144060378(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__15686369502144060378(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__15686369502144060378(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__15686369502144060378(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__15686369502144060378);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__15686369502144060378);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__15686369502144060378);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__504231566777163298 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        65536 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__504231566777163298(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__504231566777163298(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__504231566777163298(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__504231566777163298(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__504231566777163298(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__504231566777163298(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__504231566777163298(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__504231566777163298);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__504231566777163298);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__504231566777163298);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__10296950718032202436 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        65536 /* Input0 */, \
      128 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10296950718032202436(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10296950718032202436(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10296950718032202436(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10296950718032202436(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10296950718032202436(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10296950718032202436(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10296950718032202436(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10296950718032202436);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10296950718032202436);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10296950718032202436);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__13694351498573523168 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        262144 /* Input0 */, \
      384 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__13694351498573523168(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 262144} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__13694351498573523168(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__13694351498573523168(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__13694351498573523168(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__13694351498573523168(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__13694351498573523168(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__13694351498573523168(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__13694351498573523168);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__13694351498573523168);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__13694351498573523168);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__9516206706028465084 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      96 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      256 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9516206706028465084(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9516206706028465084(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9516206706028465084(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9516206706028465084(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9516206706028465084(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9516206706028465084(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9516206706028465084(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9516206706028465084);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9516206706028465084);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9516206706028465084);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__5296436437442052454 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5296436437442052454(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5296436437442052454(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5296436437442052454(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5296436437442052454(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5296436437442052454(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5296436437442052454(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5296436437442052454(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5296436437442052454);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5296436437442052454);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5296436437442052454);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__14419581579598180504 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__14419581579598180504(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__14419581579598180504(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__14419581579598180504(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__14419581579598180504(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__14419581579598180504(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__14419581579598180504(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__14419581579598180504(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__14419581579598180504);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__14419581579598180504);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__14419581579598180504);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5096481279428734828 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      14 /* Input3 */, \
      384 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5096481279428734828(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5096481279428734828(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5096481279428734828(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5096481279428734828(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5096481279428734828(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5096481279428734828(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5096481279428734828(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5096481279428734828);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5096481279428734828);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5096481279428734828);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4830328438837449332 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      384 /* Input1 */, \
      13 /* Input2 */, \
      14 /* Input3 */, \
      384 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4830328438837449332(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4830328438837449332(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4830328438837449332(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4830328438837449332(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4830328438837449332(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4830328438837449332(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4830328438837449332(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4830328438837449332);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4830328438837449332);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4830328438837449332);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__12080901151633876978 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      384 /* Input1 */, \
      13 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12080901151633876978(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12080901151633876978(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12080901151633876978(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12080901151633876978(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12080901151633876978(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12080901151633876978(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12080901151633876978(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12080901151633876978);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12080901151633876978);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12080901151633876978);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12698614221717716460 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      384 /* Input1 */, \
      13 /* Input2 */, \
      14 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12698614221717716460(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12698614221717716460(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12698614221717716460(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12698614221717716460(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12698614221717716460(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12698614221717716460(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12698614221717716460(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12698614221717716460);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12698614221717716460);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12698614221717716460);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__11679359066251336426 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11679359066251336426(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11679359066251336426(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11679359066251336426(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11679359066251336426(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11679359066251336426(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11679359066251336426(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11679359066251336426(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11679359066251336426);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11679359066251336426);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11679359066251336426);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__2560708552360094484 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__2560708552360094484(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__2560708552360094484(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__2560708552360094484(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__2560708552360094484(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__2560708552360094484(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__2560708552360094484(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__2560708552360094484(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__2560708552360094484);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__2560708552360094484);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__2560708552360094484);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6023261935327604114 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        0 /* Input0 */, \
      3 /* Input1 */, \
      416 /* Input2 */, \
      416 /* Input3 */, \
      16 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6023261935327604114(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 416} /* Input2 */, 
      {"input[3]", 416} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6023261935327604114(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6023261935327604114(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6023261935327604114(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6023261935327604114(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6023261935327604114(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6023261935327604114(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6023261935327604114);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6023261935327604114);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6023261935327604114);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__16918098971219796913 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        0 /* Input0 */, \
      16 /* Input1 */, \
      416 /* Input2 */, \
      416 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16918098971219796913(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 416} /* Input2 */, 
      {"input[3]", 416} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16918098971219796913(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16918098971219796913(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16918098971219796913(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16918098971219796913(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16918098971219796913(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16918098971219796913(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, ACTIVATION_LEAKY_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16918098971219796913);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16918098971219796913);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16918098971219796913);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__7734145712973763151 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        0 /* Input0 */, \
      16 /* Input1 */, \
      416 /* Input2 */, \
      416 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__7734145712973763151(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 416} /* Input2 */, 
      {"input[3]", 416} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__7734145712973763151(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__7734145712973763151(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__7734145712973763151(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__7734145712973763151(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__7734145712973763151(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__7734145712973763151(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__7734145712973763151);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__7734145712973763151);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__7734145712973763151);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6815706674681204708 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        0 /* Input0 */, \
      16 /* Input1 */, \
      208 /* Input2 */, \
      208 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6815706674681204708(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 208} /* Input2 */, 
      {"input[3]", 208} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6815706674681204708(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6815706674681204708(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6815706674681204708(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6815706674681204708(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6815706674681204708(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6815706674681204708(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6815706674681204708);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6815706674681204708);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6815706674681204708);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__15991210803390072935 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        0 /* Input0 */, \
      32 /* Input1 */, \
      208 /* Input2 */, \
      208 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15991210803390072935(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 208} /* Input2 */, 
      {"input[3]", 208} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15991210803390072935(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15991210803390072935(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15991210803390072935(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15991210803390072935(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15991210803390072935(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15991210803390072935(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, ACTIVATION_LEAKY_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15991210803390072935);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15991210803390072935);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15991210803390072935);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__6660872693411756441 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        0 /* Input0 */, \
      32 /* Input1 */, \
      208 /* Input2 */, \
      208 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__6660872693411756441(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 208} /* Input2 */, 
      {"input[3]", 208} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__6660872693411756441(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__6660872693411756441(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__6660872693411756441(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__6660872693411756441(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__6660872693411756441(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__6660872693411756441(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__6660872693411756441);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__6660872693411756441);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__6660872693411756441);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__3966603363610566520 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        0 /* Input0 */, \
      32 /* Input1 */, \
      104 /* Input2 */, \
      104 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3966603363610566520(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 104} /* Input2 */, 
      {"input[3]", 104} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3966603363610566520(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3966603363610566520(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3966603363610566520(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3966603363610566520(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3966603363610566520(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3966603363610566520(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3966603363610566520);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3966603363610566520);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3966603363610566520);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__6407059873323415049 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        0 /* Input0 */, \
      64 /* Input1 */, \
      104 /* Input2 */, \
      104 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6407059873323415049(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 104} /* Input2 */, 
      {"input[3]", 104} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6407059873323415049(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6407059873323415049(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6407059873323415049(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6407059873323415049(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6407059873323415049(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6407059873323415049(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, ACTIVATION_LEAKY_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6407059873323415049);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6407059873323415049);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6407059873323415049);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__15669833924455270391 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        0 /* Input0 */, \
      64 /* Input1 */, \
      104 /* Input2 */, \
      104 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__15669833924455270391(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 104} /* Input2 */, 
      {"input[3]", 104} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__15669833924455270391(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__15669833924455270391(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__15669833924455270391(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__15669833924455270391(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__15669833924455270391(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__15669833924455270391(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__15669833924455270391);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__15669833924455270391);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__15669833924455270391);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10266435012932268711 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        0 /* Input0 */, \
      64 /* Input1 */, \
      52 /* Input2 */, \
      52 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10266435012932268711(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 52} /* Input2 */, 
      {"input[3]", 52} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10266435012932268711(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10266435012932268711(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10266435012932268711(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10266435012932268711(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10266435012932268711(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10266435012932268711(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10266435012932268711);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10266435012932268711);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10266435012932268711);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9604777794731290032 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        0 /* Input0 */, \
      128 /* Input1 */, \
      52 /* Input2 */, \
      52 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9604777794731290032(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 52} /* Input2 */, 
      {"input[3]", 52} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9604777794731290032(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9604777794731290032(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9604777794731290032(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9604777794731290032(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9604777794731290032(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9604777794731290032(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, ACTIVATION_LEAKY_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9604777794731290032);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9604777794731290032);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9604777794731290032);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__348777632606248014 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        0 /* Input0 */, \
      128 /* Input1 */, \
      52 /* Input2 */, \
      52 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__348777632606248014(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 52} /* Input2 */, 
      {"input[3]", 52} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__348777632606248014(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__348777632606248014(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__348777632606248014(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__348777632606248014(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__348777632606248014(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__348777632606248014(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__348777632606248014);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__348777632606248014);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__348777632606248014);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12981700766781039282 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        0 /* Input0 */, \
      128 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12981700766781039282(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12981700766781039282(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12981700766781039282(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12981700766781039282(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12981700766781039282(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12981700766781039282(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12981700766781039282(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12981700766781039282);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12981700766781039282);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12981700766781039282);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9474072086636087676 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        0 /* Input0 */, \
      256 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9474072086636087676(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9474072086636087676(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9474072086636087676(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9474072086636087676(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9474072086636087676(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9474072086636087676(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9474072086636087676(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, ACTIVATION_LEAKY_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9474072086636087676);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9474072086636087676);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9474072086636087676);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__209056405116631170 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        0 /* Input0 */, \
      256 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__209056405116631170(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__209056405116631170(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__209056405116631170(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__209056405116631170(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__209056405116631170(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__209056405116631170(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__209056405116631170(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__209056405116631170);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__209056405116631170);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__209056405116631170);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__3911178556542384816 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        0 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3911178556542384816(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3911178556542384816(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3911178556542384816(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3911178556542384816(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3911178556542384816(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3911178556542384816(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3911178556542384816(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3911178556542384816);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3911178556542384816);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3911178556542384816);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__15812650908473051694 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        0 /* Input0 */, \
      512 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15812650908473051694(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15812650908473051694(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15812650908473051694(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15812650908473051694(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15812650908473051694(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15812650908473051694(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15812650908473051694(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, ACTIVATION_LEAKY_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15812650908473051694);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15812650908473051694);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15812650908473051694);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__6552121543614741456 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        0 /* Input0 */, \
      512 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__6552121543614741456(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__6552121543614741456(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__6552121543614741456(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__6552121543614741456(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__6552121543614741456(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__6552121543614741456(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__6552121543614741456(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__6552121543614741456);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__6552121543614741456);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__6552121543614741456);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11206028001459029239 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        0 /* Input0 */, \
      512 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      1024 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11206028001459029239(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11206028001459029239(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11206028001459029239(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11206028001459029239(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11206028001459029239(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11206028001459029239(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11206028001459029239(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11206028001459029239);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11206028001459029239);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11206028001459029239);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6189153574142127990 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        0 /* Input0 */, \
      1024 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      1024 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6189153574142127990(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6189153574142127990(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6189153574142127990(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6189153574142127990(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6189153574142127990(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6189153574142127990(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6189153574142127990(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6189153574142127990);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6189153574142127990);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6189153574142127990);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__15910746073321323311 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        0 /* Input0 */, \
      1024 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15910746073321323311(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15910746073321323311(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15910746073321323311(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15910746073321323311(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15910746073321323311(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15910746073321323311(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15910746073321323311(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, ACTIVATION_LEAKY_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15910746073321323311);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15910746073321323311);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15910746073321323311);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__15794554034599751004 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        0 /* Input0 */, \
      1024 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      125 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15794554034599751004(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"filter_count", 125} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15794554034599751004(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15794554034599751004(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15794554034599751004(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15794554034599751004(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15794554034599751004(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15794554034599751004(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15794554034599751004);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15794554034599751004);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15794554034599751004);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2302444874456274904 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      1 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      8 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2302444874456274904(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 8} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2302444874456274904(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2302444874456274904(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2302444874456274904(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2302444874456274904(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2302444874456274904(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2302444874456274904(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2302444874456274904);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2302444874456274904);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2302444874456274904);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__2481056835706794463 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      8 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2481056835706794463(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 8} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2481056835706794463(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2481056835706794463(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2481056835706794463(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2481056835706794463(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2481056835706794463(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2481056835706794463(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2481056835706794463);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2481056835706794463);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2481056835706794463);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__11813612418775892001 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      8 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__11813612418775892001(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 8} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__11813612418775892001(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__11813612418775892001(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__11813612418775892001(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__11813612418775892001(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__11813612418775892001(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__11813612418775892001(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__11813612418775892001);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__11813612418775892001);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__11813612418775892001);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12588517614440524400 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      8 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      16 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12588517614440524400(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 8} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12588517614440524400(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12588517614440524400(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12588517614440524400(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12588517614440524400(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12588517614440524400(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12588517614440524400(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12588517614440524400);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12588517614440524400);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12588517614440524400);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__5668318358266444891 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      16 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5668318358266444891(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5668318358266444891(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5668318358266444891(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5668318358266444891(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5668318358266444891(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5668318358266444891(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5668318358266444891(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5668318358266444891);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5668318358266444891);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5668318358266444891);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__14931108898853588389 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      16 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__14931108898853588389(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__14931108898853588389(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__14931108898853588389(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__14931108898853588389(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__14931108898853588389(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__14931108898853588389(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__14931108898853588389(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__14931108898853588389);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__14931108898853588389);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__14931108898853588389);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14198419017787858620 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14198419017787858620(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14198419017787858620(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14198419017787858620(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14198419017787858620(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14198419017787858620(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14198419017787858620(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14198419017787858620(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14198419017787858620);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14198419017787858620);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14198419017787858620);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14349997907744091173 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14349997907744091173(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14349997907744091173(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14349997907744091173(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14349997907744091173(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14349997907744091173(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14349997907744091173(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14349997907744091173(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14349997907744091173);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14349997907744091173);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14349997907744091173);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__13898496585881159879 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13898496585881159879(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13898496585881159879(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13898496585881159879(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13898496585881159879(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13898496585881159879(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13898496585881159879(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13898496585881159879(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13898496585881159879);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13898496585881159879);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13898496585881159879);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14390166043040606333 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      96 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14390166043040606333(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14390166043040606333(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14390166043040606333(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14390166043040606333(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14390166043040606333(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14390166043040606333(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14390166043040606333(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14390166043040606333);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14390166043040606333);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14390166043040606333);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__18437691963492759572 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      96 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      96 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18437691963492759572(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__18437691963492759572(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18437691963492759572(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18437691963492759572(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18437691963492759572(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18437691963492759572(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18437691963492759572(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__18437691963492759572);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18437691963492759572);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18437691963492759572);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__17909291660268837038 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      96 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17909291660268837038(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17909291660268837038(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17909291660268837038(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17909291660268837038(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17909291660268837038(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17909291660268837038(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17909291660268837038(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17909291660268837038);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17909291660268837038);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17909291660268837038);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__13770985569113168064 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__13770985569113168064(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__13770985569113168064(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__13770985569113168064(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__13770985569113168064(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__13770985569113168064(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__13770985569113168064(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__13770985569113168064(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__13770985569113168064);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__13770985569113168064);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__13770985569113168064);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8992136372144651691 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8992136372144651691(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8992136372144651691(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8992136372144651691(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8992136372144651691(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8992136372144651691(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8992136372144651691(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8992136372144651691(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8992136372144651691);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8992136372144651691);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8992136372144651691);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__6694632611452348157 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      32 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6694632611452348157(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6694632611452348157(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6694632611452348157(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6694632611452348157(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6694632611452348157(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6694632611452348157(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6694632611452348157(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6694632611452348157);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6694632611452348157);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6694632611452348157);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__9750358535019919148 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        4 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      96 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9750358535019919148(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9750358535019919148(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9750358535019919148(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9750358535019919148(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9750358535019919148(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9750358535019919148(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9750358535019919148(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9750358535019919148);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9750358535019919148);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9750358535019919148);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__13779835688777344837 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        4 /* Input0 */, \
      96 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      96 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13779835688777344837(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13779835688777344837(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13779835688777344837(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13779835688777344837(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13779835688777344837(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13779835688777344837(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13779835688777344837(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13779835688777344837);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13779835688777344837);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13779835688777344837);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__13289714904281712639 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        4 /* Input0 */, \
      96 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13289714904281712639(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13289714904281712639(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13289714904281712639(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13289714904281712639(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13289714904281712639(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13289714904281712639(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13289714904281712639(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13289714904281712639);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13289714904281712639);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13289714904281712639);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__115233520626755176 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        4 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__115233520626755176(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__115233520626755176(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__115233520626755176(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__115233520626755176(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__115233520626755176(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__115233520626755176(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__115233520626755176(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__115233520626755176);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__115233520626755176);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__115233520626755176);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8764098600772227725 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        4 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8764098600772227725(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8764098600772227725(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8764098600772227725(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8764098600772227725(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8764098600772227725(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8764098600772227725(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8764098600772227725(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8764098600772227725);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8764098600772227725);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8764098600772227725);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9224913184663574422 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        4 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9224913184663574422(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9224913184663574422(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9224913184663574422(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9224913184663574422(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9224913184663574422(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9224913184663574422(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9224913184663574422(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9224913184663574422);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9224913184663574422);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9224913184663574422);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12520473409532553065 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12520473409532553065(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12520473409532553065(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12520473409532553065(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12520473409532553065(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12520473409532553065(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12520473409532553065(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12520473409532553065(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12520473409532553065);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12520473409532553065);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12520473409532553065);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__12445992465023302190 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      128 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12445992465023302190(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12445992465023302190(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12445992465023302190(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12445992465023302190(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12445992465023302190(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12445992465023302190(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12445992465023302190(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12445992465023302190);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12445992465023302190);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12445992465023302190);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__15066595224623157342 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      128 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      160 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15066595224623157342(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15066595224623157342(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15066595224623157342(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15066595224623157342(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15066595224623157342(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15066595224623157342(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15066595224623157342(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15066595224623157342);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15066595224623157342);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15066595224623157342);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__18391407797513998744 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        48 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18391407797513998744(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 48} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18391407797513998744(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18391407797513998744(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18391407797513998744(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18391407797513998744(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__18391407797513998744(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18391407797513998744(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18391407797513998744);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18391407797513998744);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__18391407797513998744);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__772041063381112436 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__772041063381112436(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__772041063381112436(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__772041063381112436(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__772041063381112436(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__772041063381112436(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__772041063381112436(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__772041063381112436(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__772041063381112436);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__772041063381112436);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__772041063381112436);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__17515620270619939695 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17515620270619939695(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17515620270619939695(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17515620270619939695(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17515620270619939695(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17515620270619939695(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17515620270619939695(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17515620270619939695(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17515620270619939695);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17515620270619939695);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17515620270619939695);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__17601959682292921301 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      96 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17601959682292921301(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17601959682292921301(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17601959682292921301(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17601959682292921301(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17601959682292921301(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17601959682292921301(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17601959682292921301(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17601959682292921301);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17601959682292921301);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17601959682292921301);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__14661485306519438086 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      96 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14661485306519438086(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14661485306519438086(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14661485306519438086(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14661485306519438086(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14661485306519438086(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14661485306519438086(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14661485306519438086(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14661485306519438086);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14661485306519438086);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14661485306519438086);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__3854234356944245752 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      96 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      96 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3854234356944245752(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3854234356944245752(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3854234356944245752(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3854234356944245752(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3854234356944245752(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3854234356944245752(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3854234356944245752(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3854234356944245752);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3854234356944245752);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3854234356944245752);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__13207388820901383374 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13207388820901383374(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13207388820901383374(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13207388820901383374(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13207388820901383374(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13207388820901383374(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13207388820901383374(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13207388820901383374(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13207388820901383374);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13207388820901383374);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13207388820901383374);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__8255069775327306385 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__8255069775327306385(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__8255069775327306385(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__8255069775327306385(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__8255069775327306385(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__8255069775327306385(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__8255069775327306385(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__8255069775327306385(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__8255069775327306385);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__8255069775327306385);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__8255069775327306385);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__16897176049575078859 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        48 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      224 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16897176049575078859(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 48} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 224} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16897176049575078859(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16897176049575078859(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16897176049575078859(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16897176049575078859(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16897176049575078859(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16897176049575078859(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16897176049575078859);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16897176049575078859);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16897176049575078859);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__8809427361114020330 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        192 /* Input0 */, \
      224 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8809427361114020330(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 192} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8809427361114020330(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8809427361114020330(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8809427361114020330(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8809427361114020330(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8809427361114020330(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8809427361114020330(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8809427361114020330);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8809427361114020330);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8809427361114020330);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__476901516806820995 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        48 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__476901516806820995(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 48} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__476901516806820995(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__476901516806820995(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__476901516806820995(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__476901516806820995(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__476901516806820995(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__476901516806820995(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__476901516806820995);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__476901516806820995);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__476901516806820995);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__5112162447267911439 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        48 /* Input0 */, \
      64 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5112162447267911439(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 48} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5112162447267911439(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5112162447267911439(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5112162447267911439(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5112162447267911439(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5112162447267911439(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5112162447267911439(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5112162447267911439);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5112162447267911439);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5112162447267911439);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4730836171960170421 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        48 /* Input0 */, \
      64 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      96 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4730836171960170421(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 48} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4730836171960170421(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4730836171960170421(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4730836171960170421(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4730836171960170421(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4730836171960170421(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4730836171960170421(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4730836171960170421);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4730836171960170421);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4730836171960170421);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__297381060854555202 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        48 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__297381060854555202(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 48} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__297381060854555202(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__297381060854555202(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__297381060854555202(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__297381060854555202(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__297381060854555202(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__297381060854555202(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__297381060854555202);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__297381060854555202);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__297381060854555202);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9122595509463369574 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        48 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9122595509463369574(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 48} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9122595509463369574(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9122595509463369574(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9122595509463369574(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9122595509463369574(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9122595509463369574(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9122595509463369574(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9122595509463369574);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9122595509463369574);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9122595509463369574);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__15854092271266488320 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        48 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15854092271266488320(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 48} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15854092271266488320(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15854092271266488320(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15854092271266488320(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15854092271266488320(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15854092271266488320(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15854092271266488320(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15854092271266488320);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15854092271266488320);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15854092271266488320);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__13529956688111556904 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        48 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13529956688111556904(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 48} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13529956688111556904(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13529956688111556904(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13529956688111556904(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13529956688111556904(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13529956688111556904(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13529956688111556904(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13529956688111556904);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13529956688111556904);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13529956688111556904);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__9126397446047899750 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        48 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__9126397446047899750(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 48} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__9126397446047899750(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__9126397446047899750(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__9126397446047899750(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__9126397446047899750(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__9126397446047899750(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__9126397446047899750(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__9126397446047899750);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__9126397446047899750);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__9126397446047899750);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11666913606190785950 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        48 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11666913606190785950(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 48} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11666913606190785950(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11666913606190785950(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11666913606190785950(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11666913606190785950(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11666913606190785950(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11666913606190785950(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11666913606190785950);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11666913606190785950);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11666913606190785950);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__1826485936082845262 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        48 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1826485936082845262(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 48} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1826485936082845262(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1826485936082845262(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1826485936082845262(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1826485936082845262(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1826485936082845262(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1826485936082845262(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1826485936082845262);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1826485936082845262);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1826485936082845262);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2191339870464579756 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        192 /* Input0 */, \
      224 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2191339870464579756(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 192} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2191339870464579756(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2191339870464579756(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2191339870464579756(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2191339870464579756(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2191339870464579756(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2191339870464579756(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2191339870464579756);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2191339870464579756);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2191339870464579756);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__4597513412595167237 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        768 /* Input0 */, \
      192 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4597513412595167237(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 768} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4597513412595167237(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4597513412595167237(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4597513412595167237(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4597513412595167237(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4597513412595167237(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4597513412595167237(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4597513412595167237);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4597513412595167237);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4597513412595167237);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__9320966949310507568 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        192 /* Input0 */, \
      224 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9320966949310507568(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 192} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9320966949310507568(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9320966949310507568(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9320966949310507568(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9320966949310507568(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9320966949310507568(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9320966949310507568(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9320966949310507568);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9320966949310507568);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9320966949310507568);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__18205155717551588727 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        192 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18205155717551588727(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 192} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18205155717551588727(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18205155717551588727(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18205155717551588727(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18205155717551588727(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__18205155717551588727(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18205155717551588727(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18205155717551588727);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18205155717551588727);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__18205155717551588727);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6789528350087717393 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        192 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6789528350087717393(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 192} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6789528350087717393(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6789528350087717393(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6789528350087717393(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6789528350087717393(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6789528350087717393(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6789528350087717393(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6789528350087717393);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6789528350087717393);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6789528350087717393);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4177162388358404921 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        192 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4177162388358404921(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 192} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4177162388358404921(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4177162388358404921(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4177162388358404921(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4177162388358404921(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4177162388358404921(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4177162388358404921(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4177162388358404921);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4177162388358404921);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4177162388358404921);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__18148735114078758932 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        192 /* Input0 */, \
      224 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__18148735114078758932(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 192} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__18148735114078758932(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__18148735114078758932(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__18148735114078758932(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__18148735114078758932(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__18148735114078758932(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__18148735114078758932(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__18148735114078758932);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__18148735114078758932);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__18148735114078758932);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2635463873088024044 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        192 /* Input0 */, \
      224 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2635463873088024044(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 192} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2635463873088024044(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2635463873088024044(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2635463873088024044(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2635463873088024044(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2635463873088024044(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2635463873088024044(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2635463873088024044);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2635463873088024044);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2635463873088024044);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__11197276522190748767 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        192 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11197276522190748767(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 192} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11197276522190748767(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11197276522190748767(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11197276522190748767(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11197276522190748767(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11197276522190748767(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11197276522190748767(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11197276522190748767);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11197276522190748767);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11197276522190748767);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5151257770716461393 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        768 /* Input0 */, \
      192 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5151257770716461393(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 768} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5151257770716461393(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5151257770716461393(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5151257770716461393(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5151257770716461393(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5151257770716461393(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5151257770716461393(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5151257770716461393);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5151257770716461393);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5151257770716461393);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6998582793065432067 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        768 /* Input0 */, \
      192 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6998582793065432067(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 768} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6998582793065432067(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6998582793065432067(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6998582793065432067(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6998582793065432067(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6998582793065432067(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6998582793065432067(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6998582793065432067);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6998582793065432067);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6998582793065432067);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__11386915643734898877 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        768 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11386915643734898877(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 768} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11386915643734898877(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11386915643734898877(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11386915643734898877(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11386915643734898877(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11386915643734898877(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11386915643734898877(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11386915643734898877);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11386915643734898877);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11386915643734898877);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__1943729338616930953 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        768 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      160 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1943729338616930953(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 768} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1943729338616930953(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1943729338616930953(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1943729338616930953(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1943729338616930953(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1943729338616930953(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1943729338616930953(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1943729338616930953);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1943729338616930953);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1943729338616930953);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__18201895151408336223 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        768 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      160 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18201895151408336223(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 768} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__18201895151408336223(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18201895151408336223(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18201895151408336223(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18201895151408336223(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18201895151408336223(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18201895151408336223(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__18201895151408336223);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18201895151408336223);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18201895151408336223);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__8675603906945161067 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        768 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8675603906945161067(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 768} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8675603906945161067(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8675603906945161067(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8675603906945161067(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8675603906945161067(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8675603906945161067(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8675603906945161067(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8675603906945161067);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8675603906945161067);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8675603906945161067);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__13713949029837586939 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        768 /* Input0 */, \
      192 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__13713949029837586939(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 768} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__13713949029837586939(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__13713949029837586939(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__13713949029837586939(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__13713949029837586939(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__13713949029837586939(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__13713949029837586939(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__13713949029837586939);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__13713949029837586939);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__13713949029837586939);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14183598413523043295 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        768 /* Input0 */, \
      192 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14183598413523043295(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 768} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14183598413523043295(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14183598413523043295(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14183598413523043295(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14183598413523043295(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14183598413523043295(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14183598413523043295(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14183598413523043295);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14183598413523043295);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14183598413523043295);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__17998478074168836501 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        768 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17998478074168836501(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 768} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17998478074168836501(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17998478074168836501(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17998478074168836501(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17998478074168836501(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17998478074168836501(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17998478074168836501(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17998478074168836501);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17998478074168836501);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17998478074168836501);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__13095552579897465913 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        12288 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13095552579897465913(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 12288} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13095552579897465913(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13095552579897465913(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13095552579897465913(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13095552579897465913(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13095552579897465913(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13095552579897465913(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13095552579897465913);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13095552579897465913);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13095552579897465913);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10070408172932557599 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        3072 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10070408172932557599(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 3072} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10070408172932557599(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10070408172932557599(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10070408172932557599(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10070408172932557599(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10070408172932557599(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10070408172932557599(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10070408172932557599);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10070408172932557599);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10070408172932557599);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__3706613902726885583 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        3072 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3706613902726885583(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 3072} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3706613902726885583(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3706613902726885583(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3706613902726885583(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3706613902726885583(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3706613902726885583(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3706613902726885583(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3706613902726885583);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3706613902726885583);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3706613902726885583);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12328117435450172137 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        3072 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12328117435450172137(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 3072} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12328117435450172137(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12328117435450172137(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12328117435450172137(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12328117435450172137(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12328117435450172137(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12328117435450172137(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12328117435450172137);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12328117435450172137);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12328117435450172137);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12511636102878074445 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        3072 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12511636102878074445(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 3072} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12511636102878074445(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12511636102878074445(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12511636102878074445(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12511636102878074445(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12511636102878074445(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12511636102878074445(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12511636102878074445);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12511636102878074445);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12511636102878074445);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__15353638711268106009 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        3072 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15353638711268106009(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 3072} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15353638711268106009(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15353638711268106009(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15353638711268106009(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15353638711268106009(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15353638711268106009(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15353638711268106009(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15353638711268106009);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15353638711268106009);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15353638711268106009);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5581606760375094591 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        3072 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5581606760375094591(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 3072} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5581606760375094591(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5581606760375094591(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5581606760375094591(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5581606760375094591(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5581606760375094591(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5581606760375094591(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5581606760375094591);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5581606760375094591);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5581606760375094591);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__782940343366756945 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        3072 /* Input0 */, \
      192 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__782940343366756945(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 3072} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__782940343366756945(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__782940343366756945(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__782940343366756945(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__782940343366756945(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__782940343366756945(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__782940343366756945(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__782940343366756945);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__782940343366756945);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__782940343366756945);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__10573146603444729975 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        3072 /* Input0 */, \
      192 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10573146603444729975(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 3072} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10573146603444729975(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10573146603444729975(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10573146603444729975(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10573146603444729975(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10573146603444729975(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10573146603444729975(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10573146603444729975);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10573146603444729975);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10573146603444729975);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__6093098592884371175 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        3072 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__6093098592884371175(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 3072} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__6093098592884371175(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__6093098592884371175(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__6093098592884371175(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__6093098592884371175(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__6093098592884371175(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__6093098592884371175(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__6093098592884371175);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__6093098592884371175);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__6093098592884371175);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__3317165572865895619 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        3072 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3317165572865895619(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 3072} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3317165572865895619(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3317165572865895619(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3317165572865895619(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3317165572865895619(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3317165572865895619(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3317165572865895619(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3317165572865895619);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3317165572865895619);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3317165572865895619);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__6102811479585245671 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        3072 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6102811479585245671(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 3072} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6102811479585245671(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6102811479585245671(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6102811479585245671(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6102811479585245671(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6102811479585245671(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6102811479585245671(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6102811479585245671);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6102811479585245671);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6102811479585245671);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__16964756741235058751 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        12288 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16964756741235058751(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 12288} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16964756741235058751(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16964756741235058751(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16964756741235058751(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16964756741235058751(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16964756741235058751(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16964756741235058751(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16964756741235058751);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16964756741235058751);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16964756741235058751);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__15167486586073425169 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        12288 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15167486586073425169(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 12288} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15167486586073425169(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15167486586073425169(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15167486586073425169(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15167486586073425169(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15167486586073425169(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15167486586073425169(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15167486586073425169);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15167486586073425169);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15167486586073425169);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12919349795400796019 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        12288 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12919349795400796019(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 12288} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12919349795400796019(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12919349795400796019(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12919349795400796019(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12919349795400796019(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12919349795400796019(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12919349795400796019(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12919349795400796019);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12919349795400796019);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12919349795400796019);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__15101928762142547327 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        12288 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15101928762142547327(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 12288} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15101928762142547327(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15101928762142547327(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15101928762142547327(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15101928762142547327(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15101928762142547327(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15101928762142547327(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15101928762142547327);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15101928762142547327);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15101928762142547327);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__8337968127657946537 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        12288 /* Input0 */, \
      192 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8337968127657946537(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 12288} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8337968127657946537(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8337968127657946537(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8337968127657946537(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8337968127657946537(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8337968127657946537(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8337968127657946537(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8337968127657946537);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8337968127657946537);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8337968127657946537);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__7217889319705327543 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        12288 /* Input0 */, \
      192 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7217889319705327543(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 12288} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7217889319705327543(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7217889319705327543(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7217889319705327543(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7217889319705327543(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7217889319705327543(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7217889319705327543(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7217889319705327543);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7217889319705327543);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7217889319705327543);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__339620107701896689 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        12288 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__339620107701896689(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 12288} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__339620107701896689(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__339620107701896689(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__339620107701896689(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__339620107701896689(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__339620107701896689(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__339620107701896689(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__339620107701896689);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__339620107701896689);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__339620107701896689);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__16895908438023172011 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        12288 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16895908438023172011(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 12288} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16895908438023172011(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16895908438023172011(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16895908438023172011(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16895908438023172011(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16895908438023172011(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16895908438023172011(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16895908438023172011);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16895908438023172011);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16895908438023172011);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__11526970693307769045 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        12288 /* Input0 */, \
      256 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11526970693307769045(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 12288} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11526970693307769045(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11526970693307769045(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11526970693307769045(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11526970693307769045(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11526970693307769045(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11526970693307769045(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11526970693307769045);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11526970693307769045);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11526970693307769045);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__3756180607506410951 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        12288 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__3756180607506410951(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 12288} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__3756180607506410951(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__3756180607506410951(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__3756180607506410951(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__3756180607506410951(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__3756180607506410951(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__3756180607506410951(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__3756180607506410951);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__3756180607506410951);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__3756180607506410951);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__17028562306284522112 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        36864 /* Input0 */, \
      192 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      352 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17028562306284522112(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 36864} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 352} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17028562306284522112(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17028562306284522112(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17028562306284522112(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17028562306284522112(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17028562306284522112(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17028562306284522112(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17028562306284522112);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17028562306284522112);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17028562306284522112);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__14383365121039065194 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        147456 /* Input0 */, \
      352 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14383365121039065194(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 147456} /* Input0 */, 
      {"input[1]", 352} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14383365121039065194(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14383365121039065194(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14383365121039065194(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14383365121039065194(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14383365121039065194(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14383365121039065194(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14383365121039065194);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14383365121039065194);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14383365121039065194);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12610752668790910936 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        36864 /* Input0 */, \
      192 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12610752668790910936(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 36864} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12610752668790910936(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12610752668790910936(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12610752668790910936(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12610752668790910936(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12610752668790910936(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12610752668790910936(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12610752668790910936);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12610752668790910936);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12610752668790910936);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__14639530301465341598 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        36864 /* Input0 */, \
      192 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14639530301465341598(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 36864} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14639530301465341598(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14639530301465341598(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14639530301465341598(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14639530301465341598(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14639530301465341598(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14639530301465341598(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14639530301465341598);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14639530301465341598);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14639530301465341598);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6747703657588446902 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        36864 /* Input0 */, \
      192 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      320 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6747703657588446902(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 36864} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 320} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6747703657588446902(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6747703657588446902(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6747703657588446902(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6747703657588446902(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6747703657588446902(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6747703657588446902(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6747703657588446902);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6747703657588446902);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6747703657588446902);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__8431075372289901424 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        36864 /* Input0 */, \
      320 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8431075372289901424(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 36864} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8431075372289901424(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8431075372289901424(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8431075372289901424(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8431075372289901424(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8431075372289901424(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8431075372289901424(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8431075372289901424);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8431075372289901424);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8431075372289901424);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12942035327743879114 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        36864 /* Input0 */, \
      192 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12942035327743879114(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 36864} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12942035327743879114(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12942035327743879114(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12942035327743879114(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12942035327743879114(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12942035327743879114(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12942035327743879114(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12942035327743879114);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12942035327743879114);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12942035327743879114);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__10124732841574631920 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        36864 /* Input0 */, \
      160 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10124732841574631920(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 36864} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10124732841574631920(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10124732841574631920(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10124732841574631920(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10124732841574631920(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10124732841574631920(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10124732841574631920(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10124732841574631920);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10124732841574631920);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10124732841574631920);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__7316469572888256707 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        36864 /* Input0 */, \
      160 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      224 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7316469572888256707(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 36864} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 224} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7316469572888256707(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7316469572888256707(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7316469572888256707(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7316469572888256707(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7316469572888256707(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7316469572888256707(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7316469572888256707);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7316469572888256707);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7316469572888256707);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__7119952269216754336 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        36864 /* Input0 */, \
      224 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      224 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7119952269216754336(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 36864} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 224} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7119952269216754336(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7119952269216754336(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7119952269216754336(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7119952269216754336(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7119952269216754336(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7119952269216754336(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7119952269216754336);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7119952269216754336);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7119952269216754336);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__10071230132424164243 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        36864 /* Input0 */, \
      224 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10071230132424164243(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 36864} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10071230132424164243(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10071230132424164243(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10071230132424164243(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10071230132424164243(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10071230132424164243(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10071230132424164243(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10071230132424164243);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10071230132424164243);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10071230132424164243);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__5383511098961223520 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        36864 /* Input0 */, \
      192 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__5383511098961223520(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 36864} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__5383511098961223520(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__5383511098961223520(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__5383511098961223520(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__5383511098961223520(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__5383511098961223520(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__5383511098961223520(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__5383511098961223520);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__5383511098961223520);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__5383511098961223520);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10806489361570273944 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        36864 /* Input0 */, \
      192 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10806489361570273944(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 36864} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10806489361570273944(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10806489361570273944(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10806489361570273944(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10806489361570273944(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10806489361570273944(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10806489361570273944(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10806489361570273944);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10806489361570273944);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10806489361570273944);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__7701676924280808998 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        36864 /* Input0 */, \
      128 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7701676924280808998(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 36864} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7701676924280808998(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7701676924280808998(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7701676924280808998(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7701676924280808998(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7701676924280808998(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7701676924280808998(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7701676924280808998);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7701676924280808998);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7701676924280808998);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__16204072764969862260 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        147456 /* Input0 */, \
      352 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      352 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16204072764969862260(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 147456} /* Input0 */, 
      {"input[1]", 352} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 352} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16204072764969862260(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16204072764969862260(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16204072764969862260(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16204072764969862260(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16204072764969862260(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16204072764969862260(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16204072764969862260);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16204072764969862260);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16204072764969862260);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__16932326586733657102 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        589824 /* Input0 */, \
      352 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16932326586733657102(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 589824} /* Input0 */, 
      {"input[1]", 352} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16932326586733657102(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16932326586733657102(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16932326586733657102(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16932326586733657102(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16932326586733657102(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16932326586733657102(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16932326586733657102);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16932326586733657102);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16932326586733657102);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6670543490575844814 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        147456 /* Input0 */, \
      192 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      320 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6670543490575844814(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 147456} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 320} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6670543490575844814(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6670543490575844814(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6670543490575844814(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6670543490575844814(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6670543490575844814(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6670543490575844814(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6670543490575844814);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6670543490575844814);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6670543490575844814);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__8374194116486680584 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        147456 /* Input0 */, \
      320 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8374194116486680584(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 147456} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8374194116486680584(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8374194116486680584(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8374194116486680584(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8374194116486680584(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8374194116486680584(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8374194116486680584(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8374194116486680584);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8374194116486680584);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8374194116486680584);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11795932228439580972 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        147456 /* Input0 */, \
      352 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11795932228439580972(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 147456} /* Input0 */, 
      {"input[1]", 352} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11795932228439580972(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11795932228439580972(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11795932228439580972(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11795932228439580972(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11795932228439580972(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11795932228439580972(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11795932228439580972);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11795932228439580972);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11795932228439580972);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__14563550965917564390 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        147456 /* Input0 */, \
      192 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14563550965917564390(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 147456} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14563550965917564390(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14563550965917564390(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14563550965917564390(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14563550965917564390(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14563550965917564390(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14563550965917564390(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14563550965917564390);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14563550965917564390);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14563550965917564390);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2528547848072254677 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        147456 /* Input0 */, \
      192 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      224 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2528547848072254677(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 147456} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 224} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2528547848072254677(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2528547848072254677(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2528547848072254677(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2528547848072254677(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2528547848072254677(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2528547848072254677(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2528547848072254677);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2528547848072254677);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2528547848072254677);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__7203799601740921304 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        147456 /* Input0 */, \
      224 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      224 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7203799601740921304(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 147456} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 224} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7203799601740921304(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7203799601740921304(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7203799601740921304(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7203799601740921304(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7203799601740921304(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7203799601740921304(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7203799601740921304);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7203799601740921304);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7203799601740921304);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__10012097072530116843 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        147456 /* Input0 */, \
      224 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10012097072530116843(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 147456} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10012097072530116843(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10012097072530116843(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10012097072530116843(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10012097072530116843(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10012097072530116843(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10012097072530116843(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10012097072530116843);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10012097072530116843);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10012097072530116843);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__5046256666209325460 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        147456 /* Input0 */, \
      352 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__5046256666209325460(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 147456} /* Input0 */, 
      {"input[1]", 352} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__5046256666209325460(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__5046256666209325460(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__5046256666209325460(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__5046256666209325460(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__5046256666209325460(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__5046256666209325460(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__5046256666209325460);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__5046256666209325460);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__5046256666209325460);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11045546135996421228 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        147456 /* Input0 */, \
      352 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11045546135996421228(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 147456} /* Input0 */, 
      {"input[1]", 352} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11045546135996421228(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11045546135996421228(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11045546135996421228(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11045546135996421228(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11045546135996421228(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11045546135996421228(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11045546135996421228);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11045546135996421228);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11045546135996421228);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__7770949151926007134 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        147456 /* Input0 */, \
      128 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7770949151926007134(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 147456} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7770949151926007134(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7770949151926007134(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7770949151926007134(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7770949151926007134(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7770949151926007134(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7770949151926007134(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7770949151926007134);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7770949151926007134);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7770949151926007134);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__7739340014759513584 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        589824 /* Input0 */, \
      352 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__7739340014759513584(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 589824} /* Input0 */, 
      {"input[1]", 352} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__7739340014759513584(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__7739340014759513584(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__7739340014759513584(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__7739340014759513584(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__7739340014759513584(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__7739340014759513584(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__7739340014759513584);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__7739340014759513584);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__7739340014759513584);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14423597981328172164 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14423597981328172164(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14423597981328172164(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14423597981328172164(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14423597981328172164(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14423597981328172164(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14423597981328172164(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14423597981328172164(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14423597981328172164);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14423597981328172164);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14423597981328172164);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__380821973141987362 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      512 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__380821973141987362(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__380821973141987362(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__380821973141987362(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__380821973141987362(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__380821973141987362(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__380821973141987362(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__380821973141987362(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__380821973141987362);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__380821973141987362);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__380821973141987362);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__18096322697980054786 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18096322697980054786(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18096322697980054786(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18096322697980054786(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18096322697980054786(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18096322697980054786(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__18096322697980054786(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18096322697980054786(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18096322697980054786);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18096322697980054786);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__18096322697980054786);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__13883506285853367397 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      1024 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13883506285853367397(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13883506285853367397(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13883506285853367397(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13883506285853367397(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13883506285853367397(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13883506285853367397(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13883506285853367397(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13883506285853367397);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13883506285853367397);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13883506285853367397);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__18162799897504473460 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      512 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18162799897504473460(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18162799897504473460(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18162799897504473460(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18162799897504473460(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18162799897504473460(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__18162799897504473460(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18162799897504473460(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18162799897504473460);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18162799897504473460);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__18162799897504473460);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__4626204101633552482 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      128 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4626204101633552482(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4626204101633552482(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4626204101633552482(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4626204101633552482(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4626204101633552482(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4626204101633552482(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4626204101633552482(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4626204101633552482);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4626204101633552482);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4626204101633552482);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__18318123974799177623 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      128 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18318123974799177623(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__18318123974799177623(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18318123974799177623(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18318123974799177623(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18318123974799177623(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18318123974799177623(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18318123974799177623(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__18318123974799177623);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18318123974799177623);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18318123974799177623);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__18121205949240288277 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        2 /* Input0 */, \
      128 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18121205949240288277(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18121205949240288277(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18121205949240288277(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18121205949240288277(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18121205949240288277(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__18121205949240288277(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18121205949240288277(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18121205949240288277);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18121205949240288277);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__18121205949240288277);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5007981215488386016 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        2 /* Input0 */, \
      128 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5007981215488386016(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5007981215488386016(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5007981215488386016(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5007981215488386016(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5007981215488386016(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5007981215488386016(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5007981215488386016(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5007981215488386016);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5007981215488386016);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5007981215488386016);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__60671829790895923 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        4 /* Input0 */, \
      128 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__60671829790895923(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__60671829790895923(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__60671829790895923(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__60671829790895923(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__60671829790895923(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__60671829790895923(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__60671829790895923(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__60671829790895923);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__60671829790895923);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__60671829790895923);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__13750356180933617862 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        4 /* Input0 */, \
      128 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13750356180933617862(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13750356180933617862(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13750356180933617862(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13750356180933617862(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13750356180933617862(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13750356180933617862(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13750356180933617862(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13750356180933617862);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13750356180933617862);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13750356180933617862);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__13944690268576461954 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        8 /* Input0 */, \
      128 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13944690268576461954(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 8} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13944690268576461954(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13944690268576461954(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13944690268576461954(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13944690268576461954(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13944690268576461954(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13944690268576461954(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13944690268576461954);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13944690268576461954);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13944690268576461954);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__9187876812019118967 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        8 /* Input0 */, \
      128 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9187876812019118967(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 8} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9187876812019118967(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9187876812019118967(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9187876812019118967(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9187876812019118967(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9187876812019118967(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9187876812019118967(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9187876812019118967);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9187876812019118967);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9187876812019118967);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__8340964249300986826 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      128 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8340964249300986826(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8340964249300986826(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8340964249300986826(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8340964249300986826(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8340964249300986826(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8340964249300986826(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8340964249300986826(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8340964249300986826);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8340964249300986826);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8340964249300986826);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14827422721188260927 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      128 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14827422721188260927(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14827422721188260927(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14827422721188260927(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14827422721188260927(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14827422721188260927(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14827422721188260927(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14827422721188260927(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14827422721188260927);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14827422721188260927);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14827422721188260927);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__2633804191503976392 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        32 /* Input0 */, \
      128 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2633804191503976392(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 32} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2633804191503976392(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2633804191503976392(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2633804191503976392(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2633804191503976392(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2633804191503976392(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2633804191503976392(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2633804191503976392);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2633804191503976392);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2633804191503976392);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11135640839213908029 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        32 /* Input0 */, \
      128 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11135640839213908029(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 32} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11135640839213908029(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11135640839213908029(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11135640839213908029(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11135640839213908029(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11135640839213908029(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11135640839213908029(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11135640839213908029);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11135640839213908029);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11135640839213908029);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__15044674088201672575 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        64 /* Input0 */, \
      64 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15044674088201672575(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15044674088201672575(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15044674088201672575(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15044674088201672575(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15044674088201672575(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15044674088201672575(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15044674088201672575(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15044674088201672575);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15044674088201672575);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15044674088201672575);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10239840509892715385 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        64 /* Input0 */, \
      64 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10239840509892715385(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10239840509892715385(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10239840509892715385(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10239840509892715385(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10239840509892715385(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10239840509892715385(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10239840509892715385(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10239840509892715385);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10239840509892715385);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10239840509892715385);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__1080724625792208832 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        64 /* Input0 */, \
      128 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__1080724625792208832(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__1080724625792208832(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__1080724625792208832(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__1080724625792208832(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__1080724625792208832(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__1080724625792208832(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__1080724625792208832(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__1080724625792208832);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__1080724625792208832);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__1080724625792208832);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9431470312600769094 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        64 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9431470312600769094(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9431470312600769094(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9431470312600769094(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9431470312600769094(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9431470312600769094(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9431470312600769094(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9431470312600769094(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9431470312600769094);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9431470312600769094);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9431470312600769094);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4388697270336099763 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        64 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4388697270336099763(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4388697270336099763(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4388697270336099763(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4388697270336099763(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4388697270336099763(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4388697270336099763(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4388697270336099763(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4388697270336099763);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4388697270336099763);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4388697270336099763);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__2572509882556750475 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        128 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2572509882556750475(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 128} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2572509882556750475(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2572509882556750475(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2572509882556750475(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2572509882556750475(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2572509882556750475(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2572509882556750475(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2572509882556750475);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2572509882556750475);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2572509882556750475);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11364830047598812542 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        128 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11364830047598812542(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 128} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11364830047598812542(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11364830047598812542(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11364830047598812542(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11364830047598812542(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11364830047598812542(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11364830047598812542(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11364830047598812542);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11364830047598812542);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11364830047598812542);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__16468327665703782027 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        256 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16468327665703782027(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16468327665703782027(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16468327665703782027(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16468327665703782027(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16468327665703782027(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16468327665703782027(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16468327665703782027(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16468327665703782027);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16468327665703782027);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16468327665703782027);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6525672358251397502 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        256 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6525672358251397502(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6525672358251397502(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6525672358251397502(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6525672358251397502(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6525672358251397502(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6525672358251397502(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6525672358251397502(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6525672358251397502);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6525672358251397502);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6525672358251397502);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__200159774233907013 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        512 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__200159774233907013(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 512} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__200159774233907013(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__200159774233907013(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__200159774233907013(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__200159774233907013(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__200159774233907013(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__200159774233907013(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__200159774233907013);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__200159774233907013);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__200159774233907013);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__13601861449485043888 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        512 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13601861449485043888(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 512} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13601861449485043888(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13601861449485043888(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13601861449485043888(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13601861449485043888(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13601861449485043888(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13601861449485043888(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13601861449485043888);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13601861449485043888);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13601861449485043888);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__17890986358372535564 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1024 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17890986358372535564(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17890986358372535564(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17890986358372535564(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17890986358372535564(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17890986358372535564(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17890986358372535564(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17890986358372535564(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17890986358372535564);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17890986358372535564);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17890986358372535564);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5065729209038375673 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1024 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5065729209038375673(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5065729209038375673(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5065729209038375673(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5065729209038375673(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5065729209038375673(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5065729209038375673(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5065729209038375673(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5065729209038375673);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5065729209038375673);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5065729209038375673);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__4042673894195413191 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        2048 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4042673894195413191(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2048} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4042673894195413191(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4042673894195413191(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4042673894195413191(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4042673894195413191(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4042673894195413191(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4042673894195413191(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4042673894195413191);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4042673894195413191);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4042673894195413191);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__9664424435614566194 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        2048 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9664424435614566194(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2048} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9664424435614566194(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9664424435614566194(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9664424435614566194(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9664424435614566194(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9664424435614566194(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9664424435614566194(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9664424435614566194);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9664424435614566194);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9664424435614566194);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__6347904362244645228 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        4096 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6347904362244645228(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6347904362244645228(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6347904362244645228(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6347904362244645228(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6347904362244645228(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6347904362244645228(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6347904362244645228(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6347904362244645228);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6347904362244645228);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6347904362244645228);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__16581358618257172121 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        4096 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16581358618257172121(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16581358618257172121(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16581358618257172121(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16581358618257172121(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16581358618257172121(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16581358618257172121(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16581358618257172121(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16581358618257172121);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16581358618257172121);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16581358618257172121);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__8290499122910675541 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        8192 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8290499122910675541(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 8192} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8290499122910675541(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8290499122910675541(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8290499122910675541(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8290499122910675541(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8290499122910675541(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8290499122910675541(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8290499122910675541);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8290499122910675541);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8290499122910675541);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14774443044856743328 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        8192 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14774443044856743328(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 8192} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14774443044856743328(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14774443044856743328(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14774443044856743328(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14774443044856743328(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14774443044856743328(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14774443044856743328(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14774443044856743328);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14774443044856743328);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14774443044856743328);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__8865331196003149232 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        16384 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8865331196003149232(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8865331196003149232(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8865331196003149232(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8865331196003149232(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8865331196003149232(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8865331196003149232(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8865331196003149232(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8865331196003149232);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8865331196003149232);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8865331196003149232);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14196299773104808517 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        16384 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14196299773104808517(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14196299773104808517(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14196299773104808517(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14196299773104808517(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14196299773104808517(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14196299773104808517(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14196299773104808517(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14196299773104808517);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14196299773104808517);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14196299773104808517);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__4576952632304074870 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        32768 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4576952632304074870(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 32768} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4576952632304074870(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4576952632304074870(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4576952632304074870(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4576952632304074870(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4576952632304074870(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4576952632304074870(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4576952632304074870);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4576952632304074870);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4576952632304074870);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__9331461204195673987 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        32768 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9331461204195673987(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 32768} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9331461204195673987(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9331461204195673987(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9331461204195673987(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9331461204195673987(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9331461204195673987(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9331461204195673987(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9331461204195673987);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9331461204195673987);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9331461204195673987);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__6949274926185170917 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        65536 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6949274926185170917(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6949274926185170917(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6949274926185170917(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6949274926185170917(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6949274926185170917(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6949274926185170917(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6949274926185170917(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6949274926185170917);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6949274926185170917);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6949274926185170917);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__16029525023784026128 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        65536 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16029525023784026128(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16029525023784026128(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16029525023784026128(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16029525023784026128(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16029525023784026128(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16029525023784026128(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16029525023784026128(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16029525023784026128);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16029525023784026128);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16029525023784026128);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6678057746326470081 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        262144 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6678057746326470081(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 262144} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6678057746326470081(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6678057746326470081(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6678057746326470081(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6678057746326470081(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6678057746326470081(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6678057746326470081(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6678057746326470081);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6678057746326470081);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6678057746326470081);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__5441634106462936897 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        131072 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5441634106462936897(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 131072} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5441634106462936897(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5441634106462936897(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5441634106462936897(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5441634106462936897(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5441634106462936897(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5441634106462936897(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5441634106462936897);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5441634106462936897);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5441634106462936897);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__17690150119706826932 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        131072 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17690150119706826932(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 131072} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17690150119706826932(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17690150119706826932(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17690150119706826932(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17690150119706826932(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17690150119706826932(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17690150119706826932(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17690150119706826932);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17690150119706826932);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17690150119706826932);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__179978836074695111 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        262144 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__179978836074695111(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 262144} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__179978836074695111(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__179978836074695111(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__179978836074695111(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__179978836074695111(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__179978836074695111(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__179978836074695111(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__179978836074695111);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__179978836074695111);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__179978836074695111);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__16838043435438407865 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        262144 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16838043435438407865(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 262144} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16838043435438407865(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16838043435438407865(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16838043435438407865(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16838043435438407865(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16838043435438407865(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16838043435438407865(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16838043435438407865);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16838043435438407865);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16838043435438407865);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__6136253588520835289 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        262144 /* Input0 */, \
      256 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__6136253588520835289(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 262144} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__6136253588520835289(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__6136253588520835289(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__6136253588520835289(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__6136253588520835289(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__6136253588520835289(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__6136253588520835289(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__6136253588520835289);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__6136253588520835289);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__6136253588520835289);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__701447856375676859 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        262144 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__701447856375676859(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 262144} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__701447856375676859(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__701447856375676859(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__701447856375676859(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__701447856375676859(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__701447856375676859(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__701447856375676859(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__701447856375676859);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__701447856375676859);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__701447856375676859);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__13238493518282963022 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        262144 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13238493518282963022(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 262144} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13238493518282963022(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13238493518282963022(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13238493518282963022(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13238493518282963022(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13238493518282963022(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13238493518282963022(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13238493518282963022);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13238493518282963022);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13238493518282963022);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__4413782101667213025 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        524288 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4413782101667213025(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 524288} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4413782101667213025(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4413782101667213025(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4413782101667213025(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4413782101667213025(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4413782101667213025(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4413782101667213025(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4413782101667213025);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4413782101667213025);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4413782101667213025);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__9458825591017171220 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        524288 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9458825591017171220(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 524288} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9458825591017171220(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9458825591017171220(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9458825591017171220(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9458825591017171220(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9458825591017171220(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9458825591017171220(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9458825591017171220);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9458825591017171220);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9458825591017171220);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__5871638140445428216 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1048576 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5871638140445428216(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1048576} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5871638140445428216(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5871638140445428216(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5871638140445428216(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5871638140445428216(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5871638140445428216(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5871638140445428216(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5871638140445428216);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5871638140445428216);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5871638140445428216);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__17255726049061801485 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1048576 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17255726049061801485(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1048576} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17255726049061801485(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17255726049061801485(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17255726049061801485(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17255726049061801485(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17255726049061801485(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17255726049061801485(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17255726049061801485);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17255726049061801485);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17255726049061801485);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__5777096894175630204 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        2097152 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5777096894175630204(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2097152} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5777096894175630204(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5777096894175630204(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5777096894175630204(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5777096894175630204(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5777096894175630204(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5777096894175630204(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5777096894175630204);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5777096894175630204);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5777096894175630204);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__17161185910628537481 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        2097152 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17161185910628537481(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2097152} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17161185910628537481(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17161185910628537481(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17161185910628537481(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17161185910628537481(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17161185910628537481(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17161185910628537481(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17161185910628537481);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17161185910628537481);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17161185910628537481);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__8548563220938225528 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        4194304 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8548563220938225528(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4194304} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8548563220938225528(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8548563220938225528(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8548563220938225528(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8548563220938225528(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8548563220938225528(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8548563220938225528(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8548563220938225528);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8548563220938225528);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8548563220938225528);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14458244032596799629 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        4194304 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14458244032596799629(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4194304} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14458244032596799629(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14458244032596799629(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14458244032596799629(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14458244032596799629(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14458244032596799629(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14458244032596799629(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14458244032596799629);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14458244032596799629);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14458244032596799629);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__2522051169291438851 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        8388608 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2522051169291438851(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 8388608} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2522051169291438851(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2522051169291438851(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2522051169291438851(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2522051169291438851(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2522051169291438851(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2522051169291438851(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2522051169291438851);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2522051169291438851);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2522051169291438851);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11314371024559246582 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        8388608 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11314371024559246582(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 8388608} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11314371024559246582(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11314371024559246582(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11314371024559246582(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11314371024559246582(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11314371024559246582(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11314371024559246582(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11314371024559246582);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11314371024559246582);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11314371024559246582);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__4841525113631916823 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        16777216 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4841525113631916823(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16777216} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4841525113631916823(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4841525113631916823(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4841525113631916823(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4841525113631916823(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4841525113631916823(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4841525113631916823(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4841525113631916823);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4841525113631916823);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4841525113631916823);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__18245232030457775330 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        16777216 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18245232030457775330(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16777216} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__18245232030457775330(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18245232030457775330(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18245232030457775330(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18245232030457775330(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18245232030457775330(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18245232030457775330(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__18245232030457775330);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18245232030457775330);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18245232030457775330);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9281842452980317966 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        33554432 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9281842452980317966(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 33554432} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9281842452980317966(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9281842452980317966(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9281842452980317966(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9281842452980317966(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9281842452980317966(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9281842452980317966(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9281842452980317966);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9281842452980317966);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9281842452980317966);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4524747795653081339 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        33554432 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4524747795653081339(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 33554432} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4524747795653081339(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4524747795653081339(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4524747795653081339(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4524747795653081339(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4524747795653081339(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4524747795653081339(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4524747795653081339);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4524747795653081339);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4524747795653081339);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__1815101788786045598 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        67108864 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1815101788786045598(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 67108864} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1815101788786045598(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1815101788786045598(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1815101788786045598(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1815101788786045598(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1815101788786045598(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1815101788786045598(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1815101788786045598);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1815101788786045598);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1815101788786045598);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12048274526743475563 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        67108864 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12048274526743475563(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 67108864} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12048274526743475563(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12048274526743475563(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12048274526743475563(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12048274526743475563(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12048274526743475563(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12048274526743475563(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12048274526743475563);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12048274526743475563);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12048274526743475563);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__4346439624821747000 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        134217728 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4346439624821747000(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 134217728} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4346439624821747000(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4346439624821747000(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4346439624821747000(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4346439624821747000(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4346439624821747000(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4346439624821747000(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4346439624821747000);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4346439624821747000);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4346439624821747000);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__9391482289399311053 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        134217728 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9391482289399311053(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 134217728} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9391482289399311053(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9391482289399311053(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9391482289399311053(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9391482289399311053(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9391482289399311053(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9391482289399311053(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9391482289399311053);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9391482289399311053);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9391482289399311053);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__17993366208959740095 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        268435456 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17993366208959740095(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 268435456} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17993366208959740095(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17993366208959740095(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17993366208959740095(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17993366208959740095(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17993366208959740095(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17993366208959740095(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17993366208959740095);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17993366208959740095);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17993366208959740095);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5166156086459629386 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        268435456 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5166156086459629386(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 268435456} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5166156086459629386(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5166156086459629386(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5166156086459629386(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5166156086459629386(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5166156086459629386(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5166156086459629386(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5166156086459629386);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5166156086459629386);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5166156086459629386);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__4791733686646389937 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        536870912 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4791733686646389937(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 536870912} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4791733686646389937(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4791733686646389937(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4791733686646389937(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4791733686646389937(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4791733686646389937(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4791733686646389937(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4791733686646389937);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4791733686646389937);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4791733686646389937);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__18193188769337621316 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        536870912 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18193188769337621316(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 536870912} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__18193188769337621316(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18193188769337621316(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18193188769337621316(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18193188769337621316(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18193188769337621316(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18193188769337621316(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__18193188769337621316);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18193188769337621316);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18193188769337621316);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__15355107261753539898 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1073741824 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15355107261753539898(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1073741824} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15355107261753539898(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15355107261753539898(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15355107261753539898(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15355107261753539898(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15355107261753539898(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15355107261753539898(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15355107261753539898);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15355107261753539898);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15355107261753539898);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__7718277087223429839 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1073741824 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7718277087223429839(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1073741824} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7718277087223429839(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7718277087223429839(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7718277087223429839(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7718277087223429839(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7718277087223429839(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7718277087223429839(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7718277087223429839);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7718277087223429839);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7718277087223429839);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__13163905288147392456 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        2147483648 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13163905288147392456(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2147483648} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13163905288147392456(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13163905288147392456(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13163905288147392456(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13163905288147392456(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13163905288147392456(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13163905288147392456(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13163905288147392456);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13163905288147392456);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13163905288147392456);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__624609122427555901 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        2147483648 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__624609122427555901(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2147483648} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__624609122427555901(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__624609122427555901(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__624609122427555901(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__624609122427555901(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__624609122427555901(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__624609122427555901(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__624609122427555901);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__624609122427555901);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__624609122427555901);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__11861766074168619450 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        4294967296 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11861766074168619450(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4294967296} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11861766074168619450(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11861766074168619450(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11861766074168619450(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11861766074168619450(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11861766074168619450(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11861766074168619450(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11861766074168619450);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11861766074168619450);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11861766074168619450);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__1916824563274200655 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        4294967296 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1916824563274200655(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4294967296} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1916824563274200655(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1916824563274200655(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1916824563274200655(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1916824563274200655(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1916824563274200655(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1916824563274200655(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1916824563274200655);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1916824563274200655);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1916824563274200655);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__7856732817916443410 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        8589934592 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7856732817916443410(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 8589934592} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7856732817916443410(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7856732817916443410(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7856732817916443410(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7856732817916443410(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7856732817916443410(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7856732817916443410(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7856732817916443410);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7856732817916443410);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7856732817916443410);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__15207565716765687015 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        8589934592 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15207565716765687015(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 8589934592} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15207565716765687015(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15207565716765687015(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15207565716765687015(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15207565716765687015(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15207565716765687015(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15207565716765687015(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15207565716765687015);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15207565716765687015);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15207565716765687015);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__12223203392688328379 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        17179869184 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12223203392688328379(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 17179869184} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12223203392688328379(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12223203392688328379(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12223203392688328379(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12223203392688328379(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12223203392688328379(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12223203392688328379(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12223203392688328379);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12223203392688328379);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12223203392688328379);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__1703787912541473102 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        17179869184 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1703787912541473102(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 17179869184} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1703787912541473102(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1703787912541473102(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1703787912541473102(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1703787912541473102(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1703787912541473102(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1703787912541473102(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1703787912541473102);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1703787912541473102);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1703787912541473102);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__8408517591343922003 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        34359738368 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8408517591343922003(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 34359738368} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8408517591343922003(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8408517591343922003(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8408517591343922003(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8408517591343922003(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8408517591343922003(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8408517591343922003(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8408517591343922003);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8408517591343922003);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8408517591343922003);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14606746579487915174 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        34359738368 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14606746579487915174(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 34359738368} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14606746579487915174(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14606746579487915174(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14606746579487915174(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14606746579487915174(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14606746579487915174(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14606746579487915174(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14606746579487915174);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14606746579487915174);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14606746579487915174);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__15856010768525276576 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        68719476736 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15856010768525276576(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 68719476736} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15856010768525276576(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15856010768525276576(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15856010768525276576(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15856010768525276576(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15856010768525276576(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15856010768525276576(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15856010768525276576);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15856010768525276576);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15856010768525276576);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__7065942637133279829 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        68719476736 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7065942637133279829(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 68719476736} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7065942637133279829(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7065942637133279829(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7065942637133279829(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7065942637133279829(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7065942637133279829(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7065942637133279829(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7065942637133279829);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7065942637133279829);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7065942637133279829);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__17112959865347206042 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        137438953472 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17112959865347206042(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 137438953472} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17112959865347206042(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17112959865347206042(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17112959865347206042(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17112959865347206042(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17112959865347206042(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17112959865347206042(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17112959865347206042);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17112959865347206042);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17112959865347206042);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6015097134521740399 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        137438953472 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6015097134521740399(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 137438953472} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6015097134521740399(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6015097134521740399(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6015097134521740399(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6015097134521740399(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6015097134521740399(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6015097134521740399(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6015097134521740399);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6015097134521740399);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6015097134521740399);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__17405147481193939750 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        274877906944 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17405147481193939750(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 274877906944} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17405147481193939750(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17405147481193939750(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17405147481193939750(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17405147481193939750(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17405147481193939750(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17405147481193939750(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17405147481193939750);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17405147481193939750);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17405147481193939750);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5732829196694291667 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        274877906944 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5732829196694291667(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 274877906944} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5732829196694291667(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5732829196694291667(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5732829196694291667(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5732829196694291667(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5732829196694291667(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5732829196694291667(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5732829196694291667);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5732829196694291667);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5732829196694291667);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__16894565104063340014 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        549755813888 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16894565104063340014(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 549755813888} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16894565104063340014(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16894565104063340014(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16894565104063340014(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16894565104063340014(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16894565104063340014(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16894565104063340014(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16894565104063340014);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16894565104063340014);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16894565104063340014);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6084668616178784795 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        549755813888 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6084668616178784795(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 549755813888} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6084668616178784795(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6084668616178784795(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6084668616178784795(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6084668616178784795(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6084668616178784795(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6084668616178784795(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6084668616178784795);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6084668616178784795);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6084668616178784795);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__16404344573107983280 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1099511627776 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16404344573107983280(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1099511627776} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16404344573107983280(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16404344573107983280(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16404344573107983280(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16404344573107983280(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16404344573107983280(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16404344573107983280(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16404344573107983280);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16404344573107983280);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16404344573107983280);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6747351928891462725 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1099511627776 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6747351928891462725(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1099511627776} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6747351928891462725(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6747351928891462725(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6747351928891462725(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6747351928891462725(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6747351928891462725(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6747351928891462725(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6747351928891462725);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6747351928891462725);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6747351928891462725);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__3485889975373327477 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        4398046511104 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3485889975373327477(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4398046511104} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3485889975373327477(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3485889975373327477(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3485889975373327477(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3485889975373327477(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3485889975373327477(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3485889975373327477(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3485889975373327477);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3485889975373327477);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3485889975373327477);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9538302415625344378 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        2199023255552 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9538302415625344378(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2199023255552} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9538302415625344378(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9538302415625344378(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9538302415625344378(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9538302415625344378(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9538302415625344378(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9538302415625344378(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9538302415625344378);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9538302415625344378);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9538302415625344378);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4204747075385250447 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        2199023255552 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4204747075385250447(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2199023255552} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4204747075385250447(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4204747075385250447(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4204747075385250447(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4204747075385250447(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4204747075385250447(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4204747075385250447(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4204747075385250447);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4204747075385250447);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4204747075385250447);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__7976452845039736947 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        4398046511104 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7976452845039736947(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4398046511104} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7976452845039736947(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7976452845039736947(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7976452845039736947(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7976452845039736947(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7976452845039736947(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7976452845039736947(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7976452845039736947);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7976452845039736947);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7976452845039736947);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11274389816593092323 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        4398046511104 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11274389816593092323(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4398046511104} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11274389816593092323(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11274389816593092323(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11274389816593092323(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11274389816593092323(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11274389816593092323(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11274389816593092323(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11274389816593092323);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11274389816593092323);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11274389816593092323);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__18056682424060644819 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        4398046511104 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__18056682424060644819(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4398046511104} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__18056682424060644819(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__18056682424060644819(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__18056682424060644819(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__18056682424060644819(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__18056682424060644819(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__18056682424060644819(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__18056682424060644819);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__18056682424060644819);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__18056682424060644819);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9191348167843218079 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        4398046511104 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9191348167843218079(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4398046511104} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9191348167843218079(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9191348167843218079(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9191348167843218079(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9191348167843218079(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9191348167843218079(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9191348167843218079(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9191348167843218079);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9191348167843218079);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9191348167843218079);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__13946138248092943722 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        4398046511104 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13946138248092943722(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4398046511104} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13946138248092943722(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13946138248092943722(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13946138248092943722(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13946138248092943722(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13946138248092943722(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13946138248092943722(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13946138248092943722);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13946138248092943722);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13946138248092943722);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__13618478890718473383 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        8796093022208 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13618478890718473383(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 8796093022208} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13618478890718473383(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13618478890718473383(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13618478890718473383(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13618478890718473383(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13618478890718473383(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13618478890718473383(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13618478890718473383);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13618478890718473383);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13618478890718473383);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__214508982839290706 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        8796093022208 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__214508982839290706(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 8796093022208} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__214508982839290706(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__214508982839290706(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__214508982839290706(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__214508982839290706(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__214508982839290706(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__214508982839290706(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__214508982839290706);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__214508982839290706);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__214508982839290706);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__7647252092028959009 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        17592186044416 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7647252092028959009(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 17592186044416} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7647252092028959009(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7647252092028959009(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7647252092028959009(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7647252092028959009(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7647252092028959009(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7647252092028959009(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7647252092028959009);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7647252092028959009);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7647252092028959009);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__15286598053047680724 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        17592186044416 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15286598053047680724(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 17592186044416} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15286598053047680724(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15286598053047680724(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15286598053047680724(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15286598053047680724(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15286598053047680724(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15286598053047680724(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15286598053047680724);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15286598053047680724);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15286598053047680724);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__3354497438964768899 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        35184372088832 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3354497438964768899(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 35184372088832} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3354497438964768899(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3354497438964768899(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3354497438964768899(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3354497438964768899(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3354497438964768899(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3354497438964768899(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3354497438964768899);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3354497438964768899);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3354497438964768899);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10414865547066468214 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        35184372088832 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10414865547066468214(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 35184372088832} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10414865547066468214(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10414865547066468214(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10414865547066468214(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10414865547066468214(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10414865547066468214(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10414865547066468214(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10414865547066468214);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10414865547066468214);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10414865547066468214);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__7447182248544172785 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        70368744177664 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7447182248544172785(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 70368744177664} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7447182248544172785(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7447182248544172785(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7447182248544172785(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7447182248544172785(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7447182248544172785(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7447182248544172785(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7447182248544172785);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7447182248544172785);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7447182248544172785);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__15662723875986996484 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        70368744177664 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15662723875986996484(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 70368744177664} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15662723875986996484(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15662723875986996484(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15662723875986996484(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15662723875986996484(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15662723875986996484(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15662723875986996484(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15662723875986996484);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15662723875986996484);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15662723875986996484);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__3026528666424204556 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        140737488355328 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3026528666424204556(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 140737488355328} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3026528666424204556(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3026528666424204556(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3026528666424204556(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3026528666424204556(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3026528666424204556(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3026528666424204556(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3026528666424204556);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3026528666424204556);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3026528666424204556);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10665926225742682873 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        140737488355328 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10665926225742682873(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 140737488355328} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10665926225742682873(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10665926225742682873(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10665926225742682873(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10665926225742682873(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10665926225742682873(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10665926225742682873(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10665926225742682873);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10665926225742682873);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10665926225742682873);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__14868940001010486903 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        281474976710656 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14868940001010486903(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 281474976710656} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14868940001010486903(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14868940001010486903(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14868940001010486903(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14868940001010486903(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14868940001010486903(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14868940001010486903(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14868940001010486903);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14868940001010486903);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14868940001010486903);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8096784436491291010 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        281474976710656 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8096784436491291010(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 281474976710656} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8096784436491291010(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8096784436491291010(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8096784436491291010(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8096784436491291010(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8096784436491291010(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8096784436491291010(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8096784436491291010);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8096784436491291010);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8096784436491291010);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9278234059819235023 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        562949953421312 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9278234059819235023(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 562949953421312} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9278234059819235023(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9278234059819235023(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9278234059819235023(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9278234059819235023(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9278234059819235023(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9278234059819235023(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9278234059819235023);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9278234059819235023);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9278234059819235023);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4523426111830274362 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        562949953421312 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4523426111830274362(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 562949953421312} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4523426111830274362(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4523426111830274362(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4523426111830274362(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4523426111830274362(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4523426111830274362(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4523426111830274362(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4523426111830274362);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4523426111830274362);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4523426111830274362);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__11740871331532226879 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1125899906842624 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11740871331532226879(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1125899906842624} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11740871331532226879(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11740871331532226879(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11740871331532226879(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11740871331532226879(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11740871331532226879(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11740871331532226879(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11740871331532226879);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11740871331532226879);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11740871331532226879);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2083878996556236490 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1125899906842624 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2083878996556236490(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1125899906842624} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2083878996556236490(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2083878996556236490(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2083878996556236490(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2083878996556236490(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2083878996556236490(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2083878996556236490(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2083878996556236490);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2083878996556236490);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2083878996556236490);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__5053537359998107643 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        2251799813685248 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5053537359998107643(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2251799813685248} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5053537359998107643(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5053537359998107643(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5053537359998107643(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5053537359998107643(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5053537359998107643(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5053537359998107643(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5053537359998107643);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5053537359998107643);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5053537359998107643);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__17880801084229826574 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        2251799813685248 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17880801084229826574(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2251799813685248} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17880801084229826574(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17880801084229826574(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17880801084229826574(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17880801084229826574(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17880801084229826574(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17880801084229826574(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17880801084229826574);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17880801084229826574);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17880801084229826574);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__3264519300549663088 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        4503599627370496 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3264519300549663088(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4503599627370496} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3264519300549663088(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3264519300549663088(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3264519300549663088(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3264519300549663088(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3264519300549663088(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3264519300549663088(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3264519300549663088);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3264519300549663088);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3264519300549663088);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10613154376983862917 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        4503599627370496 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10613154376983862917(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4503599627370496} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10613154376983862917(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10613154376983862917(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10613154376983862917(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10613154376983862917(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10613154376983862917(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10613154376983862917(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10613154376983862917);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10613154376983862917);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10613154376983862917);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__608693216872211582 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        9007199254740992 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__608693216872211582(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 9007199254740992} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__608693216872211582(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__608693216872211582(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__608693216872211582(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__608693216872211582(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__608693216872211582(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__608693216872211582(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__608693216872211582);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__608693216872211582);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__608693216872211582);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__13147726863754437515 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        9007199254740992 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13147726863754437515(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 9007199254740992} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13147726863754437515(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13147726863754437515(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13147726863754437515(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13147726863754437515(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13147726863754437515(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13147726863754437515(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13147726863754437515);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13147726863754437515);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13147726863754437515);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__3385727348700422738 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        18014398509481984 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3385727348700422738(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 18014398509481984} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3385727348700422738(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3385727348700422738(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3385727348700422738(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3385727348700422738(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3385727348700422738(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3385727348700422738(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3385727348700422738);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3385727348700422738);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3385727348700422738);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10446132015701370279 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        18014398509481984 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10446132015701370279(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 18014398509481984} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10446132015701370279(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10446132015701370279(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10446132015701370279(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10446132015701370279(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10446132015701370279(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10446132015701370279(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10446132015701370279);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10446132015701370279);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10446132015701370279);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__16363437958560822832 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        36028797018963968 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16363437958560822832(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 36028797018963968} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16363437958560822832(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16363437958560822832(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16363437958560822832(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16363437958560822832(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16363437958560822832(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16363437958560822832(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16363437958560822832);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16363437958560822832);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16363437958560822832);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6706427800708133317 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        36028797018963968 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6706427800708133317(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 36028797018963968} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6706427800708133317(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6706427800708133317(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6706427800708133317(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6706427800708133317(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6706427800708133317(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6706427800708133317(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6706427800708133317);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6706427800708133317);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6706427800708133317);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__12008694873354315603 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        72057594037927936 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12008694873354315603(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 72057594037927936} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12008694873354315603(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12008694873354315603(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12008694873354315603(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12008694873354315603(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12008694873354315603(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12008694873354315603(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12008694873354315603);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12008694873354315603);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12008694873354315603);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__1775523202030411942 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        72057594037927936 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1775523202030411942(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 72057594037927936} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1775523202030411942(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1775523202030411942(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1775523202030411942(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1775523202030411942(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1775523202030411942(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1775523202030411942(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1775523202030411942);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1775523202030411942);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1775523202030411942);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__7771360959227830460 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        288230376151711744 /* Input0 */, \
      512 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7771360959227830460(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 288230376151711744} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7771360959227830460(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7771360959227830460(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7771360959227830460(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7771360959227830460(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7771360959227830460(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7771360959227830460(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7771360959227830460);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7771360959227830460);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7771360959227830460);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__16208559625228863918 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        144115188075855872 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16208559625228863918(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 144115188075855872} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16208559625228863918(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16208559625228863918(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16208559625228863918(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16208559625228863918(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16208559625228863918(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16208559625228863918(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16208559625228863918);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16208559625228863918);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16208559625228863918);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6842313322737127003 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        144115188075855872 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6842313322737127003(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 144115188075855872} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6842313322737127003(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6842313322737127003(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6842313322737127003(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6842313322737127003(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6842313322737127003(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6842313322737127003(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6842313322737127003);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6842313322737127003);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6842313322737127003);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__3822210768415921338 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        288230376151711744 /* Input0 */, \
      512 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3822210768415921338(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 288230376151711744} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3822210768415921338(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3822210768415921338(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3822210768415921338(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3822210768415921338(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3822210768415921338(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3822210768415921338(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3822210768415921338);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3822210768415921338);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3822210768415921338);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4894613476453929930 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        288230376151711744 /* Input0 */, \
      512 /* Input1 */, \
      1 /* Input2 */, \
      1 /* Input3 */, \
      1000 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4894613476453929930(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 288230376151711744} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 1} /* Input2 */, 
      {"input[3]", 1} /* Input3 */, 
      {"filter_count", 1000} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4894613476453929930(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4894613476453929930(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4894613476453929930(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4894613476453929930(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4894613476453929930(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4894613476453929930(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4894613476453929930);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4894613476453929930);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4894613476453929930);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2411883749770181388 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2411883749770181388(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2411883749770181388(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2411883749770181388(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2411883749770181388(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2411883749770181388(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2411883749770181388(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2411883749770181388(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2411883749770181388);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2411883749770181388);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2411883749770181388);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__15626813312461766167 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15626813312461766167(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15626813312461766167(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15626813312461766167(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15626813312461766167(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15626813312461766167(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15626813312461766167(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15626813312461766167(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15626813312461766167);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15626813312461766167);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15626813312461766167);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4664422036270033969 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4664422036270033969(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4664422036270033969(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4664422036270033969(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4664422036270033969(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4664422036270033969(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4664422036270033969(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4664422036270033969(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4664422036270033969);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4664422036270033969);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4664422036270033969);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__2791305349948563438 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      192 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2791305349948563438(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2791305349948563438(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2791305349948563438(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2791305349948563438(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2791305349948563438(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2791305349948563438(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2791305349948563438(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2791305349948563438);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2791305349948563438);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2791305349948563438);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__12042810679548751376 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      192 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__12042810679548751376(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__12042810679548751376(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__12042810679548751376(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__12042810679548751376(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__12042810679548751376(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__12042810679548751376(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__12042810679548751376(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__12042810679548751376);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__12042810679548751376);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__12042810679548751376);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10769150322559465585 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10769150322559465585(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10769150322559465585(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10769150322559465585(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10769150322559465585(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10769150322559465585(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10769150322559465585(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10769150322559465585(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10769150322559465585);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10769150322559465585);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10769150322559465585);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10938535623703072432 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10938535623703072432(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10938535623703072432(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10938535623703072432(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10938535623703072432(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10938535623703072432(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10938535623703072432(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10938535623703072432(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10938535623703072432);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10938535623703072432);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10938535623703072432);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__12311803274137128186 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      96 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12311803274137128186(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12311803274137128186(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12311803274137128186(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12311803274137128186(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12311803274137128186(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12311803274137128186(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12311803274137128186(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12311803274137128186);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12311803274137128186);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12311803274137128186);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__594681294367419292 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      96 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__594681294367419292(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__594681294367419292(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__594681294367419292(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__594681294367419292(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__594681294367419292(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__594681294367419292(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__594681294367419292(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__594681294367419292);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__594681294367419292);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__594681294367419292);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__14780054044965270994 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      128 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14780054044965270994(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14780054044965270994(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14780054044965270994(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14780054044965270994(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14780054044965270994(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14780054044965270994(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14780054044965270994(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14780054044965270994);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14780054044965270994);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14780054044965270994);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2609686270873279149 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      16 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2609686270873279149(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2609686270873279149(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2609686270873279149(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2609686270873279149(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2609686270873279149(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2609686270873279149(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2609686270873279149(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2609686270873279149);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2609686270873279149);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2609686270873279149);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__14250690406327804911 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      16 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14250690406327804911(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14250690406327804911(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14250690406327804911(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14250690406327804911(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14250690406327804911(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14250690406327804911(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14250690406327804911(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14250690406327804911);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14250690406327804911);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14250690406327804911);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__1209137880701152534 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      16 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      32 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1209137880701152534(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1209137880701152534(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1209137880701152534(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1209137880701152534(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1209137880701152534(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1209137880701152534(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1209137880701152534(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1209137880701152534);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1209137880701152534);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1209137880701152534);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__17098624421651451028 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__17098624421651451028(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__17098624421651451028(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__17098624421651451028(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__17098624421651451028(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__17098624421651451028(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__17098624421651451028(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__17098624421651451028(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__17098624421651451028);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__17098624421651451028);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__17098624421651451028);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__3358742332551916031 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3358742332551916031(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3358742332551916031(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3358742332551916031(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3358742332551916031(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3358742332551916031(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3358742332551916031(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3358742332551916031(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3358742332551916031);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3358742332551916031);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3358742332551916031);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__1062505208371950249 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      32 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1062505208371950249(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1062505208371950249(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1062505208371950249(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1062505208371950249(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1062505208371950249(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1062505208371950249(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1062505208371950249(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1062505208371950249);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1062505208371950249);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1062505208371950249);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10124637858072950724 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        4 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10124637858072950724(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10124637858072950724(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10124637858072950724(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10124637858072950724(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10124637858072950724(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10124637858072950724(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10124637858072950724(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10124637858072950724);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10124637858072950724);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10124637858072950724);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__1558131921650104485 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        4 /* Input0 */, \
      128 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1558131921650104485(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1558131921650104485(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1558131921650104485(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1558131921650104485(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1558131921650104485(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1558131921650104485(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1558131921650104485(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1558131921650104485);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1558131921650104485);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1558131921650104485);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__3184749820646220347 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        4 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3184749820646220347(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3184749820646220347(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3184749820646220347(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3184749820646220347(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3184749820646220347(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3184749820646220347(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3184749820646220347(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3184749820646220347);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3184749820646220347);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3184749820646220347);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2452248540666004558 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        4 /* Input0 */, \
      32 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      96 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2452248540666004558(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2452248540666004558(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2452248540666004558(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2452248540666004558(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2452248540666004558(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2452248540666004558(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2452248540666004558(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2452248540666004558);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2452248540666004558);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2452248540666004558);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__16877335756575789995 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        4 /* Input0 */, \
      96 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16877335756575789995(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16877335756575789995(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16877335756575789995(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16877335756575789995(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16877335756575789995(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16877335756575789995(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16877335756575789995(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16877335756575789995);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16877335756575789995);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16877335756575789995);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__6039255076813615676 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        4 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__6039255076813615676(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__6039255076813615676(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__6039255076813615676(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__6039255076813615676(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__6039255076813615676(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__6039255076813615676(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__6039255076813615676(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__6039255076813615676);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__6039255076813615676);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__6039255076813615676);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__3167876644871931609 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        4 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3167876644871931609(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3167876644871931609(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3167876644871931609(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3167876644871931609(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3167876644871931609(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3167876644871931609(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3167876644871931609(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3167876644871931609);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3167876644871931609);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3167876644871931609);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__15155637018793296834 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        4 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15155637018793296834(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15155637018793296834(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15155637018793296834(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15155637018793296834(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15155637018793296834(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15155637018793296834(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15155637018793296834(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15155637018793296834);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15155637018793296834);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15155637018793296834);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__716677539661403596 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__716677539661403596(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__716677539661403596(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__716677539661403596(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__716677539661403596(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__716677539661403596(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__716677539661403596(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__716677539661403596(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__716677539661403596);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__716677539661403596);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__716677539661403596);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10863148915389818704 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10863148915389818704(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10863148915389818704(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10863148915389818704(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10863148915389818704(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10863148915389818704(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10863148915389818704(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10863148915389818704(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10863148915389818704);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10863148915389818704);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10863148915389818704);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__728262551857442210 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      96 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__728262551857442210(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__728262551857442210(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__728262551857442210(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__728262551857442210(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__728262551857442210(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__728262551857442210(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__728262551857442210(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__728262551857442210);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__728262551857442210);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__728262551857442210);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2314341683910303295 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      96 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      208 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2314341683910303295(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 208} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2314341683910303295(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2314341683910303295(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2314341683910303295(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2314341683910303295(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2314341683910303295(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2314341683910303295(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2314341683910303295);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2314341683910303295);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2314341683910303295);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__12719909202066124041 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      208 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12719909202066124041(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 208} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12719909202066124041(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12719909202066124041(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12719909202066124041(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12719909202066124041(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12719909202066124041(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12719909202066124041(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12719909202066124041);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12719909202066124041);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12719909202066124041);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2682917798289634125 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      16 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2682917798289634125(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2682917798289634125(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2682917798289634125(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2682917798289634125(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2682917798289634125(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2682917798289634125(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2682917798289634125(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2682917798289634125);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2682917798289634125);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2682917798289634125);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__7278835548133622455 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      16 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7278835548133622455(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7278835548133622455(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7278835548133622455(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7278835548133622455(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7278835548133622455(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7278835548133622455(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7278835548133622455(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7278835548133622455);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7278835548133622455);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7278835548133622455);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14108614882704051060 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      16 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      48 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14108614882704051060(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14108614882704051060(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14108614882704051060(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14108614882704051060(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14108614882704051060(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14108614882704051060(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14108614882704051060(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14108614882704051060);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14108614882704051060);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14108614882704051060);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__17025383412022076788 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__17025383412022076788(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__17025383412022076788(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__17025383412022076788(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__17025383412022076788(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__17025383412022076788(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__17025383412022076788(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__17025383412022076788(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__17025383412022076788);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__17025383412022076788);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__17025383412022076788);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10700378419537727889 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10700378419537727889(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10700378419537727889(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10700378419537727889(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10700378419537727889(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10700378419537727889(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10700378419537727889(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10700378419537727889(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10700378419537727889);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10700378419537727889);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10700378419537727889);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__3634609477152619979 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      64 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3634609477152619979(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3634609477152619979(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3634609477152619979(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3634609477152619979(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3634609477152619979(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3634609477152619979(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3634609477152619979(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3634609477152619979);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3634609477152619979);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3634609477152619979);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5188812311678468242 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        64 /* Input0 */, \
      192 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5188812311678468242(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5188812311678468242(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5188812311678468242(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5188812311678468242(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5188812311678468242(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5188812311678468242(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5188812311678468242(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5188812311678468242);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5188812311678468242);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5188812311678468242);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__1257440688019257957 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        256 /* Input0 */, \
      160 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1257440688019257957(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1257440688019257957(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1257440688019257957(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1257440688019257957(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1257440688019257957(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1257440688019257957(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1257440688019257957(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1257440688019257957);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1257440688019257957);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1257440688019257957);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__16759335886785363168 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        64 /* Input0 */, \
      192 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      112 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16759335886785363168(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 112} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16759335886785363168(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16759335886785363168(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16759335886785363168(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16759335886785363168(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16759335886785363168(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16759335886785363168(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16759335886785363168);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16759335886785363168);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16759335886785363168);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__1794100117098127458 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        64 /* Input0 */, \
      112 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1794100117098127458(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1794100117098127458(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1794100117098127458(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1794100117098127458(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1794100117098127458(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1794100117098127458(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1794100117098127458(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1794100117098127458);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1794100117098127458);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1794100117098127458);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__17433144035549251921 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        64 /* Input0 */, \
      112 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      224 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17433144035549251921(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 224} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17433144035549251921(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17433144035549251921(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17433144035549251921(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17433144035549251921(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17433144035549251921(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17433144035549251921(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17433144035549251921);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17433144035549251921);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17433144035549251921);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__8096688950261081291 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        64 /* Input0 */, \
      224 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8096688950261081291(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8096688950261081291(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8096688950261081291(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8096688950261081291(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8096688950261081291(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8096688950261081291(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8096688950261081291(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8096688950261081291);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8096688950261081291);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8096688950261081291);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8635128834358667164 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        64 /* Input0 */, \
      192 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      24 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8635128834358667164(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 24} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8635128834358667164(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8635128834358667164(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8635128834358667164(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8635128834358667164(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8635128834358667164(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8635128834358667164(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8635128834358667164);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8635128834358667164);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8635128834358667164);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__1770066695138679046 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        64 /* Input0 */, \
      24 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1770066695138679046(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1770066695138679046(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1770066695138679046(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1770066695138679046(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1770066695138679046(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1770066695138679046(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1770066695138679046(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1770066695138679046);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1770066695138679046);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1770066695138679046);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8537908043433281137 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        64 /* Input0 */, \
      24 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      64 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8537908043433281137(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8537908043433281137(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8537908043433281137(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8537908043433281137(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8537908043433281137(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8537908043433281137(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8537908043433281137(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8537908043433281137);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8537908043433281137);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8537908043433281137);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__12767448841516152888 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        64 /* Input0 */, \
      192 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__12767448841516152888(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__12767448841516152888(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__12767448841516152888(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__12767448841516152888(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__12767448841516152888(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__12767448841516152888(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__12767448841516152888(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__12767448841516152888);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__12767448841516152888);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__12767448841516152888);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__16992647164506873396 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1024 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16992647164506873396(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16992647164506873396(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16992647164506873396(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16992647164506873396(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16992647164506873396(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16992647164506873396(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16992647164506873396(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16992647164506873396);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16992647164506873396);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16992647164506873396);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5737867726387744355 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        256 /* Input0 */, \
      160 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5737867726387744355(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5737867726387744355(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5737867726387744355(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5737867726387744355(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5737867726387744355(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5737867726387744355(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5737867726387744355(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5737867726387744355);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5737867726387744355);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5737867726387744355);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__17802957061510632883 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        256 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17802957061510632883(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17802957061510632883(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17802957061510632883(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17802957061510632883(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17802957061510632883(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17802957061510632883(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17802957061510632883(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17802957061510632883);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17802957061510632883);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17802957061510632883);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__16180798846947037101 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        256 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16180798846947037101(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16180798846947037101(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16180798846947037101(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16180798846947037101(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16180798846947037101(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16180798846947037101(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16180798846947037101(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16180798846947037101);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16180798846947037101);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16180798846947037101);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6203280463285081151 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        256 /* Input0 */, \
      160 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      24 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6203280463285081151(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 24} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6203280463285081151(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6203280463285081151(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6203280463285081151(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6203280463285081151(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6203280463285081151(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6203280463285081151(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6203280463285081151);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6203280463285081151);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6203280463285081151);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9149232804293807563 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        256 /* Input0 */, \
      24 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9149232804293807563(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9149232804293807563(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9149232804293807563(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9149232804293807563(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9149232804293807563(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9149232804293807563(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9149232804293807563(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9149232804293807563);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9149232804293807563);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9149232804293807563);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__1158743023656342204 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        256 /* Input0 */, \
      24 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      64 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1158743023656342204(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1158743023656342204(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1158743023656342204(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1158743023656342204(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1158743023656342204(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1158743023656342204(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1158743023656342204(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1158743023656342204);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1158743023656342204);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1158743023656342204);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__10443652132427975579 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        256 /* Input0 */, \
      160 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__10443652132427975579(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__10443652132427975579(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__10443652132427975579(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__10443652132427975579(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__10443652132427975579(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__10443652132427975579(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__10443652132427975579(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__10443652132427975579);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__10443652132427975579);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__10443652132427975579);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__16777737308428631934 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        256 /* Input0 */, \
      160 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16777737308428631934(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16777737308428631934(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16777737308428631934(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16777737308428631934(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16777737308428631934(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16777737308428631934(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16777737308428631934(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16777737308428631934);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16777737308428631934);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16777737308428631934);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__12158092809808100594 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        256 /* Input0 */, \
      64 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12158092809808100594(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12158092809808100594(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12158092809808100594(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12158092809808100594(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12158092809808100594(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12158092809808100594(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12158092809808100594(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12158092809808100594);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12158092809808100594);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12158092809808100594);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__3743642883629099794 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1024 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      112 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3743642883629099794(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 112} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3743642883629099794(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3743642883629099794(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3743642883629099794(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3743642883629099794(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3743642883629099794(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3743642883629099794(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3743642883629099794);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3743642883629099794);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3743642883629099794);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__13987887598666142536 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        4096 /* Input0 */, \
      112 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13987887598666142536(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13987887598666142536(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13987887598666142536(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13987887598666142536(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13987887598666142536(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13987887598666142536(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13987887598666142536(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13987887598666142536);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13987887598666142536);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13987887598666142536);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6424322653422800172 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1024 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      144 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6424322653422800172(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 144} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6424322653422800172(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6424322653422800172(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6424322653422800172(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6424322653422800172(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6424322653422800172(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6424322653422800172(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6424322653422800172);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6424322653422800172);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6424322653422800172);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__12676082093653053150 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1024 /* Input0 */, \
      144 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12676082093653053150(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12676082093653053150(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12676082093653053150(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12676082093653053150(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12676082093653053150(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12676082093653053150(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12676082093653053150(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12676082093653053150);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12676082093653053150);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12676082093653053150);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__16100062131416821315 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1024 /* Input0 */, \
      144 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      288 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16100062131416821315(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 288} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16100062131416821315(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16100062131416821315(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16100062131416821315(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16100062131416821315(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16100062131416821315(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16100062131416821315(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16100062131416821315);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16100062131416821315);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16100062131416821315);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__6218569701229880727 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1024 /* Input0 */, \
      288 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6218569701229880727(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 288} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6218569701229880727(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6218569701229880727(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6218569701229880727(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6218569701229880727(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6218569701229880727(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6218569701229880727(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6218569701229880727);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6218569701229880727);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6218569701229880727);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12220585168283306657 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1024 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12220585168283306657(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12220585168283306657(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12220585168283306657(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12220585168283306657(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12220585168283306657(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12220585168283306657(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12220585168283306657(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12220585168283306657);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12220585168283306657);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12220585168283306657);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__2914260601977100623 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1024 /* Input0 */, \
      32 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2914260601977100623(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2914260601977100623(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2914260601977100623(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2914260601977100623(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2914260601977100623(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2914260601977100623(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2914260601977100623(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2914260601977100623);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2914260601977100623);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2914260601977100623);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5088430916573185592 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1024 /* Input0 */, \
      32 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      64 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5088430916573185592(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5088430916573185592(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5088430916573185592(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5088430916573185592(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5088430916573185592(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5088430916573185592(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5088430916573185592(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5088430916573185592);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5088430916573185592);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5088430916573185592);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__7660094634066407370 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        1024 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__7660094634066407370(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__7660094634066407370(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__7660094634066407370(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__7660094634066407370(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__7660094634066407370(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__7660094634066407370(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__7660094634066407370(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__7660094634066407370);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__7660094634066407370);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__7660094634066407370);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__1330582812273498927 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1024 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1330582812273498927(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1330582812273498927(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1330582812273498927(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1330582812273498927(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1330582812273498927(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1330582812273498927(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1330582812273498927(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1330582812273498927);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1330582812273498927);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1330582812273498927);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__13004413792429487989 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1024 /* Input0 */, \
      64 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13004413792429487989(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13004413792429487989(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13004413792429487989(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13004413792429487989(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13004413792429487989(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13004413792429487989(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13004413792429487989(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13004413792429487989);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13004413792429487989);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13004413792429487989);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__3012102407231625782 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        4096 /* Input0 */, \
      112 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3012102407231625782(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3012102407231625782(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3012102407231625782(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3012102407231625782(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3012102407231625782(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3012102407231625782(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3012102407231625782(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3012102407231625782);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3012102407231625782);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3012102407231625782);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__13715530272800642664 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        16384 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13715530272800642664(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13715530272800642664(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13715530272800642664(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13715530272800642664(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13715530272800642664(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13715530272800642664(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13715530272800642664(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13715530272800642664);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13715530272800642664);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13715530272800642664);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__13452148842808220188 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        4096 /* Input0 */, \
      112 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13452148842808220188(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13452148842808220188(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13452148842808220188(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13452148842808220188(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13452148842808220188(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13452148842808220188(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13452148842808220188(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13452148842808220188);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13452148842808220188);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13452148842808220188);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__12529113138504605058 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        4096 /* Input0 */, \
      160 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12529113138504605058(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12529113138504605058(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12529113138504605058(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12529113138504605058(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12529113138504605058(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12529113138504605058(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12529113138504605058(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12529113138504605058);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12529113138504605058);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12529113138504605058);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4282639036276696490 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        4096 /* Input0 */, \
      160 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      320 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4282639036276696490(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 320} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4282639036276696490(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4282639036276696490(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4282639036276696490(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4282639036276696490(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4282639036276696490(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4282639036276696490(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4282639036276696490);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4282639036276696490);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4282639036276696490);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__6080755073851643650 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        4096 /* Input0 */, \
      320 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6080755073851643650(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6080755073851643650(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6080755073851643650(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6080755073851643650(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6080755073851643650(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6080755073851643650(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6080755073851643650(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6080755073851643650);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6080755073851643650);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6080755073851643650);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__9247238374669043677 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        4096 /* Input0 */, \
      112 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9247238374669043677(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9247238374669043677(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9247238374669043677(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9247238374669043677(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9247238374669043677(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9247238374669043677(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9247238374669043677(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9247238374669043677);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9247238374669043677);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9247238374669043677);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9808924360165815599 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        4096 /* Input0 */, \
      32 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9808924360165815599(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9808924360165815599(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9808924360165815599(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9808924360165815599(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9808924360165815599(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9808924360165815599(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9808924360165815599(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9808924360165815599);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9808924360165815599);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9808924360165815599);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4736387825172028229 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        4096 /* Input0 */, \
      32 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      128 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4736387825172028229(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4736387825172028229(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4736387825172028229(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4736387825172028229(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4736387825172028229(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4736387825172028229(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4736387825172028229(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4736387825172028229);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4736387825172028229);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4736387825172028229);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__4864705927813145270 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        4096 /* Input0 */, \
      112 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__4864705927813145270(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__4864705927813145270(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__4864705927813145270(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__4864705927813145270(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__4864705927813145270(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__4864705927813145270(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__4864705927813145270(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__4864705927813145270);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__4864705927813145270);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__4864705927813145270);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11298588391799646030 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        4096 /* Input0 */, \
      112 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11298588391799646030(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11298588391799646030(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11298588391799646030(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11298588391799646030(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11298588391799646030(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11298588391799646030(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11298588391799646030(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11298588391799646030);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11298588391799646030);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11298588391799646030);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__5441411300018238036 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        4096 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5441411300018238036(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5441411300018238036(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5441411300018238036(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5441411300018238036(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5441411300018238036(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5441411300018238036(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5441411300018238036(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5441411300018238036);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5441411300018238036);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5441411300018238036);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__4596846948580485014 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        16384 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__4596846948580485014(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__4596846948580485014(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__4596846948580485014(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__4596846948580485014(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__4596846948580485014(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__4596846948580485014(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__4596846948580485014(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__4596846948580485014);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__4596846948580485014);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__4596846948580485014);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10711361837099719838 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        16384 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10711361837099719838(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10711361837099719838(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10711361837099719838(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10711361837099719838(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10711361837099719838(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10711361837099719838(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10711361837099719838(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10711361837099719838);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10711361837099719838);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10711361837099719838);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__7206466567264778165 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        65536 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7206466567264778165(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7206466567264778165(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7206466567264778165(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7206466567264778165(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7206466567264778165(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7206466567264778165(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7206466567264778165(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7206466567264778165);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7206466567264778165);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7206466567264778165);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__559827208636914868 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        16384 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__559827208636914868(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__559827208636914868(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__559827208636914868(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__559827208636914868(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__559827208636914868(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__559827208636914868(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__559827208636914868(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__559827208636914868);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__559827208636914868);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__559827208636914868);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__5754759107838466774 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        16384 /* Input0 */, \
      160 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5754759107838466774(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5754759107838466774(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5754759107838466774(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5754759107838466774(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5754759107838466774(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5754759107838466774(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5754759107838466774(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5754759107838466774);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5754759107838466774);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5754759107838466774);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__15660042212839170814 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        16384 /* Input0 */, \
      160 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      320 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15660042212839170814(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 320} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15660042212839170814(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15660042212839170814(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15660042212839170814(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15660042212839170814(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15660042212839170814(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15660042212839170814(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15660042212839170814);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15660042212839170814);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15660042212839170814);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__13141070964287344726 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        16384 /* Input0 */, \
      320 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13141070964287344726(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13141070964287344726(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13141070964287344726(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13141070964287344726(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13141070964287344726(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13141070964287344726(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13141070964287344726(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13141070964287344726);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13141070964287344726);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13141070964287344726);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4413404171703714165 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        16384 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4413404171703714165(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4413404171703714165(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4413404171703714165(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4413404171703714165(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4413404171703714165(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4413404171703714165(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4413404171703714165(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4413404171703714165);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4413404171703714165);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4413404171703714165);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__7646290089089380987 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        16384 /* Input0 */, \
      32 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7646290089089380987(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7646290089089380987(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7646290089089380987(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7646290089089380987(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7646290089089380987(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7646290089089380987(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7646290089089380987(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7646290089089380987);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7646290089089380987);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7646290089089380987);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11783210266765155345 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        16384 /* Input0 */, \
      32 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      128 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11783210266765155345(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11783210266765155345(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11783210266765155345(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11783210266765155345(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11783210266765155345(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11783210266765155345(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11783210266765155345(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11783210266765155345);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11783210266765155345);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11783210266765155345);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__18368403580708774942 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        16384 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__18368403580708774942(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__18368403580708774942(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__18368403580708774942(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__18368403580708774942(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__18368403580708774942(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__18368403580708774942(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__18368403580708774942(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__18368403580708774942);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__18368403580708774942);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__18368403580708774942);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2425157335714570726 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        16384 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2425157335714570726(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2425157335714570726(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2425157335714570726(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2425157335714570726(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2425157335714570726(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2425157335714570726(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2425157335714570726(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2425157335714570726);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2425157335714570726);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2425157335714570726);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__12231387704620414208 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        16384 /* Input0 */, \
      128 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12231387704620414208(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12231387704620414208(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12231387704620414208(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12231387704620414208(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12231387704620414208(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12231387704620414208(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12231387704620414208(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12231387704620414208);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12231387704620414208);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12231387704620414208);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8999064973855723859 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        65536 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      384 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8999064973855723859(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8999064973855723859(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8999064973855723859(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8999064973855723859(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8999064973855723859(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8999064973855723859(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8999064973855723859(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8999064973855723859);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8999064973855723859);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8999064973855723859);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__270131294854890639 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        262144 /* Input0 */, \
      384 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__270131294854890639(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 262144} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__270131294854890639(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__270131294854890639(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__270131294854890639(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__270131294854890639(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__270131294854890639(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__270131294854890639(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__270131294854890639);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__270131294854890639);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__270131294854890639);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12077771894585075 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        65536 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12077771894585075(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12077771894585075(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12077771894585075(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12077771894585075(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12077771894585075(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12077771894585075(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12077771894585075(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12077771894585075);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12077771894585075);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12077771894585075);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__1369789508651442157 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        65536 /* Input0 */, \
      192 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1369789508651442157(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1369789508651442157(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1369789508651442157(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1369789508651442157(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1369789508651442157(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1369789508651442157(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1369789508651442157(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1369789508651442157);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1369789508651442157);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1369789508651442157);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__17846997608818076267 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        65536 /* Input0 */, \
      192 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      384 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17846997608818076267(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17846997608818076267(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17846997608818076267(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17846997608818076267(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17846997608818076267(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17846997608818076267(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17846997608818076267(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17846997608818076267);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17846997608818076267);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17846997608818076267);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__7039147720368655021 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        65536 /* Input0 */, \
      384 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7039147720368655021(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7039147720368655021(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7039147720368655021(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7039147720368655021(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7039147720368655021(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7039147720368655021(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7039147720368655021(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7039147720368655021);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7039147720368655021);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7039147720368655021);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6162010582878397466 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        65536 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      48 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6162010582878397466(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6162010582878397466(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6162010582878397466(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6162010582878397466(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6162010582878397466(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6162010582878397466(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6162010582878397466(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6162010582878397466);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6162010582878397466);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6162010582878397466);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__15448623330791593132 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        65536 /* Input0 */, \
      48 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15448623330791593132(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15448623330791593132(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15448623330791593132(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15448623330791593132(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15448623330791593132(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15448623330791593132(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15448623330791593132(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15448623330791593132);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15448623330791593132);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15448623330791593132);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2305774891387058886 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        65536 /* Input0 */, \
      48 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      128 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2305774891387058886(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2305774891387058886(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2305774891387058886(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2305774891387058886(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2305774891387058886(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2305774891387058886(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2305774891387058886(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2305774891387058886);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2305774891387058886);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2305774891387058886);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__16545836654164166219 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        65536 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__16545836654164166219(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__16545836654164166219(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__16545836654164166219(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__16545836654164166219(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__16545836654164166219(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__16545836654164166219(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__16545836654164166219(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__16545836654164166219);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__16545836654164166219);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__16545836654164166219);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4238116798237246387 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        65536 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4238116798237246387(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4238116798237246387(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4238116798237246387(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4238116798237246387(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4238116798237246387(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4238116798237246387(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4238116798237246387(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4238116798237246387);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4238116798237246387);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4238116798237246387);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__12883018602416297813 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        65536 /* Input0 */, \
      128 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12883018602416297813(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12883018602416297813(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12883018602416297813(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12883018602416297813(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12883018602416297813(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12883018602416297813(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12883018602416297813(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12883018602416297813);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12883018602416297813);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12883018602416297813);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__9377531366328415601 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        262144 /* Input0 */, \
      384 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__9377531366328415601(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 262144} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__9377531366328415601(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__9377531366328415601(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__9377531366328415601(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__9377531366328415601(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__9377531366328415601(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__9377531366328415601(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__9377531366328415601);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__9377531366328415601);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__9377531366328415601);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10624008691834446865 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      24 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10624008691834446865(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 24} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10624008691834446865(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10624008691834446865(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10624008691834446865(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10624008691834446865(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10624008691834446865(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10624008691834446865(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10624008691834446865);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10624008691834446865);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10624008691834446865);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__6246973960432061926 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      24 /* Input1 */, \
      113 /* Input2 */, \
      113 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6246973960432061926(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 113} /* Input2 */, 
      {"input[3]", 113} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6246973960432061926(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6246973960432061926(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6246973960432061926(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6246973960432061926(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6246973960432061926(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6246973960432061926(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6246973960432061926);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6246973960432061926);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6246973960432061926);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__15505234173793631256 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      24 /* Input1 */, \
      113 /* Input2 */, \
      113 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__15505234173793631256(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 113} /* Input2 */, 
      {"input[3]", 113} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__15505234173793631256(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__15505234173793631256(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__15505234173793631256(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__15505234173793631256(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__15505234173793631256(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__15505234173793631256(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__15505234173793631256);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__15505234173793631256);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__15505234173793631256);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__1296425422143036732 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      24 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      112 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1296425422143036732(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 112} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1296425422143036732(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1296425422143036732(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1296425422143036732(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1296425422143036732(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1296425422143036732(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1296425422143036732(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1296425422143036732);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1296425422143036732);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1296425422143036732);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__14532736704712200574 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      112 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14532736704712200574(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14532736704712200574(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14532736704712200574(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14532736704712200574(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14532736704712200574(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14532736704712200574(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14532736704712200574(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14532736704712200574);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14532736704712200574);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14532736704712200574);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__15263633532097929288 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      112 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      112 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15263633532097929288(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 112} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15263633532097929288(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15263633532097929288(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15263633532097929288(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15263633532097929288(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15263633532097929288(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15263633532097929288(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15263633532097929288);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15263633532097929288);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15263633532097929288);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8958509875836500440 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      112 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      112 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8958509875836500440(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 112} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8958509875836500440(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8958509875836500440(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8958509875836500440(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8958509875836500440(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8958509875836500440(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8958509875836500440(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8958509875836500440);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8958509875836500440);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8958509875836500440);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__5208373589863684580 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      24 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__5208373589863684580(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__5208373589863684580(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__5208373589863684580(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__5208373589863684580(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__5208373589863684580(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__5208373589863684580(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__5208373589863684580(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__5208373589863684580);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__5208373589863684580);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__5208373589863684580);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14865537619099157052 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      136 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      136 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14865537619099157052(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 136} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 136} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14865537619099157052(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14865537619099157052(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14865537619099157052(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14865537619099157052(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14865537619099157052(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14865537619099157052(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14865537619099157052);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14865537619099157052);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14865537619099157052);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__3603540504620470620 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      136 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      136 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3603540504620470620(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 136} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 136} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3603540504620470620(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3603540504620470620(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3603540504620470620(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3603540504620470620(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3603540504620470620(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3603540504620470620(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3603540504620470620);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3603540504620470620);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3603540504620470620);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__2251219650798000265 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        2 /* Input0 */, \
      112 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2251219650798000265(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2251219650798000265(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2251219650798000265(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2251219650798000265(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2251219650798000265(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2251219650798000265(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2251219650798000265(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2251219650798000265);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2251219650798000265);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2251219650798000265);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__13680423709873302542 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        2 /* Input0 */, \
      112 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      136 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13680423709873302542(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 136} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13680423709873302542(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13680423709873302542(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13680423709873302542(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13680423709873302542(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13680423709873302542(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13680423709873302542(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13680423709873302542);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13680423709873302542);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13680423709873302542);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__5473725048166989908 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        2 /* Input0 */, \
      136 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5473725048166989908(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 136} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5473725048166989908(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5473725048166989908(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5473725048166989908(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5473725048166989908(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5473725048166989908(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5473725048166989908(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5473725048166989908);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5473725048166989908);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5473725048166989908);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__3971418395174548088 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      136 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      136 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3971418395174548088(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 136} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 136} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3971418395174548088(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3971418395174548088(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3971418395174548088(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3971418395174548088(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3971418395174548088(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3971418395174548088(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3971418395174548088);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3971418395174548088);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3971418395174548088);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4669455272073652364 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      136 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      136 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4669455272073652364(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 136} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 136} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4669455272073652364(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4669455272073652364(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4669455272073652364(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4669455272073652364(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4669455272073652364(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4669455272073652364(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4669455272073652364);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4669455272073652364);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4669455272073652364);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__11430678690446305655 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        2 /* Input0 */, \
      112 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__11430678690446305655(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__11430678690446305655(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__11430678690446305655(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__11430678690446305655(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__11430678690446305655(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__11430678690446305655(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__11430678690446305655(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__11430678690446305655);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__11430678690446305655);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__11430678690446305655);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10425853395156755740 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      272 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      272 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10425853395156755740(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 272} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 272} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10425853395156755740(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10425853395156755740(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10425853395156755740(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10425853395156755740(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10425853395156755740(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10425853395156755740(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10425853395156755740);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10425853395156755740);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10425853395156755740);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__7853913981983946364 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      272 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      272 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7853913981983946364(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 272} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 272} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7853913981983946364(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7853913981983946364(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7853913981983946364(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7853913981983946364(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7853913981983946364(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7853913981983946364(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7853913981983946364);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7853913981983946364);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7853913981983946364);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__17360733521910007463 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        3 /* Input0 */, \
      136 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17360733521910007463(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 3} /* Input0 */, 
      {"input[1]", 136} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17360733521910007463(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17360733521910007463(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17360733521910007463(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17360733521910007463(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17360733521910007463(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17360733521910007463(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17360733521910007463);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17360733521910007463);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17360733521910007463);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5161873031494948124 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        3 /* Input0 */, \
      136 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      272 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5161873031494948124(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 3} /* Input0 */, 
      {"input[1]", 136} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 272} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5161873031494948124(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5161873031494948124(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5161873031494948124(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5161873031494948124(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5161873031494948124(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5161873031494948124(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5161873031494948124);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5161873031494948124);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5161873031494948124);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__12716608003774804167 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        3 /* Input0 */, \
      272 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12716608003774804167(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 3} /* Input0 */, 
      {"input[1]", 272} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12716608003774804167(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12716608003774804167(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12716608003774804167(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12716608003774804167(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12716608003774804167(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12716608003774804167(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12716608003774804167);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12716608003774804167);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12716608003774804167);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__7629557526053672280 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      272 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      272 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7629557526053672280(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 272} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 272} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7629557526053672280(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7629557526053672280(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7629557526053672280(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7629557526053672280(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7629557526053672280(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7629557526053672280(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7629557526053672280);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7629557526053672280);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7629557526053672280);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5960954592692131696 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      272 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      272 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5960954592692131696(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 272} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 272} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5960954592692131696(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5960954592692131696(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5960954592692131696(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5960954592692131696(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5960954592692131696(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5960954592692131696(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5960954592692131696);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5960954592692131696);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5960954592692131696);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__8174502560082032473 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        3 /* Input0 */, \
      136 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__8174502560082032473(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 3} /* Input0 */, 
      {"input[1]", 136} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__8174502560082032473(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__8174502560082032473(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__8174502560082032473(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__8174502560082032473(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__8174502560082032473(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__8174502560082032473(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__8174502560082032473);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__8174502560082032473);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__8174502560082032473);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11032441647660024617 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        4 /* Input0 */, \
      272 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      544 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11032441647660024617(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 272} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 544} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11032441647660024617(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11032441647660024617(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11032441647660024617(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11032441647660024617(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11032441647660024617(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11032441647660024617(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11032441647660024617);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11032441647660024617);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11032441647660024617);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__11210151534378817890 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        4 /* Input0 */, \
      544 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11210151534378817890(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 544} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11210151534378817890(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11210151534378817890(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11210151534378817890(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11210151534378817890(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11210151534378817890(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11210151534378817890(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11210151534378817890);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11210151534378817890);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11210151534378817890);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__16321461558798636768 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      544 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      544 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16321461558798636768(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 544} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 544} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16321461558798636768(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16321461558798636768(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16321461558798636768(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16321461558798636768(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16321461558798636768(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16321461558798636768(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16321461558798636768);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16321461558798636768);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16321461558798636768);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2220236799532123520 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      544 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      544 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2220236799532123520(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 544} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 544} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2220236799532123520(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2220236799532123520(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2220236799532123520(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2220236799532123520(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2220236799532123520(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2220236799532123520(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2220236799532123520);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2220236799532123520);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2220236799532123520);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__11894735803776834458 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        4 /* Input0 */, \
      272 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11894735803776834458(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 272} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11894735803776834458(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11894735803776834458(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11894735803776834458(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11894735803776834458(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11894735803776834458(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11894735803776834458(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11894735803776834458);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11894735803776834458);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11894735803776834458);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__2634195517636176484 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        4 /* Input0 */, \
      272 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__2634195517636176484(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 272} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__2634195517636176484(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__2634195517636176484(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__2634195517636176484(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__2634195517636176484(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__2634195517636176484(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__2634195517636176484(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__2634195517636176484);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__2634195517636176484);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__2634195517636176484);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4923603254596275713 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4923603254596275713(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4923603254596275713(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4923603254596275713(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4923603254596275713(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4923603254596275713(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4923603254596275713(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4923603254596275713(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4923603254596275713);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4923603254596275713);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4923603254596275713);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14061612534742206392 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      128 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14061612534742206392(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14061612534742206392(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14061612534742206392(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14061612534742206392(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14061612534742206392(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14061612534742206392(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14061612534742206392(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14061612534742206392);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14061612534742206392);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14061612534742206392);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2192309203456471198 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2192309203456471198(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2192309203456471198(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2192309203456471198(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2192309203456471198(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2192309203456471198(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2192309203456471198(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2192309203456471198(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2192309203456471198);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2192309203456471198);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2192309203456471198);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10835407164820463326 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10835407164820463326(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10835407164820463326(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10835407164820463326(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10835407164820463326(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10835407164820463326(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10835407164820463326(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10835407164820463326(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10835407164820463326);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10835407164820463326);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10835407164820463326);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6823047445792015984 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      32 /* Input1 */, \
      113 /* Input2 */, \
      113 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6823047445792015984(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 113} /* Input2 */, 
      {"input[3]", 113} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6823047445792015984(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6823047445792015984(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6823047445792015984(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6823047445792015984(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6823047445792015984(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6823047445792015984(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6823047445792015984);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6823047445792015984);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6823047445792015984);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11745091834038179088 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      32 /* Input1 */, \
      113 /* Input2 */, \
      113 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11745091834038179088(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 113} /* Input2 */, 
      {"input[3]", 113} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11745091834038179088(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11745091834038179088(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11745091834038179088(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11745091834038179088(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11745091834038179088(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11745091834038179088(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11745091834038179088);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11745091834038179088);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11745091834038179088);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__2088098364683309797 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      32 /* Input1 */, \
      113 /* Input2 */, \
      113 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2088098364683309797(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 113} /* Input2 */, 
      {"input[3]", 113} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2088098364683309797(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2088098364683309797(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2088098364683309797(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2088098364683309797(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2088098364683309797(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2088098364683309797(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2088098364683309797);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2088098364683309797);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2088098364683309797);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6060582916895716642 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      32 /* Input1 */, \
      113 /* Input2 */, \
      113 /* Input3 */, \
      16 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6060582916895716642(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 113} /* Input2 */, 
      {"input[3]", 113} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6060582916895716642(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6060582916895716642(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6060582916895716642(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6060582916895716642(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6060582916895716642(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6060582916895716642(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6060582916895716642);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6060582916895716642);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6060582916895716642);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__3214398444889927801 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      16 /* Input1 */, \
      113 /* Input2 */, \
      113 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3214398444889927801(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 113} /* Input2 */, 
      {"input[3]", 113} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3214398444889927801(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3214398444889927801(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3214398444889927801(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3214398444889927801(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3214398444889927801(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3214398444889927801(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3214398444889927801);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3214398444889927801);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3214398444889927801);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__13301631522729209014 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      96 /* Input1 */, \
      113 /* Input2 */, \
      113 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13301631522729209014(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 113} /* Input2 */, 
      {"input[3]", 113} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13301631522729209014(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13301631522729209014(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13301631522729209014(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13301631522729209014(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13301631522729209014(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13301631522729209014(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13301631522729209014);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13301631522729209014);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13301631522729209014);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5087961383746567240 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      96 /* Input1 */, \
      113 /* Input2 */, \
      113 /* Input3 */, \
      96 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5087961383746567240(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 113} /* Input2 */, 
      {"input[3]", 113} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5087961383746567240(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5087961383746567240(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5087961383746567240(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5087961383746567240(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5087961383746567240(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5087961383746567240(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5087961383746567240);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5087961383746567240);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5087961383746567240);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__2878529271042083146 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      96 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2878529271042083146(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2878529271042083146(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2878529271042083146(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2878529271042083146(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2878529271042083146(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2878529271042083146(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2878529271042083146(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2878529271042083146);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2878529271042083146);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2878529271042083146);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6960240713304893200 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      96 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      24 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6960240713304893200(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 24} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6960240713304893200(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6960240713304893200(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6960240713304893200(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6960240713304893200(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6960240713304893200(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6960240713304893200(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6960240713304893200);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6960240713304893200);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6960240713304893200);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5381953008580894960 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      144 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      144 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5381953008580894960(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 144} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5381953008580894960(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5381953008580894960(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5381953008580894960(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5381953008580894960(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5381953008580894960(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5381953008580894960(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5381953008580894960);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5381953008580894960);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5381953008580894960);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4859034479700741842 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      144 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      24 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4859034479700741842(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 24} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4859034479700741842(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4859034479700741842(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4859034479700741842(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4859034479700741842(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4859034479700741842(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4859034479700741842(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4859034479700741842);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4859034479700741842);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4859034479700741842);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8876036559124331266 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      24 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      144 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8876036559124331266(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 144} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8876036559124331266(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8876036559124331266(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8876036559124331266(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8876036559124331266(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8876036559124331266(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8876036559124331266(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8876036559124331266);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8876036559124331266);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8876036559124331266);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__291585049584263304 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      144 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__291585049584263304(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__291585049584263304(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__291585049584263304(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__291585049584263304(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__291585049584263304(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__291585049584263304(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__291585049584263304(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__291585049584263304);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__291585049584263304);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__291585049584263304);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12961704932029173940 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      144 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      144 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12961704932029173940(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 144} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12961704932029173940(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12961704932029173940(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12961704932029173940(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12961704932029173940(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12961704932029173940(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12961704932029173940(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12961704932029173940);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12961704932029173940);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12961704932029173940);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__7624695933349182728 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      144 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7624695933349182728(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7624695933349182728(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7624695933349182728(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7624695933349182728(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7624695933349182728(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7624695933349182728(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7624695933349182728(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7624695933349182728);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7624695933349182728);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7624695933349182728);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__3141933005227993501 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      144 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3141933005227993501(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3141933005227993501(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3141933005227993501(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3141933005227993501(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3141933005227993501(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3141933005227993501(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3141933005227993501(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3141933005227993501);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3141933005227993501);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3141933005227993501);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14873640375546212815 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      192 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14873640375546212815(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14873640375546212815(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14873640375546212815(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14873640375546212815(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14873640375546212815(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14873640375546212815(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14873640375546212815(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14873640375546212815);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14873640375546212815);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14873640375546212815);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__9972027427649022943 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      32 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9972027427649022943(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9972027427649022943(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9972027427649022943(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9972027427649022943(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9972027427649022943(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9972027427649022943(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9972027427649022943(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9972027427649022943);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9972027427649022943);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9972027427649022943);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__1460637636623546236 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      192 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1460637636623546236(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1460637636623546236(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1460637636623546236(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1460637636623546236(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1460637636623546236(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1460637636623546236(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1460637636623546236(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1460637636623546236);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1460637636623546236);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1460637636623546236);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__10098186335081915738 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      192 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10098186335081915738(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10098186335081915738(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10098186335081915738(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10098186335081915738(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10098186335081915738(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10098186335081915738(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10098186335081915738(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10098186335081915738);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10098186335081915738);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10098186335081915738);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8468696306950643777 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      192 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8468696306950643777(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8468696306950643777(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8468696306950643777(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8468696306950643777(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8468696306950643777(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8468696306950643777(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8468696306950643777(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8468696306950643777);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8468696306950643777);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8468696306950643777);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__1891125580653987228 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      384 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      384 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1891125580653987228(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1891125580653987228(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1891125580653987228(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1891125580653987228(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1891125580653987228(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1891125580653987228(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1891125580653987228(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1891125580653987228);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1891125580653987228);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1891125580653987228);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__519235219889222913 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      384 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__519235219889222913(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__519235219889222913(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__519235219889222913(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__519235219889222913(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__519235219889222913(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__519235219889222913(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__519235219889222913(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__519235219889222913);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__519235219889222913);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__519235219889222913);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__7684071975151067717 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      384 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7684071975151067717(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7684071975151067717(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7684071975151067717(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7684071975151067717(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7684071975151067717(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7684071975151067717(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7684071975151067717(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7684071975151067717);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7684071975151067717);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7684071975151067717);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__18343615631173627930 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      384 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18343615631173627930(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18343615631173627930(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18343615631173627930(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18343615631173627930(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18343615631173627930(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__18343615631173627930(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18343615631173627930(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18343615631173627930);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18343615631173627930);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__18343615631173627930);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__16388336239713007064 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      384 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      384 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16388336239713007064(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16388336239713007064(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16388336239713007064(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16388336239713007064(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16388336239713007064(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16388336239713007064(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16388336239713007064(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16388336239713007064);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16388336239713007064);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16388336239713007064);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__17007142964852848690 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      384 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17007142964852848690(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17007142964852848690(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17007142964852848690(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17007142964852848690(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17007142964852848690(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17007142964852848690(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17007142964852848690(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17007142964852848690);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17007142964852848690);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17007142964852848690);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__1665883157319617512 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      384 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1665883157319617512(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1665883157319617512(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1665883157319617512(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1665883157319617512(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1665883157319617512(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1665883157319617512(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1665883157319617512(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1665883157319617512);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1665883157319617512);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1665883157319617512);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__15748872946452709808 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      576 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      576 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15748872946452709808(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 576} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15748872946452709808(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15748872946452709808(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15748872946452709808(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15748872946452709808(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15748872946452709808(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15748872946452709808(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15748872946452709808);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15748872946452709808);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15748872946452709808);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4075490229609747071 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      576 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4075490229609747071(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4075490229609747071(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4075490229609747071(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4075490229609747071(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4075490229609747071(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4075490229609747071(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4075490229609747071(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4075490229609747071);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4075490229609747071);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4075490229609747071);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__13684272468683845527 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      96 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      576 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13684272468683845527(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 576} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13684272468683845527(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13684272468683845527(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13684272468683845527(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13684272468683845527(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13684272468683845527(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13684272468683845527(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13684272468683845527);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13684272468683845527);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13684272468683845527);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__14093145636383170981 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      576 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14093145636383170981(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14093145636383170981(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14093145636383170981(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14093145636383170981(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14093145636383170981(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14093145636383170981(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14093145636383170981(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14093145636383170981);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14093145636383170981);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14093145636383170981);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2584658034384630260 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      576 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      576 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2584658034384630260(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 576} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2584658034384630260(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2584658034384630260(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2584658034384630260(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2584658034384630260(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2584658034384630260(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2584658034384630260(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2584658034384630260);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2584658034384630260);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2584658034384630260);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9574593489371352837 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      576 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9574593489371352837(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9574593489371352837(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9574593489371352837(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9574593489371352837(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9574593489371352837(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9574593489371352837(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9574593489371352837(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9574593489371352837);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9574593489371352837);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9574593489371352837);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__18189649383147767377 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      576 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18189649383147767377(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__18189649383147767377(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18189649383147767377(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18189649383147767377(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18189649383147767377(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18189649383147767377(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18189649383147767377(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__18189649383147767377);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18189649383147767377);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18189649383147767377);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8237718998752586338 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      960 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8237718998752586338(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 960} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8237718998752586338(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8237718998752586338(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8237718998752586338(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8237718998752586338(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8237718998752586338(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8237718998752586338(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8237718998752586338);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8237718998752586338);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8237718998752586338);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12909764585544611250 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      160 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      960 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12909764585544611250(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 960} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12909764585544611250(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12909764585544611250(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12909764585544611250(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12909764585544611250(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12909764585544611250(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12909764585544611250(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12909764585544611250);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12909764585544611250);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12909764585544611250);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__15839582640988889432 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      960 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      960 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15839582640988889432(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 960} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 960} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15839582640988889432(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15839582640988889432(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15839582640988889432(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15839582640988889432(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15839582640988889432(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15839582640988889432(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15839582640988889432);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15839582640988889432);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15839582640988889432);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__784495472628736822 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      960 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__784495472628736822(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 960} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__784495472628736822(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__784495472628736822(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__784495472628736822(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__784495472628736822(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__784495472628736822(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__784495472628736822(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__784495472628736822);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__784495472628736822);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__784495472628736822);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6927405622741649534 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      960 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      320 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6927405622741649534(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 960} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 320} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6927405622741649534(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6927405622741649534(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6927405622741649534(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6927405622741649534(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6927405622741649534(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6927405622741649534(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6927405622741649534);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6927405622741649534);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6927405622741649534);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__18060385221010422550 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      320 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      1280 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18060385221010422550(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 1280} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__18060385221010422550(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18060385221010422550(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18060385221010422550(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18060385221010422550(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18060385221010422550(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18060385221010422550(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__18060385221010422550);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18060385221010422550);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18060385221010422550);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__5242388569250839214 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      1280 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5242388569250839214(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1280} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5242388569250839214(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5242388569250839214(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5242388569250839214(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5242388569250839214(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5242388569250839214(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5242388569250839214(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5242388569250839214);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5242388569250839214);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5242388569250839214);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14381851246169843422 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      1280 /* Input1 */, \
      1 /* Input2 */, \
      1 /* Input3 */, \
      1000 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14381851246169843422(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1280} /* Input1 */, 
      {"input[2]", 1} /* Input2 */, 
      {"input[3]", 1} /* Input3 */, 
      {"filter_count", 1000} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14381851246169843422(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14381851246169843422(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14381851246169843422(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14381851246169843422(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14381851246169843422(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14381851246169843422(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14381851246169843422);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14381851246169843422);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14381851246169843422);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5366393086785912001 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      96 /* FilterCount */, \
      11 /* FilterHeight */, \
      11 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      4 /* StrideHeight */, \
      4 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5366393086785912001(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 11} /* FilterHeight */, 
      {"filter_width", 11} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 4} /* StrideHeight */, 
      {"stride_width", 4} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5366393086785912001(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5366393086785912001(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5366393086785912001(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5366393086785912001(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5366393086785912001(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5366393086785912001(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5366393086785912001);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5366393086785912001);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5366393086785912001);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__16904617427455723770 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      96 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16904617427455723770(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16904617427455723770(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16904617427455723770(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16904617427455723770(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16904617427455723770(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16904617427455723770(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16904617427455723770(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16904617427455723770);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16904617427455723770);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16904617427455723770);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__7711673363847723268 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      96 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__7711673363847723268(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__7711673363847723268(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__7711673363847723268(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__7711673363847723268(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__7711673363847723268(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__7711673363847723268(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__7711673363847723268(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__7711673363847723268);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__7711673363847723268);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__7711673363847723268);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__15440170952454545384 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      96 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      256 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15440170952454545384(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15440170952454545384(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15440170952454545384(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15440170952454545384(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15440170952454545384(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15440170952454545384(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15440170952454545384(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15440170952454545384);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15440170952454545384);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15440170952454545384);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__2005899366903354674 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2005899366903354674(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2005899366903354674(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2005899366903354674(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2005899366903354674(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2005899366903354674(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2005899366903354674(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2005899366903354674(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2005899366903354674);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2005899366903354674);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2005899366903354674);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__11117838491900765388 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__11117838491900765388(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__11117838491900765388(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__11117838491900765388(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__11117838491900765388(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__11117838491900765388(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__11117838491900765388(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__11117838491900765388(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__11117838491900765388);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__11117838491900765388);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__11117838491900765388);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__7803828526831344196 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      384 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7803828526831344196(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7803828526831344196(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7803828526831344196(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7803828526831344196(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7803828526831344196(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7803828526831344196(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7803828526831344196(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7803828526831344196);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7803828526831344196);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7803828526831344196);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__7637634467444500316 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      384 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      384 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7637634467444500316(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7637634467444500316(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7637634467444500316(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7637634467444500316(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7637634467444500316(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7637634467444500316(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7637634467444500316(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7637634467444500316);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7637634467444500316);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7637634467444500316);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__10183036357516858074 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      384 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10183036357516858074(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10183036357516858074(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10183036357516858074(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10183036357516858074(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10183036357516858074(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10183036357516858074(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10183036357516858074(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10183036357516858074);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10183036357516858074);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10183036357516858074);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11154317111988684996 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      384 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11154317111988684996(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11154317111988684996(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11154317111988684996(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11154317111988684996(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11154317111988684996(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11154317111988684996(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11154317111988684996(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11154317111988684996);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11154317111988684996);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11154317111988684996);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9864001639213871042 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9864001639213871042(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9864001639213871042(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9864001639213871042(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9864001639213871042(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9864001639213871042(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9864001639213871042(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9864001639213871042(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9864001639213871042);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9864001639213871042);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9864001639213871042);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__684517276185714236 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__684517276185714236(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__684517276185714236(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__684517276185714236(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__684517276185714236(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__684517276185714236(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__684517276185714236(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__684517276185714236(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__684517276185714236);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__684517276185714236);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__684517276185714236);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5773954396662101040 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      3 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5773954396662101040(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5773954396662101040(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5773954396662101040(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5773954396662101040(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5773954396662101040(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5773954396662101040(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5773954396662101040(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5773954396662101040);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5773954396662101040);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5773954396662101040);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__18205288761369174264 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18205288761369174264(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__18205288761369174264(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18205288761369174264(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18205288761369174264(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18205288761369174264(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18205288761369174264(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18205288761369174264(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__18205288761369174264);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18205288761369174264);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18205288761369174264);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__17964197501919196803 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17964197501919196803(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17964197501919196803(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17964197501919196803(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17964197501919196803(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17964197501919196803(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17964197501919196803(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17964197501919196803(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, ACTIVATION_PRELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17964197501919196803);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17964197501919196803);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17964197501919196803);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__429985896337117372 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__429985896337117372(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__429985896337117372(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__429985896337117372(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__429985896337117372(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__429985896337117372(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__429985896337117372(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__429985896337117372(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__429985896337117372);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__429985896337117372);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__429985896337117372);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__17994478305195328476 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17994478305195328476(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17994478305195328476(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17994478305195328476(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17994478305195328476(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17994478305195328476(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17994478305195328476(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17994478305195328476(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17994478305195328476);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17994478305195328476);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17994478305195328476);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__2276961036573251875 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2276961036573251875(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2276961036573251875(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2276961036573251875(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2276961036573251875(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2276961036573251875(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2276961036573251875(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2276961036573251875(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, ACTIVATION_PRELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2276961036573251875);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2276961036573251875);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2276961036573251875);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__13620459069082123845 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13620459069082123845(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13620459069082123845(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13620459069082123845(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13620459069082123845(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13620459069082123845(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13620459069082123845(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13620459069082123845(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13620459069082123845);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13620459069082123845);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13620459069082123845);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__4626204101633552482 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      128 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4626204101633552482(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4626204101633552482(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4626204101633552482(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4626204101633552482(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4626204101633552482(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4626204101633552482(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4626204101633552482(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, ACTIVATION_PRELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4626204101633552482);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4626204101633552482);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4626204101633552482);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2017279428645824320 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      128 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2017279428645824320(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2017279428645824320(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2017279428645824320(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2017279428645824320(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2017279428645824320(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2017279428645824320(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2017279428645824320(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2017279428645824320);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2017279428645824320);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2017279428645824320);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__13266056775856364897 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13266056775856364897(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13266056775856364897(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13266056775856364897(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13266056775856364897(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13266056775856364897(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13266056775856364897(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13266056775856364897(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13266056775856364897);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13266056775856364897);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13266056775856364897);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__3307933086703873506 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      128 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3307933086703873506(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3307933086703873506(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3307933086703873506(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3307933086703873506(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3307933086703873506(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3307933086703873506(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3307933086703873506(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, ACTIVATION_PRELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3307933086703873506);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3307933086703873506);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3307933086703873506);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4212363531925157884 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      128 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4212363531925157884(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4212363531925157884(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4212363531925157884(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4212363531925157884(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4212363531925157884(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4212363531925157884(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4212363531925157884(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4212363531925157884);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4212363531925157884);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4212363531925157884);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__18096322697980054786 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18096322697980054786(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18096322697980054786(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18096322697980054786(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18096322697980054786(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18096322697980054786(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__18096322697980054786(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18096322697980054786(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, ACTIVATION_PRELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18096322697980054786);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18096322697980054786);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__18096322697980054786);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__1580706403892537176 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1580706403892537176(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1580706403892537176(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1580706403892537176(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1580706403892537176(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1580706403892537176(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1580706403892537176(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1580706403892537176(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1580706403892537176);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1580706403892537176);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1580706403892537176);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4569540991555687640 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      128 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4569540991555687640(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4569540991555687640(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4569540991555687640(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4569540991555687640(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4569540991555687640(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4569540991555687640(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4569540991555687640(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4569540991555687640);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4569540991555687640);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4569540991555687640);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__16840950002880529706 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16840950002880529706(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16840950002880529706(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16840950002880529706(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16840950002880529706(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16840950002880529706(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16840950002880529706(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16840950002880529706(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, ACTIVATION_PRELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16840950002880529706);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16840950002880529706);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16840950002880529706);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__16662498195990872282 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16662498195990872282(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16662498195990872282(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16662498195990872282(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16662498195990872282(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16662498195990872282(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16662498195990872282(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16662498195990872282(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16662498195990872282);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16662498195990872282);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16662498195990872282);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__18162799897504473460 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      512 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18162799897504473460(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18162799897504473460(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18162799897504473460(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18162799897504473460(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18162799897504473460(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__18162799897504473460(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18162799897504473460(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, ACTIVATION_PRELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18162799897504473460);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18162799897504473460);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__18162799897504473460);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__852461332381308096 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      512 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__852461332381308096(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__852461332381308096(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__852461332381308096(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__852461332381308096(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__852461332381308096(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__852461332381308096(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__852461332381308096(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__852461332381308096);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__852461332381308096);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__852461332381308096);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__16294589502476362750 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16294589502476362750(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16294589502476362750(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16294589502476362750(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16294589502476362750(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16294589502476362750(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16294589502476362750(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16294589502476362750(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16294589502476362750);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16294589502476362750);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16294589502476362750);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__13493937887599202260 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      512 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13493937887599202260(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13493937887599202260(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13493937887599202260(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13493937887599202260(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13493937887599202260(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13493937887599202260(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13493937887599202260(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, ACTIVATION_PRELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13493937887599202260);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13493937887599202260);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13493937887599202260);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4746089987387864624 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      64 /* FilterCount */, \
      7 /* FilterHeight */, \
      7 /* FilterWidth */, \
      3 /* PadHeight */, \
      3 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4746089987387864624(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 7} /* FilterHeight */, 
      {"filter_width", 7} /* FilterWidth */, 
      {"pad_height", 3} /* PadHeight */, 
      {"pad_width", 3} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4746089987387864624(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4746089987387864624(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4746089987387864624(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4746089987387864624(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4746089987387864624(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4746089987387864624(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4746089987387864624);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4746089987387864624);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4746089987387864624);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9291796313395182815 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      113 /* Input2 */, \
      113 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9291796313395182815(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 113} /* Input2 */, 
      {"input[3]", 113} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9291796313395182815(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9291796313395182815(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9291796313395182815(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9291796313395182815(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9291796313395182815(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9291796313395182815(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9291796313395182815);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9291796313395182815);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9291796313395182815);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__103312762300026145 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      113 /* Input2 */, \
      113 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__103312762300026145(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 113} /* Input2 */, 
      {"input[3]", 113} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__103312762300026145(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__103312762300026145(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__103312762300026145(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__103312762300026145(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__103312762300026145(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__103312762300026145(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__103312762300026145);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__103312762300026145);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__103312762300026145);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__16590222450796321848 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16590222450796321848(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16590222450796321848(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16590222450796321848(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16590222450796321848(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16590222450796321848(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16590222450796321848(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16590222450796321848(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16590222450796321848);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16590222450796321848);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16590222450796321848);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8024735633449325977 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8024735633449325977(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8024735633449325977(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8024735633449325977(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8024735633449325977(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8024735633449325977(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8024735633449325977(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8024735633449325977(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8024735633449325977);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8024735633449325977);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8024735633449325977);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__1905840818989748056 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1905840818989748056(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1905840818989748056(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1905840818989748056(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1905840818989748056(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1905840818989748056(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1905840818989748056(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1905840818989748056(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1905840818989748056);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1905840818989748056);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1905840818989748056);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__2276961036573251875 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2276961036573251875(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2276961036573251875(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2276961036573251875(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2276961036573251875(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2276961036573251875(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2276961036573251875(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2276961036573251875(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2276961036573251875);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2276961036573251875);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2276961036573251875);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__17603081626822053981 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17603081626822053981(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17603081626822053981(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17603081626822053981(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17603081626822053981(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17603081626822053981(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17603081626822053981(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17603081626822053981(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17603081626822053981);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17603081626822053981);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17603081626822053981);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__10878649540544352386 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10878649540544352386(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10878649540544352386(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10878649540544352386(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10878649540544352386(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10878649540544352386(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10878649540544352386(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10878649540544352386(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10878649540544352386);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10878649540544352386);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10878649540544352386);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__3565542181019276480 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3565542181019276480(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3565542181019276480(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3565542181019276480(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3565542181019276480(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3565542181019276480(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3565542181019276480(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3565542181019276480(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3565542181019276480);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3565542181019276480);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3565542181019276480);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11343909968865375830 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11343909968865375830(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11343909968865375830(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11343909968865375830(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11343909968865375830(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11343909968865375830(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11343909968865375830(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11343909968865375830(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11343909968865375830);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11343909968865375830);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11343909968865375830);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12702815033671327066 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      512 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12702815033671327066(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12702815033671327066(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12702815033671327066(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12702815033671327066(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12702815033671327066(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12702815033671327066(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12702815033671327066(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12702815033671327066);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12702815033671327066);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12702815033671327066);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10337123688553859716 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      128 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10337123688553859716(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10337123688553859716(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10337123688553859716(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10337123688553859716(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10337123688553859716(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10337123688553859716(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10337123688553859716(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10337123688553859716);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10337123688553859716);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10337123688553859716);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__3307933086703873506 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      128 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3307933086703873506(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3307933086703873506(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3307933086703873506(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3307933086703873506(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3307933086703873506(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3307933086703873506(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3307933086703873506(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3307933086703873506);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3307933086703873506);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3307933086703873506);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__16081373271107418994 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      128 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16081373271107418994(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16081373271107418994(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16081373271107418994(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16081373271107418994(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16081373271107418994(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16081373271107418994(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16081373271107418994(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16081373271107418994);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16081373271107418994);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16081373271107418994);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__17193376786646247772 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      512 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17193376786646247772(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17193376786646247772(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17193376786646247772(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17193376786646247772(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17193376786646247772(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17193376786646247772(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17193376786646247772(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17193376786646247772);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17193376786646247772);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17193376786646247772);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__18165625167647248486 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      512 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18165625167647248486(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__18165625167647248486(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18165625167647248486(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18165625167647248486(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18165625167647248486(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18165625167647248486(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18165625167647248486(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__18165625167647248486);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18165625167647248486);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18165625167647248486);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10712544777580007185 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      512 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      1024 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10712544777580007185(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10712544777580007185(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10712544777580007185(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10712544777580007185(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10712544777580007185(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10712544777580007185(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10712544777580007185(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10712544777580007185);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10712544777580007185);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10712544777580007185);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__15689249975717036939 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      1024 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15689249975717036939(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15689249975717036939(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15689249975717036939(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15689249975717036939(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15689249975717036939(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15689249975717036939(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15689249975717036939(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15689249975717036939);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15689249975717036939);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15689249975717036939);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__18314559691226963764 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18314559691226963764(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__18314559691226963764(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18314559691226963764(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18314559691226963764(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18314559691226963764(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18314559691226963764(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18314559691226963764(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__18314559691226963764);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18314559691226963764);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18314559691226963764);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__16840950002880529706 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16840950002880529706(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16840950002880529706(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16840950002880529706(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16840950002880529706(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16840950002880529706(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16840950002880529706(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16840950002880529706(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16840950002880529706);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16840950002880529706);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16840950002880529706);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__7698654135782380323 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      1024 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7698654135782380323(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7698654135782380323(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7698654135782380323(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7698654135782380323(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7698654135782380323(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7698654135782380323(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7698654135782380323(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7698654135782380323);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7698654135782380323);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7698654135782380323);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__3632363366114003701 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      1024 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3632363366114003701(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3632363366114003701(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3632363366114003701(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3632363366114003701(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3632363366114003701(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3632363366114003701(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3632363366114003701(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3632363366114003701);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3632363366114003701);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3632363366114003701);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4178688587727465505 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      1024 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4178688587727465505(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4178688587727465505(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4178688587727465505(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4178688587727465505(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4178688587727465505(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4178688587727465505(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4178688587727465505(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4178688587727465505);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4178688587727465505);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4178688587727465505);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2737451518500554883 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      1024 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      2048 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2737451518500554883(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 2048} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2737451518500554883(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2737451518500554883(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2737451518500554883(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2737451518500554883(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2737451518500554883(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2737451518500554883(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2737451518500554883);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2737451518500554883);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2737451518500554883);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6175432820121806598 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      2048 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6175432820121806598(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6175432820121806598(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6175432820121806598(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6175432820121806598(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6175432820121806598(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6175432820121806598(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6175432820121806598(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6175432820121806598);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6175432820121806598);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6175432820121806598);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__13100441475850193444 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      512 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13100441475850193444(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13100441475850193444(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13100441475850193444(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13100441475850193444(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13100441475850193444(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13100441475850193444(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13100441475850193444(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13100441475850193444);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13100441475850193444);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13100441475850193444);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__13493937887599202260 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      512 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13493937887599202260(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13493937887599202260(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13493937887599202260(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13493937887599202260(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13493937887599202260(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13493937887599202260(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13493937887599202260(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13493937887599202260);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13493937887599202260);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13493937887599202260);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6161056031897779686 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      512 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      2048 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6161056031897779686(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 2048} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6161056031897779686(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6161056031897779686(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6161056031897779686(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6161056031897779686(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6161056031897779686(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6161056031897779686(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6161056031897779686);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6161056031897779686);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6161056031897779686);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__12067161256428663190 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      2048 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12067161256428663190(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12067161256428663190(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12067161256428663190(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12067161256428663190(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12067161256428663190(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12067161256428663190(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12067161256428663190(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12067161256428663190);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12067161256428663190);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12067161256428663190);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__15831484865317576288 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15831484865317576288(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15831484865317576288(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15831484865317576288(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15831484865317576288(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15831484865317576288(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15831484865317576288(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15831484865317576288(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15831484865317576288);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15831484865317576288);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15831484865317576288);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__17964197501919196803 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17964197501919196803(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17964197501919196803(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17964197501919196803(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17964197501919196803(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17964197501919196803(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17964197501919196803(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17964197501919196803(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17964197501919196803);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17964197501919196803);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17964197501919196803);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__8705945849215453053 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__8705945849215453053(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__8705945849215453053(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__8705945849215453053(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__8705945849215453053(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__8705945849215453053(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__8705945849215453053(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__8705945849215453053(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__8705945849215453053);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__8705945849215453053);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__8705945849215453053);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11102315870299799380 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      16 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11102315870299799380(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11102315870299799380(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11102315870299799380(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11102315870299799380(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11102315870299799380(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11102315870299799380(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11102315870299799380(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11102315870299799380);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11102315870299799380);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11102315870299799380);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9620132247914113007 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      16 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9620132247914113007(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9620132247914113007(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9620132247914113007(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9620132247914113007(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9620132247914113007(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9620132247914113007(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9620132247914113007(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9620132247914113007);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9620132247914113007);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9620132247914113007);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8945485709280879348 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      16 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8945485709280879348(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8945485709280879348(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8945485709280879348(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8945485709280879348(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8945485709280879348(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8945485709280879348(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8945485709280879348(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8945485709280879348);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8945485709280879348);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8945485709280879348);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__9253235393275048340 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      16 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9253235393275048340(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9253235393275048340(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9253235393275048340(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9253235393275048340(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9253235393275048340(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9253235393275048340(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9253235393275048340(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9253235393275048340);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9253235393275048340);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9253235393275048340);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__15200413027900709011 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15200413027900709011(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15200413027900709011(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15200413027900709011(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15200413027900709011(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15200413027900709011(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15200413027900709011(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15200413027900709011(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15200413027900709011);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15200413027900709011);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15200413027900709011);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2403718942223608611 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        2 /* Input0 */, \
      64 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      16 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2403718942223608611(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2403718942223608611(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2403718942223608611(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2403718942223608611(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2403718942223608611(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2403718942223608611(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2403718942223608611(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2403718942223608611);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2403718942223608611);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2403718942223608611);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__4524274419006731160 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        2 /* Input0 */, \
      16 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4524274419006731160(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4524274419006731160(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4524274419006731160(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4524274419006731160(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4524274419006731160(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4524274419006731160(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4524274419006731160(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4524274419006731160);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4524274419006731160);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4524274419006731160);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14369965003954337411 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        2 /* Input0 */, \
      16 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14369965003954337411(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14369965003954337411(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14369965003954337411(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14369965003954337411(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14369965003954337411(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14369965003954337411(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14369965003954337411(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14369965003954337411);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14369965003954337411);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14369965003954337411);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__10526794377691416514 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        4 /* Input0 */, \
      64 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10526794377691416514(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10526794377691416514(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10526794377691416514(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10526794377691416514(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10526794377691416514(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10526794377691416514(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10526794377691416514(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10526794377691416514);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10526794377691416514);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10526794377691416514);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4261241160422401507 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        2 /* Input0 */, \
      16 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4261241160422401507(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4261241160422401507(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4261241160422401507(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4261241160422401507(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4261241160422401507(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4261241160422401507(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4261241160422401507(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4261241160422401507);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4261241160422401507);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4261241160422401507);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__7618498267516947684 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        2 /* Input0 */, \
      64 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7618498267516947684(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7618498267516947684(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7618498267516947684(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7618498267516947684(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7618498267516947684(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7618498267516947684(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7618498267516947684(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7618498267516947684);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7618498267516947684);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7618498267516947684);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__1408133934087943740 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        4 /* Input0 */, \
      64 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__1408133934087943740(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__1408133934087943740(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__1408133934087943740(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__1408133934087943740(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__1408133934087943740(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__1408133934087943740(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__1408133934087943740(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__1408133934087943740);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__1408133934087943740);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__1408133934087943740);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10383502438697752407 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        4 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10383502438697752407(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10383502438697752407(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10383502438697752407(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10383502438697752407(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10383502438697752407(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10383502438697752407(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10383502438697752407(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10383502438697752407);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10383502438697752407);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10383502438697752407);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__5646069655679184376 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        4 /* Input0 */, \
      32 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5646069655679184376(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5646069655679184376(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5646069655679184376(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5646069655679184376(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5646069655679184376(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5646069655679184376(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5646069655679184376(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5646069655679184376);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5646069655679184376);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5646069655679184376);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__1191357817019767294 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        4 /* Input0 */, \
      32 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1191357817019767294(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1191357817019767294(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1191357817019767294(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1191357817019767294(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1191357817019767294(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1191357817019767294(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1191357817019767294(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1191357817019767294);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1191357817019767294);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1191357817019767294);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__17061371329869449886 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        4 /* Input0 */, \
      32 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17061371329869449886(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17061371329869449886(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17061371329869449886(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17061371329869449886(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17061371329869449886(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17061371329869449886(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17061371329869449886(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17061371329869449886);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17061371329869449886);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17061371329869449886);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__10230300881478707843 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        4 /* Input0 */, \
      128 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10230300881478707843(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10230300881478707843(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10230300881478707843(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10230300881478707843(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10230300881478707843(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10230300881478707843(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10230300881478707843(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10230300881478707843);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10230300881478707843);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10230300881478707843);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__1072308382308033959 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        8 /* Input0 */, \
      128 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1072308382308033959(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 8} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1072308382308033959(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1072308382308033959(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1072308382308033959(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1072308382308033959(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1072308382308033959(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1072308382308033959(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1072308382308033959);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1072308382308033959);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1072308382308033959);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__10306539630107375177 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        8 /* Input0 */, \
      32 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10306539630107375177(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 8} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10306539630107375177(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10306539630107375177(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10306539630107375177(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10306539630107375177(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10306539630107375177(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10306539630107375177(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10306539630107375177);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10306539630107375177);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10306539630107375177);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__15121535866080494159 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        8 /* Input0 */, \
      32 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15121535866080494159(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 8} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15121535866080494159(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15121535866080494159(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15121535866080494159(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15121535866080494159(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15121535866080494159(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15121535866080494159(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15121535866080494159);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15121535866080494159);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15121535866080494159);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__18370136756201270906 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      128 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18370136756201270906(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18370136756201270906(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18370136756201270906(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18370136756201270906(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18370136756201270906(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__18370136756201270906(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18370136756201270906(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18370136756201270906);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18370136756201270906);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__18370136756201270906);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__3284491421497725231 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        8 /* Input0 */, \
      32 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3284491421497725231(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 8} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3284491421497725231(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3284491421497725231(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3284491421497725231(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3284491421497725231(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3284491421497725231(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3284491421497725231(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3284491421497725231);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3284491421497725231);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3284491421497725231);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__5524690980159156530 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        8 /* Input0 */, \
      128 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5524690980159156530(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 8} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5524690980159156530(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5524690980159156530(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5524690980159156530(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5524690980159156530(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5524690980159156530(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5524690980159156530(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5524690980159156530);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5524690980159156530);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5524690980159156530);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__9183908229617639300 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      128 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__9183908229617639300(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__9183908229617639300(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__9183908229617639300(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__9183908229617639300(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__9183908229617639300(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__9183908229617639300(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__9183908229617639300(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__9183908229617639300);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__9183908229617639300);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__9183908229617639300);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6654936228453582629 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      48 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6654936228453582629(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6654936228453582629(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6654936228453582629(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6654936228453582629(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6654936228453582629(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6654936228453582629(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6654936228453582629(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6654936228453582629);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6654936228453582629);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6654936228453582629);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__682130237044807539 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      48 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__682130237044807539(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__682130237044807539(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__682130237044807539(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__682130237044807539(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__682130237044807539(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__682130237044807539(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__682130237044807539(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__682130237044807539);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__682130237044807539);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__682130237044807539);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__7881020433013937717 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      48 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7881020433013937717(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7881020433013937717(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7881020433013937717(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7881020433013937717(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7881020433013937717(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7881020433013937717(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7881020433013937717(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7881020433013937717);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7881020433013937717);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7881020433013937717);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10453317762990867797 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      48 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10453317762990867797(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10453317762990867797(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10453317762990867797(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10453317762990867797(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10453317762990867797(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10453317762990867797(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10453317762990867797(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10453317762990867797);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10453317762990867797);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10453317762990867797);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__14704701983005547570 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        16 /* Input0 */, \
      192 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14704701983005547570(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14704701983005547570(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14704701983005547570(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14704701983005547570(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14704701983005547570(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14704701983005547570(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14704701983005547570(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14704701983005547570);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14704701983005547570);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14704701983005547570);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12311971014252716959 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        32 /* Input0 */, \
      192 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      48 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12311971014252716959(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 32} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12311971014252716959(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12311971014252716959(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12311971014252716959(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12311971014252716959(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12311971014252716959(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12311971014252716959(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12311971014252716959);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12311971014252716959);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12311971014252716959);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__6790093335744559985 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        32 /* Input0 */, \
      48 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6790093335744559985(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 32} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6790093335744559985(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6790093335744559985(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6790093335744559985(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6790093335744559985(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6790093335744559985(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6790093335744559985(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6790093335744559985);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6790093335744559985);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6790093335744559985);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4184735121568627255 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        32 /* Input0 */, \
      48 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4184735121568627255(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 32} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4184735121568627255(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4184735121568627255(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4184735121568627255(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4184735121568627255(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4184735121568627255(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4184735121568627255(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4184735121568627255);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4184735121568627255);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4184735121568627255);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__3509160626026351046 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        64 /* Input0 */, \
      192 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3509160626026351046(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3509160626026351046(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3509160626026351046(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3509160626026351046(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3509160626026351046(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3509160626026351046(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3509160626026351046(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3509160626026351046);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3509160626026351046);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3509160626026351046);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14293735964714789207 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        32 /* Input0 */, \
      48 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14293735964714789207(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 32} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14293735964714789207(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14293735964714789207(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14293735964714789207(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14293735964714789207(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14293735964714789207(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14293735964714789207(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14293735964714789207);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14293735964714789207);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14293735964714789207);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__11195298538006376496 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        32 /* Input0 */, \
      192 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11195298538006376496(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 32} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11195298538006376496(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11195298538006376496(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11195298538006376496(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11195298538006376496(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11195298538006376496(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11195298538006376496(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11195298538006376496);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11195298538006376496);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11195298538006376496);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14489811092418645213 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        64 /* Input0 */, \
      192 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14489811092418645213(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14489811092418645213(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14489811092418645213(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14489811092418645213(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14489811092418645213(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14489811092418645213(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14489811092418645213(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14489811092418645213);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14489811092418645213);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14489811092418645213);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__14903018628652902463 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        64 /* Input0 */, \
      64 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14903018628652902463(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14903018628652902463(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14903018628652902463(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14903018628652902463(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14903018628652902463(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14903018628652902463(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14903018628652902463(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14903018628652902463);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14903018628652902463);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14903018628652902463);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2666692819096641857 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        64 /* Input0 */, \
      64 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2666692819096641857(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2666692819096641857(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2666692819096641857(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2666692819096641857(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2666692819096641857(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2666692819096641857(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2666692819096641857(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2666692819096641857);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2666692819096641857);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2666692819096641857);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__15658833069713469985 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        64 /* Input0 */, \
      64 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15658833069713469985(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15658833069713469985(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15658833069713469985(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15658833069713469985(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15658833069713469985(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15658833069713469985(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15658833069713469985(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15658833069713469985);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15658833069713469985);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15658833069713469985);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__5166738843761959326 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        64 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5166738843761959326(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5166738843761959326(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5166738843761959326(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5166738843761959326(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5166738843761959326(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5166738843761959326(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5166738843761959326(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5166738843761959326);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5166738843761959326);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5166738843761959326);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2253745202065747016 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        128 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2253745202065747016(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 128} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2253745202065747016(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2253745202065747016(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2253745202065747016(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2253745202065747016(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2253745202065747016(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2253745202065747016(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2253745202065747016);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2253745202065747016);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2253745202065747016);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__8035015639266402546 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        128 /* Input0 */, \
      64 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8035015639266402546(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 128} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8035015639266402546(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8035015639266402546(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8035015639266402546(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8035015639266402546(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8035015639266402546(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8035015639266402546(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8035015639266402546);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8035015639266402546);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8035015639266402546);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__9534624682206767500 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        128 /* Input0 */, \
      64 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9534624682206767500(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 128} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9534624682206767500(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9534624682206767500(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9534624682206767500(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9534624682206767500(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9534624682206767500(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9534624682206767500(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9534624682206767500);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9534624682206767500);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9534624682206767500);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__2439791149206074707 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        256 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2439791149206074707(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2439791149206074707(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2439791149206074707(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2439791149206074707(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2439791149206074707(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2439791149206074707(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2439791149206074707(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2439791149206074707);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2439791149206074707);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2439791149206074707);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8655757863901543148 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        128 /* Input0 */, \
      64 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8655757863901543148(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 128} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8655757863901543148(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8655757863901543148(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8655757863901543148(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8655757863901543148(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8655757863901543148(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8655757863901543148(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8655757863901543148);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8655757863901543148);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8655757863901543148);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__16637358903359236435 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        128 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16637358903359236435(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 128} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16637358903359236435(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16637358903359236435(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16637358903359236435(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16637358903359236435(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16637358903359236435(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16637358903359236435(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16637358903359236435);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16637358903359236435);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16637358903359236435);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__18383045495614074351 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        256 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      1000 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18383045495614074351(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 1000} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__18383045495614074351(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18383045495614074351(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18383045495614074351(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18383045495614074351(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18383045495614074351(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18383045495614074351(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__18383045495614074351);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18383045495614074351);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18383045495614074351);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__501781176752586541 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        256 /* Input0 */, \
      1000 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__501781176752586541(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 1000} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__501781176752586541(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__501781176752586541(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__501781176752586541(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__501781176752586541(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__501781176752586541(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__501781176752586541(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__501781176752586541);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__501781176752586541);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__501781176752586541);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__9757808075855417043 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        256 /* Input0 */, \
      1000 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__9757808075855417043(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 1000} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__9757808075855417043(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__9757808075855417043(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__9757808075855417043(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__9757808075855417043(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__9757808075855417043(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__9757808075855417043(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__9757808075855417043);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__9757808075855417043);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__9757808075855417043);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11835755202448434304 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      1 /* Input1 */, \
      64 /* Input2 */, \
      64 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11835755202448434304(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1} /* Input1 */, 
      {"input[2]", 64} /* Input2 */, 
      {"input[3]", 64} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11835755202448434304(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11835755202448434304(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11835755202448434304(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11835755202448434304(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11835755202448434304(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11835755202448434304(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11835755202448434304);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11835755202448434304);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11835755202448434304);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12931993179955788384 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      64 /* Input2 */, \
      64 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12931993179955788384(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 64} /* Input2 */, 
      {"input[3]", 64} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12931993179955788384(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12931993179955788384(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12931993179955788384(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12931993179955788384(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12931993179955788384(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12931993179955788384(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12931993179955788384);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12931993179955788384);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12931993179955788384);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__4654629754463419691 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      64 /* Input2 */, \
      64 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4654629754463419691(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 64} /* Input2 */, 
      {"input[3]", 64} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4654629754463419691(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4654629754463419691(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4654629754463419691(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4654629754463419691(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4654629754463419691(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4654629754463419691(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4654629754463419691);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4654629754463419691);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4654629754463419691);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__13908396533580893397 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      64 /* Input2 */, \
      64 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__13908396533580893397(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 64} /* Input2 */, 
      {"input[3]", 64} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__13908396533580893397(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__13908396533580893397(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__13908396533580893397(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__13908396533580893397(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__13908396533580893397(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__13908396533580893397(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__13908396533580893397);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__13908396533580893397);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__13908396533580893397);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__17364363547411602481 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17364363547411602481(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17364363547411602481(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17364363547411602481(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17364363547411602481(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17364363547411602481(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17364363547411602481(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17364363547411602481(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17364363547411602481);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17364363547411602481);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17364363547411602481);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12632714896492852592 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      128 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12632714896492852592(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12632714896492852592(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12632714896492852592(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12632714896492852592(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12632714896492852592(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12632714896492852592(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12632714896492852592(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12632714896492852592);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12632714896492852592);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12632714896492852592);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__18142802094833566502 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      128 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18142802094833566502(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18142802094833566502(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18142802094833566502(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18142802094833566502(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18142802094833566502(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__18142802094833566502(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18142802094833566502(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18142802094833566502);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18142802094833566502);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__18142802094833566502);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__8814727054648472280 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      128 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__8814727054648472280(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__8814727054648472280(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__8814727054648472280(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__8814727054648472280(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__8814727054648472280(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__8814727054648472280(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__8814727054648472280(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__8814727054648472280);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__8814727054648472280);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__8814727054648472280);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2866173528408703340 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      128 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2866173528408703340(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2866173528408703340(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2866173528408703340(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2866173528408703340(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2866173528408703340(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2866173528408703340(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2866173528408703340(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2866173528408703340);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2866173528408703340);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2866173528408703340);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__17369714861426319756 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17369714861426319756(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17369714861426319756(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17369714861426319756(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17369714861426319756(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17369714861426319756(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17369714861426319756(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17369714861426319756(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17369714861426319756);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17369714861426319756);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17369714861426319756);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__1194685284573833890 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1194685284573833890(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1194685284573833890(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1194685284573833890(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1194685284573833890(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1194685284573833890(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1194685284573833890(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1194685284573833890(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1194685284573833890);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1194685284573833890);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1194685284573833890);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__10450675517810318172 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__10450675517810318172(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__10450675517810318172(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__10450675517810318172(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__10450675517810318172(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__10450675517810318172(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__10450675517810318172(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__10450675517810318172(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__10450675517810318172);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__10450675517810318172);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__10450675517810318172);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5721244730378805412 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5721244730378805412(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5721244730378805412(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5721244730378805412(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5721244730378805412(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5721244730378805412(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5721244730378805412(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5721244730378805412(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5721244730378805412);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5721244730378805412);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5721244730378805412);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__12609007523677514634 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12609007523677514634(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12609007523677514634(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12609007523677514634(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12609007523677514634(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12609007523677514634(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12609007523677514634(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12609007523677514634(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12609007523677514634);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12609007523677514634);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12609007523677514634);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__3413777330850159220 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__3413777330850159220(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__3413777330850159220(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__3413777330850159220(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__3413777330850159220(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__3413777330850159220(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__3413777330850159220(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__3413777330850159220(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__3413777330850159220);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__3413777330850159220);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__3413777330850159220);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__5892618640161977131 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        4096 /* Input0 */, \
      4096 /* Input1 */, \
      -1 /* Input2 */, \
      -1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5892618640161977131(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 4096} /* Input1 */, 
      {"input[2]", -1} /* Input2 */, 
      {"input[3]", -1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5892618640161977131(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5892618640161977131(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5892618640161977131(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5892618640161977131(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5892618640161977131(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5892618640161977131(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5892618640161977131);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5892618640161977131);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5892618640161977131);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__14322172070247351735 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        4096 /* Input0 */, \
      1024 /* Input1 */, \
      -1 /* Input2 */, \
      -1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14322172070247351735(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", -1} /* Input2 */, 
      {"input[3]", -1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14322172070247351735(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14322172070247351735(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14322172070247351735(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14322172070247351735(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14322172070247351735(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14322172070247351735(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14322172070247351735);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14322172070247351735);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14322172070247351735);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__15339746672715693844 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15339746672715693844(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15339746672715693844(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15339746672715693844(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15339746672715693844(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15339746672715693844(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15339746672715693844(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15339746672715693844(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15339746672715693844);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15339746672715693844);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15339746672715693844);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8675801029464927196 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8675801029464927196(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8675801029464927196(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8675801029464927196(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8675801029464927196(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8675801029464927196(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8675801029464927196(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8675801029464927196(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8675801029464927196);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8675801029464927196);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8675801029464927196);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9046932095061065127 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9046932095061065127(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9046932095061065127(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9046932095061065127(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9046932095061065127(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9046932095061065127(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9046932095061065127(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9046932095061065127(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9046932095061065127);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9046932095061065127);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9046932095061065127);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__18163357193945844825 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__18163357193945844825(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__18163357193945844825(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__18163357193945844825(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__18163357193945844825(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__18163357193945844825(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__18163357193945844825(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__18163357193945844825(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__18163357193945844825);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__18163357193945844825);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__18163357193945844825);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6616160412211859941 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6616160412211859941(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6616160412211859941(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6616160412211859941(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6616160412211859941(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6616160412211859941(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6616160412211859941(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6616160412211859941(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6616160412211859941);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6616160412211859941);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6616160412211859941);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__323033451159353508 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      128 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__323033451159353508(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__323033451159353508(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__323033451159353508(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__323033451159353508(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__323033451159353508(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__323033451159353508(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__323033451159353508(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__323033451159353508);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__323033451159353508);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__323033451159353508);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__12026671017571278786 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      128 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12026671017571278786(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12026671017571278786(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12026671017571278786(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12026671017571278786(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12026671017571278786(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12026671017571278786(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12026671017571278786(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12026671017571278786);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12026671017571278786);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12026671017571278786);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__2844951696353668668 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      128 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__2844951696353668668(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__2844951696353668668(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__2844951696353668668(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__2844951696353668668(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__2844951696353668668(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__2844951696353668668(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__2844951696353668668(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__2844951696353668668);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__2844951696353668668);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__2844951696353668668);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10442178224088894792 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      128 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10442178224088894792(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10442178224088894792(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10442178224088894792(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10442178224088894792(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10442178224088894792(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10442178224088894792(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10442178224088894792(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10442178224088894792);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10442178224088894792);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10442178224088894792);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5053390372838649256 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5053390372838649256(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5053390372838649256(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5053390372838649256(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5053390372838649256(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5053390372838649256(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5053390372838649256(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5053390372838649256(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5053390372838649256);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5053390372838649256);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5053390372838649256);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__5889668212867905462 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5889668212867905462(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5889668212867905462(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5889668212867905462(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5889668212867905462(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5889668212867905462(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5889668212867905462(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5889668212867905462(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5889668212867905462);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5889668212867905462);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5889668212867905462);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__14997078998943188552 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__14997078998943188552(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__14997078998943188552(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__14997078998943188552(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__14997078998943188552(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__14997078998943188552(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__14997078998943188552(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__14997078998943188552(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__14997078998943188552);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__14997078998943188552);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__14997078998943188552);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10804823767184537866 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10804823767184537866(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10804823767184537866(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10804823767184537866(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10804823767184537866(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10804823767184537866(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10804823767184537866(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10804823767184537866(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10804823767184537866);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10804823767184537866);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10804823767184537866);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__9244299531816066388 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      512 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9244299531816066388(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9244299531816066388(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9244299531816066388(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9244299531816066388(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9244299531816066388(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9244299531816066388(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9244299531816066388(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9244299531816066388);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9244299531816066388);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9244299531816066388);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__10286325371903514788 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      512 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10286325371903514788(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10286325371903514788(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10286325371903514788(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10286325371903514788(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10286325371903514788(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10286325371903514788(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10286325371903514788(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10286325371903514788);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10286325371903514788);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10286325371903514788);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__1106833831987112282 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      512 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__1106833831987112282(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__1106833831987112282(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__1106833831987112282(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__1106833831987112282(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__1106833831987112282(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__1106833831987112282(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__1106833831987112282(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__1106833831987112282);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__1106833831987112282);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__1106833831987112282);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10054633226223086376 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10054633226223086376(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10054633226223086376(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10054633226223086376(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10054633226223086376(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10054633226223086376(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10054633226223086376(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10054633226223086376(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10054633226223086376);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10054633226223086376);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10054633226223086376);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9584571205571882712 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9584571205571882712(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9584571205571882712(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9584571205571882712(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9584571205571882712(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9584571205571882712(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9584571205571882712(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9584571205571882712(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9584571205571882712);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9584571205571882712);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9584571205571882712);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__333021552177110822 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__333021552177110822(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__333021552177110822(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__333021552177110822(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__333021552177110822(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__333021552177110822(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__333021552177110822(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__333021552177110822(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__333021552177110822);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__333021552177110822);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__333021552177110822);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6523802494011570693 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      96 /* FilterCount */, \
      7 /* FilterHeight */, \
      7 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6523802494011570693(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 7} /* FilterHeight */, 
      {"filter_width", 7} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6523802494011570693(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6523802494011570693(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6523802494011570693(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6523802494011570693(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6523802494011570693(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6523802494011570693(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6523802494011570693);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6523802494011570693);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6523802494011570693);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__1867093662682776174 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      96 /* Input1 */, \
      110 /* Input2 */, \
      110 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1867093662682776174(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 110} /* Input2 */, 
      {"input[3]", 110} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1867093662682776174(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1867093662682776174(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1867093662682776174(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1867093662682776174(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1867093662682776174(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1867093662682776174(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1867093662682776174);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1867093662682776174);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1867093662682776174);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__10985724419456594832 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      96 /* Input1 */, \
      110 /* Input2 */, \
      110 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__10985724419456594832(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 110} /* Input2 */, 
      {"input[3]", 110} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__10985724419456594832(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__10985724419456594832(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__10985724419456594832(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__10985724419456594832(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__10985724419456594832(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__10985724419456594832(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__10985724419456594832);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__10985724419456594832);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__10985724419456594832);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__3484935072646577504 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      96 /* Input1 */, \
      54 /* Input2 */, \
      54 /* Input3 */, \
      256 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3484935072646577504(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 54} /* Input2 */, 
      {"input[3]", 54} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3484935072646577504(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3484935072646577504(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3484935072646577504(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3484935072646577504(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3484935072646577504(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3484935072646577504(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3484935072646577504);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3484935072646577504);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3484935072646577504);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__14218050759677774030 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14218050759677774030(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14218050759677774030(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14218050759677774030(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14218050759677774030(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14218050759677774030(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14218050759677774030(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14218050759677774030(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14218050759677774030);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14218050759677774030);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14218050759677774030);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__4957535447968217392 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__4957535447968217392(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__4957535447968217392(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__4957535447968217392(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__4957535447968217392(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__4957535447968217392(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__4957535447968217392(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__4957535447968217392(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__4957535447968217392);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__4957535447968217392);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__4957535447968217392);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5742646041131479218 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      256 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5742646041131479218(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5742646041131479218(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5742646041131479218(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5742646041131479218(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5742646041131479218(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5742646041131479218(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5742646041131479218(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5742646041131479218);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5742646041131479218);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5742646041131479218);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6487948375738963180 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      512 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6487948375738963180(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6487948375738963180(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6487948375738963180(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6487948375738963180(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6487948375738963180(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6487948375738963180(benchmark::State& state) {
  CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6487948375738963180(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6487948375738963180);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6487948375738963180);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6487948375738963180);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__6089946087583322396 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      512 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6089946087583322396(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6089946087583322396(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6089946087583322396(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6089946087583322396(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6089946087583322396(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6089946087583322396(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6089946087583322396(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6089946087583322396);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6089946087583322396);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6089946087583322396);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__15357213568113271010 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      512 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__15357213568113271010(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__15357213568113271010(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__15357213568113271010(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__15357213568113271010(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__15357213568113271010(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__15357213568113271010(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__15357213568113271010(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__15357213568113271010);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__15357213568113271010);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__15357213568113271010);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
$undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__4930835671014821469 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      4096 /* Input1 */, \
      -1 /* Input2 */, \
      -1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4930835671014821469(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 4096} /* Input1 */, 
      {"input[2]", -1} /* Input2 */, 
      {"input[3]", -1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4930835671014821469(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4930835671014821469(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4930835671014821469(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4930835671014821469(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4930835671014821469(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4930835671014821469(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4930835671014821469);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4930835671014821469);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4930835671014821469);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__15233850255759358145 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
        1 /* Input0 */, \
      1024 /* Input1 */, \
      -1 /* Input2 */, \
      -1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15233850255759358145(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", -1} /* Input2 */, 
      {"input[3]", -1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15233850255759358145(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15233850255759358145(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15233850255759358145(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15233850255759358145(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15233850255759358145(benchmark::State& state) {
  CUDNN_RELU_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15233850255759358145(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15233850255759358145);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15233850255759358145);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15233850255759358145);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
