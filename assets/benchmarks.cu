
namespace LAYER_CUDNN_ACTIVATION_FWD__17162476239411228769 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17162476239411228769(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17162476239411228769(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17162476239411228769(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17162476239411228769(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17162476239411228769(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17162476239411228769(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17162476239411228769(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17162476239411228769);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17162476239411228769);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17162476239411228769);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__12290640332589300200 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      36864 /* Input0 */, \
      192 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__12290640332589300200(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 36864} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__12290640332589300200(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__12290640332589300200(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__12290640332589300200(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__12290640332589300200(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__12290640332589300200(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__12290640332589300200(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__12290640332589300200);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__12290640332589300200);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__12290640332589300200);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__7787190682734603109 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7787190682734603109(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7787190682734603109(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7787190682734603109(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7787190682734603109(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7787190682734603109(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7787190682734603109(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7787190682734603109(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7787190682734603109);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7787190682734603109);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7787190682734603109);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9754641982578257719 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9754641982578257719(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9754641982578257719(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9754641982578257719(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9754641982578257719(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9754641982578257719(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9754641982578257719(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9754641982578257719(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9754641982578257719);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9754641982578257719);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9754641982578257719);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__12102182736801705083 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      256 /* Input0 */, \
      1000 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__12102182736801705083(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 1000} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__12102182736801705083(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__12102182736801705083(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__12102182736801705083(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__12102182736801705083(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__12102182736801705083(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__12102182736801705083(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__12102182736801705083);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__12102182736801705083);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__12102182736801705083);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__15689249975717036939 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15689249975717036939(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15689249975717036939(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15689249975717036939(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15689249975717036939(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15689249975717036939(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15689249975717036939(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15689249975717036939(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15689249975717036939);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15689249975717036939);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15689249975717036939);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__3975504346080421269 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__3975504346080421269(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__3975504346080421269(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__3975504346080421269(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__3975504346080421269(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__3975504346080421269(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__3975504346080421269(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__3975504346080421269(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__3975504346080421269);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__3975504346080421269);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__3975504346080421269);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__3484935072646577504 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      54 /* Input2 */, \
      54 /* Input3 */, \
      256 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3484935072646577504(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 54} /* Input2 */, 
      {"input[3]", 54} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3484935072646577504(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3484935072646577504(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3484935072646577504(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3484935072646577504(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3484935072646577504(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3484935072646577504(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3484935072646577504);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3484935072646577504);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3484935072646577504);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__10744330852151118341 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      96 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10744330852151118341(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10744330852151118341(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10744330852151118341(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10744330852151118341(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10744330852151118341(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10744330852151118341(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10744330852151118341(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10744330852151118341);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10744330852151118341);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10744330852151118341);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__13062218085499300189 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13062218085499300189(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13062218085499300189(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13062218085499300189(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13062218085499300189(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13062218085499300189(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__13062218085499300189(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13062218085499300189(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13062218085499300189);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13062218085499300189);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__13062218085499300189);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__13211590592350964577 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      113 /* Input2 */, \
      113 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13211590592350964577(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 113} /* Input2 */, 
      {"input[3]", 113} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13211590592350964577(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13211590592350964577(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13211590592350964577(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13211590592350964577(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13211590592350964577(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13211590592350964577(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13211590592350964577);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13211590592350964577);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13211590592350964577);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__1580706403892537176 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1580706403892537176(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1580706403892537176(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1580706403892537176(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1580706403892537176(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1580706403892537176(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1580706403892537176(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1580706403892537176(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1580706403892537176);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1580706403892537176);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1580706403892537176);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5166156086459629386 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      268435456 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5166156086459629386(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 268435456} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5166156086459629386(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5166156086459629386(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5166156086459629386(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5166156086459629386(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5166156086459629386(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5166156086459629386(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5166156086459629386);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5166156086459629386);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5166156086459629386);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4730836171960170421 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      48 /* Input0 */, \
      64 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      96 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4730836171960170421(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 48} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4730836171960170421(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4730836171960170421(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4730836171960170421(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4730836171960170421(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4730836171960170421(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4730836171960170421(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4730836171960170421);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4730836171960170421);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4730836171960170421);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__1943729338616930953 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      768 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      160 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1943729338616930953(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 768} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1943729338616930953(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1943729338616930953(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1943729338616930953(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1943729338616930953(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1943729338616930953(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1943729338616930953(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1943729338616930953);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1943729338616930953);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1943729338616930953);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6161056031897779686 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      2048 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6161056031897779686(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 2048} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6161056031897779686(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6161056031897779686(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6161056031897779686(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6161056031897779686(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6161056031897779686(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6161056031897779686(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6161056031897779686);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6161056031897779686);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6161056031897779686);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__12726861371975606155 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      128 /* Input1 */, \
      52 /* Input2 */, \
      52 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__12726861371975606155(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 52} /* Input2 */, 
      {"input[3]", 52} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__12726861371975606155(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__12726861371975606155(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__12726861371975606155(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__12726861371975606155(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__12726861371975606155(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__12726861371975606155(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__12726861371975606155);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__12726861371975606155);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__12726861371975606155);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__18314559691226963764 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18314559691226963764(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__18314559691226963764(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18314559691226963764(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18314559691226963764(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18314559691226963764(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18314559691226963764(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18314559691226963764(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__18314559691226963764);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18314559691226963764);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18314559691226963764);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__16409329696749216261 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      70368744177664 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16409329696749216261(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 70368744177664} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__16409329696749216261(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16409329696749216261(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16409329696749216261(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16409329696749216261(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__16409329696749216261(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16409329696749216261(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__16409329696749216261);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16409329696749216261);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__16409329696749216261);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__13392592182353136385 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1024 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13392592182353136385(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13392592182353136385(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13392592182353136385(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13392592182353136385(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13392592182353136385(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13392592182353136385(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13392592182353136385(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13392592182353136385);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13392592182353136385);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13392592182353136385);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__452360821537151075 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__452360821537151075(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__452360821537151075(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__452360821537151075(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__452360821537151075(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__452360821537151075(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__452360821537151075(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__452360821537151075(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__452360821537151075);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__452360821537151075);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__452360821537151075);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__1858835808264698995 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      12288 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1858835808264698995(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 12288} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__1858835808264698995(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1858835808264698995(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1858835808264698995(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1858835808264698995(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__1858835808264698995(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1858835808264698995(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__1858835808264698995);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1858835808264698995);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__1858835808264698995);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__10224052846270232782 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      2048 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10224052846270232782(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__10224052846270232782(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10224052846270232782(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10224052846270232782(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10224052846270232782(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__10224052846270232782(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10224052846270232782(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__10224052846270232782);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10224052846270232782);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__10224052846270232782);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__6288708080796642293 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      147456 /* Input0 */, \
      128 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6288708080796642293(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 147456} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__6288708080796642293(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6288708080796642293(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6288708080796642293(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6288708080796642293(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__6288708080796642293(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6288708080796642293(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__6288708080796642293);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6288708080796642293);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__6288708080796642293);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9595851555439306485 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9595851555439306485(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9595851555439306485(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9595851555439306485(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9595851555439306485(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9595851555439306485(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9595851555439306485(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9595851555439306485(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9595851555439306485);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9595851555439306485);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9595851555439306485);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14251938285816836589 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14251938285816836589(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14251938285816836589(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14251938285816836589(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14251938285816836589(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14251938285816836589(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14251938285816836589(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14251938285816836589(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14251938285816836589);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14251938285816836589);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14251938285816836589);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__1049173819779073624 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      2048 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1049173819779073624(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1049173819779073624(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1049173819779073624(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1049173819779073624(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1049173819779073624(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1049173819779073624(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1049173819779073624(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1049173819779073624);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1049173819779073624);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1049173819779073624);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2635463873088024044 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      192 /* Input0 */, \
      224 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2635463873088024044(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 192} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2635463873088024044(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2635463873088024044(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2635463873088024044(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2635463873088024044(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2635463873088024044(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2635463873088024044(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2635463873088024044);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2635463873088024044);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2635463873088024044);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8992136372144651691 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8992136372144651691(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8992136372144651691(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8992136372144651691(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8992136372144651691(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8992136372144651691(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8992136372144651691(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8992136372144651691(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8992136372144651691);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8992136372144651691);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8992136372144651691);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2528547848072254677 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      147456 /* Input0 */, \
      192 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      224 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2528547848072254677(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 147456} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 224} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2528547848072254677(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2528547848072254677(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2528547848072254677(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2528547848072254677(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2528547848072254677(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2528547848072254677(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2528547848072254677);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2528547848072254677);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2528547848072254677);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__7698654135782380323 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      1024 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7698654135782380323(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7698654135782380323(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7698654135782380323(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7698654135782380323(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7698654135782380323(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7698654135782380323(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7698654135782380323(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7698654135782380323);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7698654135782380323);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7698654135782380323);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__11607880522970834303 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      8796093022208 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11607880522970834303(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 8796093022208} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__11607880522970834303(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11607880522970834303(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11607880522970834303(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11607880522970834303(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__11607880522970834303(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11607880522970834303(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__11607880522970834303);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11607880522970834303);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__11607880522970834303);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__12922391288831387533 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      401 /* Input2 */, \
      401 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12922391288831387533(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 401} /* Input2 */, 
      {"input[3]", 401} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12922391288831387533(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12922391288831387533(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12922391288831387533(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12922391288831387533(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12922391288831387533(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12922391288831387533(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12922391288831387533);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12922391288831387533);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12922391288831387533);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6670543490575844814 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      147456 /* Input0 */, \
      192 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      320 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6670543490575844814(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 147456} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 320} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6670543490575844814(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6670543490575844814(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6670543490575844814(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6670543490575844814(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6670543490575844814(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6670543490575844814(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6670543490575844814);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6670543490575844814);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6670543490575844814);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2411883749770181388 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2411883749770181388(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2411883749770181388(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2411883749770181388(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2411883749770181388(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2411883749770181388(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2411883749770181388(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2411883749770181388(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2411883749770181388);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2411883749770181388);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2411883749770181388);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__7872598735042137847 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7872598735042137847(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__7872598735042137847(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7872598735042137847(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7872598735042137847(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7872598735042137847(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__7872598735042137847(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7872598735042137847(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__7872598735042137847);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7872598735042137847);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__7872598735042137847);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__13826717052703284460 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13826717052703284460(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13826717052703284460(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13826717052703284460(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13826717052703284460(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13826717052703284460(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__13826717052703284460(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13826717052703284460(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13826717052703284460);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13826717052703284460);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__13826717052703284460);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__3031590442738265767 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3031590442738265767(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__3031590442738265767(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3031590442738265767(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3031590442738265767(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3031590442738265767(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__3031590442738265767(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3031590442738265767(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__3031590442738265767);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3031590442738265767);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__3031590442738265767);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__13750356180933617862 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      128 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13750356180933617862(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13750356180933617862(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13750356180933617862(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13750356180933617862(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13750356180933617862(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13750356180933617862(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13750356180933617862(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13750356180933617862);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13750356180933617862);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13750356180933617862);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__17182291978213083080 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      16 /* Input1 */, \
      416 /* Input2 */, \
      416 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__17182291978213083080(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 416} /* Input2 */, 
      {"input[3]", 416} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__17182291978213083080(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__17182291978213083080(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__17182291978213083080(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__17182291978213083080(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__17182291978213083080(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__17182291978213083080(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__17182291978213083080);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__17182291978213083080);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__17182291978213083080);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11102315870299799380 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      16 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11102315870299799380(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11102315870299799380(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11102315870299799380(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11102315870299799380(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11102315870299799380(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11102315870299799380(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11102315870299799380(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11102315870299799380);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11102315870299799380);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11102315870299799380);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__7114057717939641446 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      4503599627370496 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7114057717939641446(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4503599627370496} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7114057717939641446(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7114057717939641446(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7114057717939641446(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7114057717939641446(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7114057717939641446(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7114057717939641446(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7114057717939641446);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7114057717939641446);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7114057717939641446);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__15911950668741181433 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15911950668741181433(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15911950668741181433(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15911950668741181433(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15911950668741181433(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15911950668741181433(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__15911950668741181433(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15911950668741181433(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15911950668741181433);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15911950668741181433);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__15911950668741181433);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__10384027600876510392 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      128 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10384027600876510392(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10384027600876510392(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10384027600876510392(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10384027600876510392(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10384027600876510392(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10384027600876510392(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10384027600876510392(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10384027600876510392);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10384027600876510392);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10384027600876510392);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12702815033671327066 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12702815033671327066(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12702815033671327066(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12702815033671327066(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12702815033671327066(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12702815033671327066(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12702815033671327066(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12702815033671327066(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12702815033671327066);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12702815033671327066);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12702815033671327066);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__624609122427555901 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2147483648 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__624609122427555901(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2147483648} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__624609122427555901(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__624609122427555901(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__624609122427555901(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__624609122427555901(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__624609122427555901(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__624609122427555901(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__624609122427555901);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__624609122427555901);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__624609122427555901);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__13880981363830365899 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13880981363830365899(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13880981363830365899(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13880981363830365899(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13880981363830365899(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13880981363830365899(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13880981363830365899(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13880981363830365899(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13880981363830365899);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13880981363830365899);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13880981363830365899);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__18312299428214155702 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      147456 /* Input0 */, \
      192 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18312299428214155702(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 147456} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18312299428214155702(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18312299428214155702(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18312299428214155702(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18312299428214155702(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__18312299428214155702(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18312299428214155702(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18312299428214155702);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18312299428214155702);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__18312299428214155702);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4894613476453929930 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      288230376151711744 /* Input0 */, \
      512 /* Input1 */, \
      1 /* Input2 */, \
      1 /* Input3 */, \
      1000 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4894613476453929930(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 288230376151711744} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 1} /* Input2 */, 
      {"input[3]", 1} /* Input3 */, 
      {"filter_count", 1000} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4894613476453929930(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4894613476453929930(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4894613476453929930(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4894613476453929930(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4894613476453929930(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4894613476453929930(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4894613476453929930);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4894613476453929930);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4894613476453929930);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__15101928762142547327 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      12288 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15101928762142547327(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 12288} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15101928762142547327(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15101928762142547327(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15101928762142547327(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15101928762142547327(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15101928762142547327(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15101928762142547327(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15101928762142547327);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15101928762142547327);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15101928762142547327);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__9750358535019919148 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      96 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9750358535019919148(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9750358535019919148(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9750358535019919148(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9750358535019919148(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9750358535019919148(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9750358535019919148(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9750358535019919148(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9750358535019919148);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9750358535019919148);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9750358535019919148);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4177162388358404921 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      192 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4177162388358404921(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 192} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4177162388358404921(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4177162388358404921(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4177162388358404921(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4177162388358404921(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4177162388358404921(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4177162388358404921(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4177162388358404921);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4177162388358404921);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4177162388358404921);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10337123688553859716 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10337123688553859716(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10337123688553859716(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10337123688553859716(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10337123688553859716(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10337123688553859716(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10337123688553859716(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10337123688553859716(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10337123688553859716);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10337123688553859716);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10337123688553859716);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__16363006243484398478 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16363006243484398478(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16363006243484398478(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16363006243484398478(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16363006243484398478(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16363006243484398478(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16363006243484398478(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16363006243484398478(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16363006243484398478);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16363006243484398478);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16363006243484398478);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9726650279275694071 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      192 /* Input0 */, \
      224 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9726650279275694071(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 192} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9726650279275694071(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9726650279275694071(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9726650279275694071(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9726650279275694071(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9726650279275694071(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9726650279275694071(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9726650279275694071);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9726650279275694071);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9726650279275694071);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__424572043831356650 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      110 /* Input2 */, \
      110 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__424572043831356650(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 110} /* Input2 */, 
      {"input[3]", 110} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__424572043831356650(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__424572043831356650(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__424572043831356650(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__424572043831356650(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__424572043831356650(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__424572043831356650(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__424572043831356650);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__424572043831356650);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__424572043831356650);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__7045207322019847226 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      288230376151711744 /* Input0 */, \
      512 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7045207322019847226(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 288230376151711744} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__7045207322019847226(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7045207322019847226(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7045207322019847226(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7045207322019847226(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__7045207322019847226(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7045207322019847226(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__7045207322019847226);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7045207322019847226);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__7045207322019847226);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__6888837691303838726 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      12288 /* Input0 */, \
      256 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6888837691303838726(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 12288} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6888837691303838726(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6888837691303838726(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6888837691303838726(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6888837691303838726(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6888837691303838726(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6888837691303838726(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6888837691303838726);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6888837691303838726);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6888837691303838726);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14482067834342257391 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14482067834342257391(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14482067834342257391(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14482067834342257391(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14482067834342257391(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14482067834342257391(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__14482067834342257391(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14482067834342257391(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14482067834342257391);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14482067834342257391);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__14482067834342257391);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9368863105727111507 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      3072 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9368863105727111507(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 3072} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9368863105727111507(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9368863105727111507(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9368863105727111507(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9368863105727111507(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9368863105727111507(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9368863105727111507(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9368863105727111507);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9368863105727111507);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9368863105727111507);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__11195049598159916619 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      4194304 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11195049598159916619(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4194304} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11195049598159916619(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11195049598159916619(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11195049598159916619(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11195049598159916619(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11195049598159916619(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11195049598159916619(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11195049598159916619);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11195049598159916619);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11195049598159916619);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__7433574873688280702 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      256 /* Input0 */, \
      160 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7433574873688280702(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7433574873688280702(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7433574873688280702(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7433574873688280702(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7433574873688280702(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7433574873688280702(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7433574873688280702(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7433574873688280702);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7433574873688280702);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7433574873688280702);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__5446795974066621561 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5446795974066621561(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5446795974066621561(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5446795974066621561(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5446795974066621561(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5446795974066621561(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5446795974066621561(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5446795974066621561(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5446795974066621561);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5446795974066621561);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5446795974066621561);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6927405622741649534 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      960 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      320 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6927405622741649534(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 960} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 320} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6927405622741649534(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6927405622741649534(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6927405622741649534(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6927405622741649534(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6927405622741649534(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6927405622741649534(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6927405622741649534);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6927405622741649534);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6927405622741649534);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14707583734228891496 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      147456 /* Input0 */, \
      224 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14707583734228891496(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 147456} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14707583734228891496(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14707583734228891496(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14707583734228891496(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14707583734228891496(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__14707583734228891496(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14707583734228891496(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14707583734228891496);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14707583734228891496);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__14707583734228891496);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__2191339870464579756 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      192 /* Input0 */, \
      224 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2191339870464579756(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 192} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2191339870464579756(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2191339870464579756(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2191339870464579756(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2191339870464579756(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2191339870464579756(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2191339870464579756(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2191339870464579756);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2191339870464579756);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2191339870464579756);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__4363449296233128570 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4363449296233128570(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4363449296233128570(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4363449296233128570(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4363449296233128570(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4363449296233128570(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4363449296233128570(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4363449296233128570(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4363449296233128570);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4363449296233128570);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4363449296233128570);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__13131346225968070961 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      262144 /* Input0 */, \
      256 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__13131346225968070961(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 262144} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__13131346225968070961(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__13131346225968070961(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__13131346225968070961(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__13131346225968070961(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__13131346225968070961(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__13131346225968070961(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__13131346225968070961);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__13131346225968070961);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__13131346225968070961);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__1658527679807893178 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      16 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1658527679807893178(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1658527679807893178(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1658527679807893178(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1658527679807893178(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1658527679807893178(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1658527679807893178(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1658527679807893178(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1658527679807893178);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1658527679807893178);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1658527679807893178);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__852461332381308096 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__852461332381308096(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__852461332381308096(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__852461332381308096(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__852461332381308096(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__852461332381308096(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__852461332381308096(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__852461332381308096(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__852461332381308096);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__852461332381308096);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__852461332381308096);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5773954396662101040 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      3 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5773954396662101040(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5773954396662101040(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5773954396662101040(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5773954396662101040(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5773954396662101040(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5773954396662101040(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5773954396662101040(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5773954396662101040);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5773954396662101040);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5773954396662101040);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__6872345469721878292 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      36864 /* Input0 */, \
      160 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6872345469721878292(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 36864} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__6872345469721878292(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6872345469721878292(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6872345469721878292(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6872345469721878292(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__6872345469721878292(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6872345469721878292(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__6872345469721878292);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6872345469721878292);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__6872345469721878292);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_SOFTMAX_FWD__1789545703051437014 {

#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1000 /* Input1 */, \
      -1 /* Input2 */, \
      -1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_ADD_COUNTERS__1789545703051437014(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1000} /* Input1 */, 
      {"input[2]", -1} /* Input2 */, 
      {"input[3]", -1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnSoftmaxAlgorithm_t softmax_algorithm, cudnnSoftmaxMode_t softmax_mode>
static void LAYER_CUDNN_SOFTMAX_FWD_FLOAT16__1789545703051437014(benchmark::State& state) {
  LAYER_CUDNN_SOFTMAX_FWD_Impl<__half, softmax_algorithm, softmax_mode>(state);
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_ADD_COUNTERS__1789545703051437014(state);
}

template <cudnnSoftmaxAlgorithm_t softmax_algorithm, cudnnSoftmaxMode_t softmax_mode>
static void LAYER_CUDNN_SOFTMAX_FWD_FLOAT32__1789545703051437014(benchmark::State& state) {
  LAYER_CUDNN_SOFTMAX_FWD_Impl<float, softmax_algorithm, softmax_mode>(state);
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_ADD_COUNTERS__1789545703051437014(state);
}

template <cudnnSoftmaxAlgorithm_t softmax_algorithm, cudnnSoftmaxMode_t softmax_mode>
static void LAYER_CUDNN_SOFTMAX_FWD_TENSORCOREHALF__1789545703051437014(benchmark::State& state) {
  LAYER_CUDNN_SOFTMAX_FWD_Impl<__half, softmax_algorithm, softmax_mode>(state);
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_ADD_COUNTERS__1789545703051437014(state);
}

#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, SOFTMAX_MODE) \
 BENCHMARK_TEMPLATE(b, CUDNN_SOFTMAX_MODE_INSTANCE, SOFTMAX_MODE)->BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_SOFTMAX_MODE_CHANNEL, SOFTMAX_MODE)->BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD(b)                                                                                             \
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, CUDNN_SOFTMAX_MODE_INSTANCE);                                                                    \
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, CUDNN_SOFTMAX_MODE_CHANNEL)

BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD(LAYER_CUDNN_SOFTMAX_FWD_FLOAT16__1789545703051437014);
BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD(LAYER_CUDNN_SOFTMAX_FWD_FLOAT32__1789545703051437014);
BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD(LAYER_CUDNN_SOFTMAX_FWD_TENSORCOREHALF__1789545703051437014);

#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0
#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD
}

namespace CUDNN_POOLING_FWD__101145250496186114 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__101145250496186114(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__101145250496186114(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__101145250496186114(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__101145250496186114(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__101145250496186114(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__101145250496186114(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__101145250496186114(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__101145250496186114);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__101145250496186114);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__101145250496186114);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__10246036693726114381 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      32 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10246036693726114381(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10246036693726114381(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10246036693726114381(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10246036693726114381(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10246036693726114381(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10246036693726114381(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10246036693726114381(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10246036693726114381);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10246036693726114381);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10246036693726114381);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2737451518500554883 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      2048 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2737451518500554883(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 2048} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2737451518500554883(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2737451518500554883(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2737451518500554883(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2737451518500554883(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2737451518500554883(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2737451518500554883(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2737451518500554883);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2737451518500554883);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2737451518500554883);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__2484634829879663593 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      8796093022208 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2484634829879663593(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 8796093022208} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2484634829879663593(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2484634829879663593(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2484634829879663593(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2484634829879663593(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2484634829879663593(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2484634829879663593(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2484634829879663593);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2484634829879663593);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2484634829879663593);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__1175628607415716479 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      3072 /* Input0 */, \
      192 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1175628607415716479(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 3072} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1175628607415716479(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1175628607415716479(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1175628607415716479(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1175628607415716479(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1175628607415716479(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1175628607415716479(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1175628607415716479);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1175628607415716479);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1175628607415716479);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__13020194778551638696 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13020194778551638696(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13020194778551638696(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13020194778551638696(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13020194778551638696(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13020194778551638696(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13020194778551638696(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13020194778551638696(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13020194778551638696);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13020194778551638696);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13020194778551638696);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__782940343366756945 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      3072 /* Input0 */, \
      192 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__782940343366756945(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 3072} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__782940343366756945(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__782940343366756945(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__782940343366756945(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__782940343366756945(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__782940343366756945(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__782940343366756945(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__782940343366756945);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__782940343366756945);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__782940343366756945);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__8519664154636636269 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8519664154636636269(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__8519664154636636269(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8519664154636636269(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8519664154636636269(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8519664154636636269(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__8519664154636636269(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8519664154636636269(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__8519664154636636269);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8519664154636636269);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__8519664154636636269);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__6175432820121806598 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      2048 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6175432820121806598(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6175432820121806598(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6175432820121806598(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6175432820121806598(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6175432820121806598(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6175432820121806598(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6175432820121806598(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6175432820121806598);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6175432820121806598);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6175432820121806598);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__16863537562862778209 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      524288 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16863537562862778209(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 524288} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__16863537562862778209(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16863537562862778209(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16863537562862778209(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16863537562862778209(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__16863537562862778209(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16863537562862778209(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__16863537562862778209);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16863537562862778209);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__16863537562862778209);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__6747351928891462725 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1099511627776 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6747351928891462725(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1099511627776} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6747351928891462725(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6747351928891462725(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6747351928891462725(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6747351928891462725(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6747351928891462725(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6747351928891462725(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6747351928891462725);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6747351928891462725);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6747351928891462725);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__117307621069660164 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__117307621069660164(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__117307621069660164(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__117307621069660164(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__117307621069660164(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__117307621069660164(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__117307621069660164(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__117307621069660164(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__117307621069660164);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__117307621069660164);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__117307621069660164);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__5433145394851722154 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      68719476736 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5433145394851722154(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 68719476736} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__5433145394851722154(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5433145394851722154(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5433145394851722154(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5433145394851722154(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__5433145394851722154(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5433145394851722154(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__5433145394851722154);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5433145394851722154);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__5433145394851722154);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace CUDNN_POOLING_FWD__15358528626140098358 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      768 /* Input0 */, \
      192 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__15358528626140098358(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 768} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__15358528626140098358(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__15358528626140098358(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__15358528626140098358(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__15358528626140098358(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__15358528626140098358(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__15358528626140098358(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__15358528626140098358);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__15358528626140098358);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__15358528626140098358);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__13100441475850193444 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13100441475850193444(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13100441475850193444(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13100441475850193444(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13100441475850193444(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13100441475850193444(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13100441475850193444(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13100441475850193444(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13100441475850193444);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13100441475850193444);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13100441475850193444);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__8899385618613310520 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1024 /* Input0 */, \
      144 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8899385618613310520(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8899385618613310520(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8899385618613310520(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8899385618613310520(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8899385618613310520(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8899385618613310520(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8899385618613310520(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8899385618613310520);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8899385618613310520);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8899385618613310520);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__15818695139860452761 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15818695139860452761(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15818695139860452761(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15818695139860452761(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15818695139860452761(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15818695139860452761(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__15818695139860452761(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15818695139860452761(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15818695139860452761);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15818695139860452761);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__15818695139860452761);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__5065729209038375673 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1024 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5065729209038375673(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5065729209038375673(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5065729209038375673(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5065729209038375673(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5065729209038375673(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5065729209038375673(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5065729209038375673(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5065729209038375673);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5065729209038375673);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5065729209038375673);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__2955121900384308105 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1073741824 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2955121900384308105(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1073741824} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2955121900384308105(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2955121900384308105(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2955121900384308105(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2955121900384308105(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2955121900384308105(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2955121900384308105(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2955121900384308105);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2955121900384308105);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2955121900384308105);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__302648067509719457 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__302648067509719457(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__302648067509719457(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__302648067509719457(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__302648067509719457(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__302648067509719457(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__302648067509719457(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__302648067509719457(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__302648067509719457);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__302648067509719457);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__302648067509719457);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__9187876812019118967 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      8 /* Input0 */, \
      128 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9187876812019118967(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 8} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9187876812019118967(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9187876812019118967(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9187876812019118967(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9187876812019118967(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9187876812019118967(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9187876812019118967(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9187876812019118967);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9187876812019118967);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9187876812019118967);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__15971098156419113583 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      96 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15971098156419113583(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15971098156419113583(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15971098156419113583(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15971098156419113583(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15971098156419113583(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15971098156419113583(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15971098156419113583(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15971098156419113583);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15971098156419113583);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15971098156419113583);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__8082007310715868385 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      16384 /* Input0 */, \
      32 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8082007310715868385(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8082007310715868385(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8082007310715868385(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8082007310715868385(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8082007310715868385(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8082007310715868385(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8082007310715868385(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8082007310715868385);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8082007310715868385);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8082007310715868385);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__16162092092560412912 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      8 /* Input0 */, \
      32 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16162092092560412912(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 8} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16162092092560412912(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16162092092560412912(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16162092092560412912(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16162092092560412912(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16162092092560412912(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16162092092560412912(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16162092092560412912);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16162092092560412912);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16162092092560412912);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__11869081811777424391 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      4096 /* Input1 */, \
      -1 /* Input2 */, \
      -1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11869081811777424391(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 4096} /* Input1 */, 
      {"input[2]", -1} /* Input2 */, 
      {"input[3]", -1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11869081811777424391(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11869081811777424391(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11869081811777424391(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11869081811777424391(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11869081811777424391(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11869081811777424391(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11869081811777424391);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11869081811777424391);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11869081811777424391);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__7718277087223429839 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1073741824 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7718277087223429839(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1073741824} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7718277087223429839(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7718277087223429839(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7718277087223429839(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7718277087223429839(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7718277087223429839(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7718277087223429839(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7718277087223429839);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7718277087223429839);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7718277087223429839);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__13883506285853367397 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13883506285853367397(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13883506285853367397(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13883506285853367397(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13883506285853367397(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13883506285853367397(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13883506285853367397(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13883506285853367397(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13883506285853367397);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13883506285853367397);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13883506285853367397);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__18000935675144245799 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      128 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__18000935675144245799(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__18000935675144245799(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__18000935675144245799(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__18000935675144245799(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__18000935675144245799(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__18000935675144245799(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__18000935675144245799(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__18000935675144245799);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__18000935675144245799);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__18000935675144245799);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__2163999833900491749 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2163999833900491749(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2163999833900491749(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2163999833900491749(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2163999833900491749(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2163999833900491749(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2163999833900491749(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2163999833900491749(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2163999833900491749);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2163999833900491749);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2163999833900491749);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__17979387756531152753 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      64 /* Input0 */, \
      24 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      64 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17979387756531152753(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17979387756531152753(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17979387756531152753(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17979387756531152753(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17979387756531152753(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17979387756531152753(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17979387756531152753(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17979387756531152753);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17979387756531152753);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17979387756531152753);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__1188768397476470467 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      12288 /* Input0 */, \
      192 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1188768397476470467(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 12288} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1188768397476470467(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1188768397476470467(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1188768397476470467(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1188768397476470467(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1188768397476470467(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1188768397476470467(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1188768397476470467);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1188768397476470467);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1188768397476470467);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4569540991555687640 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4569540991555687640(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4569540991555687640(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4569540991555687640(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4569540991555687640(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4569540991555687640(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4569540991555687640(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4569540991555687640(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4569540991555687640);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4569540991555687640);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4569540991555687640);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__15845923391242472523 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15845923391242472523(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15845923391242472523(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15845923391242472523(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15845923391242472523(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15845923391242472523(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15845923391242472523(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15845923391242472523(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15845923391242472523);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15845923391242472523);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15845923391242472523);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__16138048218771131627 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      8 /* Input0 */, \
      128 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16138048218771131627(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 8} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__16138048218771131627(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16138048218771131627(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16138048218771131627(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16138048218771131627(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__16138048218771131627(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16138048218771131627(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__16138048218771131627);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16138048218771131627);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__16138048218771131627);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__15658833069713469985 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      64 /* Input0 */, \
      64 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15658833069713469985(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15658833069713469985(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15658833069713469985(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15658833069713469985(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15658833069713469985(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15658833069713469985(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15658833069713469985(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15658833069713469985);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15658833069713469985);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15658833069713469985);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5151257770716461393 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      768 /* Input0 */, \
      192 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5151257770716461393(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 768} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5151257770716461393(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5151257770716461393(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5151257770716461393(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5151257770716461393(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5151257770716461393(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5151257770716461393(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5151257770716461393);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5151257770716461393);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5151257770716461393);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__835442692094487364 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      112 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__835442692094487364(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__835442692094487364(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__835442692094487364(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__835442692094487364(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__835442692094487364(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__835442692094487364(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__835442692094487364(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__835442692094487364);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__835442692094487364);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__835442692094487364);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__15854092271266488320 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      48 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15854092271266488320(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 48} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15854092271266488320(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15854092271266488320(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15854092271266488320(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15854092271266488320(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15854092271266488320(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15854092271266488320(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15854092271266488320);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15854092271266488320);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15854092271266488320);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__18201895151408336223 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      768 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      160 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18201895151408336223(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 768} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__18201895151408336223(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18201895151408336223(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18201895151408336223(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18201895151408336223(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18201895151408336223(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18201895151408336223(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__18201895151408336223);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18201895151408336223);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18201895151408336223);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__214508982839290706 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      8796093022208 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__214508982839290706(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 8796093022208} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__214508982839290706(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__214508982839290706(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__214508982839290706(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__214508982839290706(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__214508982839290706(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__214508982839290706(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__214508982839290706);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__214508982839290706);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__214508982839290706);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__8206953166115485741 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      768 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8206953166115485741(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 768} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8206953166115485741(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8206953166115485741(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8206953166115485741(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8206953166115485741(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8206953166115485741(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8206953166115485741(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8206953166115485741);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8206953166115485741);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8206953166115485741);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14349997907744091173 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14349997907744091173(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14349997907744091173(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14349997907744091173(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14349997907744091173(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14349997907744091173(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14349997907744091173(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14349997907744091173(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14349997907744091173);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14349997907744091173);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14349997907744091173);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__12213221853060288561 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12213221853060288561(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12213221853060288561(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12213221853060288561(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12213221853060288561(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12213221853060288561(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12213221853060288561(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12213221853060288561(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12213221853060288561);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12213221853060288561);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12213221853060288561);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__17881823977575224988 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      64 /* Input0 */, \
      192 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      24 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17881823977575224988(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 24} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17881823977575224988(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17881823977575224988(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17881823977575224988(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17881823977575224988(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17881823977575224988(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17881823977575224988(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17881823977575224988);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17881823977575224988);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17881823977575224988);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__16029525023784026128 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      65536 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16029525023784026128(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16029525023784026128(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16029525023784026128(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16029525023784026128(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16029525023784026128(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16029525023784026128(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16029525023784026128(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16029525023784026128);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16029525023784026128);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16029525023784026128);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__16581358618257172121 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16581358618257172121(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16581358618257172121(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16581358618257172121(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16581358618257172121(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16581358618257172121(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16581358618257172121(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16581358618257172121(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16581358618257172121);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16581358618257172121);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16581358618257172121);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__15552093234706751582 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      768 /* Input0 */, \
      192 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15552093234706751582(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 768} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15552093234706751582(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15552093234706751582(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15552093234706751582(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15552093234706751582(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__15552093234706751582(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15552093234706751582(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15552093234706751582);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15552093234706751582);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__15552093234706751582);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__3163224117532380878 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      17592186044416 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3163224117532380878(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 17592186044416} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3163224117532380878(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3163224117532380878(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3163224117532380878(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3163224117532380878(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3163224117532380878(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3163224117532380878(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3163224117532380878);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3163224117532380878);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3163224117532380878);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__17419522032425327732 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      401 /* Input2 */, \
      401 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17419522032425327732(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 401} /* Input2 */, 
      {"input[3]", 401} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17419522032425327732(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17419522032425327732(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17419522032425327732(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17419522032425327732(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17419522032425327732(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17419522032425327732(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17419522032425327732);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17419522032425327732);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17419522032425327732);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__8683378755633414800 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      32 /* Input1 */, \
      208 /* Input2 */, \
      208 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8683378755633414800(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 208} /* Input2 */, 
      {"input[3]", 208} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__8683378755633414800(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8683378755633414800(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8683378755633414800(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8683378755633414800(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__8683378755633414800(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8683378755633414800(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__8683378755633414800);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8683378755633414800);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__8683378755633414800);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__17161185910628537481 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2097152 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17161185910628537481(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2097152} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17161185910628537481(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17161185910628537481(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17161185910628537481(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17161185910628537481(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17161185910628537481(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17161185910628537481(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17161185910628537481);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17161185910628537481);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17161185910628537481);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__15207565716765687015 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      8589934592 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15207565716765687015(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 8589934592} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15207565716765687015(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15207565716765687015(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15207565716765687015(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15207565716765687015(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15207565716765687015(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15207565716765687015(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15207565716765687015);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15207565716765687015);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15207565716765687015);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__16081373271107418994 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16081373271107418994(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16081373271107418994(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16081373271107418994(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16081373271107418994(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16081373271107418994(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16081373271107418994(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16081373271107418994(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16081373271107418994);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16081373271107418994);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16081373271107418994);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__11674390299267100474 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      9007199254740992 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11674390299267100474(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 9007199254740992} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11674390299267100474(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11674390299267100474(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11674390299267100474(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11674390299267100474(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11674390299267100474(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11674390299267100474(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11674390299267100474);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11674390299267100474);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11674390299267100474);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__11083906769993313442 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      36864 /* Input0 */, \
      320 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11083906769993313442(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 36864} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11083906769993313442(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11083906769993313442(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11083906769993313442(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11083906769993313442(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11083906769993313442(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11083906769993313442(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11083906769993313442);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11083906769993313442);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11083906769993313442);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__13109744425449233597 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      35184372088832 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13109744425449233597(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 35184372088832} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13109744425449233597(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13109744425449233597(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13109744425449233597(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13109744425449233597(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13109744425449233597(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13109744425449233597(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13109744425449233597);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13109744425449233597);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13109744425449233597);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__17061371329869449886 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      32 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17061371329869449886(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17061371329869449886(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17061371329869449886(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17061371329869449886(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17061371329869449886(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17061371329869449886(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17061371329869449886(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17061371329869449886);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17061371329869449886);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17061371329869449886);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__76627715148881124 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      16384 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__76627715148881124(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__76627715148881124(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__76627715148881124(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__76627715148881124(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__76627715148881124(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__76627715148881124(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__76627715148881124(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__76627715148881124);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__76627715148881124);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__76627715148881124);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14423597981328172164 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14423597981328172164(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14423597981328172164(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14423597981328172164(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14423597981328172164(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14423597981328172164(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14423597981328172164(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14423597981328172164(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14423597981328172164);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14423597981328172164);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14423597981328172164);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__1775523202030411942 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      72057594037927936 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1775523202030411942(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 72057594037927936} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1775523202030411942(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1775523202030411942(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1775523202030411942(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1775523202030411942(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1775523202030411942(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1775523202030411942(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1775523202030411942);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1775523202030411942);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1775523202030411942);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__1830901344880125122 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      131072 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1830901344880125122(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 131072} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__1830901344880125122(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1830901344880125122(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1830901344880125122(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1830901344880125122(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__1830901344880125122(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1830901344880125122(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__1830901344880125122);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1830901344880125122);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__1830901344880125122);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__3003797070812890897 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      16 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3003797070812890897(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3003797070812890897(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3003797070812890897(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3003797070812890897(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3003797070812890897(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3003797070812890897(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3003797070812890897(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3003797070812890897);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3003797070812890897);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3003797070812890897);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__17083706301383904884 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      4398046511104 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17083706301383904884(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4398046511104} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__17083706301383904884(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17083706301383904884(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17083706301383904884(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17083706301383904884(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__17083706301383904884(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17083706301383904884(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__17083706301383904884);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17083706301383904884);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__17083706301383904884);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__11341288172227452275 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11341288172227452275(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__11341288172227452275(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11341288172227452275(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11341288172227452275(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11341288172227452275(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__11341288172227452275(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11341288172227452275(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__11341288172227452275);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11341288172227452275);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__11341288172227452275);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__17257737100801030441 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      113 /* Input2 */, \
      113 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17257737100801030441(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 113} /* Input2 */, 
      {"input[3]", 113} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__17257737100801030441(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17257737100801030441(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17257737100801030441(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17257737100801030441(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__17257737100801030441(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17257737100801030441(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__17257737100801030441);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17257737100801030441);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__17257737100801030441);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__13301115791413218725 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      8192 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13301115791413218725(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 8192} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13301115791413218725(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13301115791413218725(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13301115791413218725(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13301115791413218725(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__13301115791413218725(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13301115791413218725(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13301115791413218725);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13301115791413218725);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__13301115791413218725);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__7331830275720028502 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      512 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7331830275720028502(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 512} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7331830275720028502(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7331830275720028502(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7331830275720028502(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7331830275720028502(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7331830275720028502(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7331830275720028502(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7331830275720028502);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7331830275720028502);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7331830275720028502);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12937232957488818445 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12937232957488818445(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12937232957488818445(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12937232957488818445(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12937232957488818445(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12937232957488818445(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__12937232957488818445(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12937232957488818445(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12937232957488818445);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12937232957488818445);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__12937232957488818445);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__10350668811418153723 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      32 /* Input0 */, \
      128 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10350668811418153723(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 32} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__10350668811418153723(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10350668811418153723(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10350668811418153723(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10350668811418153723(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__10350668811418153723(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10350668811418153723(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__10350668811418153723);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10350668811418153723);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__10350668811418153723);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__1905840818989748056 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1905840818989748056(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1905840818989748056(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1905840818989748056(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1905840818989748056(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1905840818989748056(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1905840818989748056(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1905840818989748056(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1905840818989748056);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1905840818989748056);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1905840818989748056);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__3928089933404660267 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      35184372088832 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3928089933404660267(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 35184372088832} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__3928089933404660267(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3928089933404660267(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3928089933404660267(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3928089933404660267(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__3928089933404660267(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3928089933404660267(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__3928089933404660267);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3928089933404660267);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__3928089933404660267);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace CUDNN_POOLING_FWD__14617176895170932749 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__14617176895170932749(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__14617176895170932749(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__14617176895170932749(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__14617176895170932749(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__14617176895170932749(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__14617176895170932749(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__14617176895170932749(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__14617176895170932749);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__14617176895170932749);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__14617176895170932749);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__13601861449485043888 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      512 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13601861449485043888(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 512} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13601861449485043888(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13601861449485043888(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13601861449485043888(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13601861449485043888(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13601861449485043888(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13601861449485043888(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13601861449485043888);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13601861449485043888);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13601861449485043888);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__11506361386688413402 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      256 /* Input0 */, \
      24 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11506361386688413402(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11506361386688413402(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11506361386688413402(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11506361386688413402(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11506361386688413402(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11506361386688413402(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11506361386688413402(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11506361386688413402);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11506361386688413402);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11506361386688413402);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__1564182305414152372 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1564182305414152372(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__1564182305414152372(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1564182305414152372(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1564182305414152372(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1564182305414152372(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__1564182305414152372(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1564182305414152372(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__1564182305414152372);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1564182305414152372);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__1564182305414152372);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__4859034479700741842 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      144 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      24 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4859034479700741842(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 24} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4859034479700741842(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4859034479700741842(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4859034479700741842(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4859034479700741842(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4859034479700741842(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4859034479700741842(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4859034479700741842);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4859034479700741842);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4859034479700741842);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5053390372838649256 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5053390372838649256(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5053390372838649256(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5053390372838649256(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5053390372838649256(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5053390372838649256(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5053390372838649256(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5053390372838649256(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5053390372838649256);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5053390372838649256);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5053390372838649256);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__9664424435614566194 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2048 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9664424435614566194(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2048} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9664424435614566194(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9664424435614566194(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9664424435614566194(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9664424435614566194(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9664424435614566194(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9664424435614566194(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9664424435614566194);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9664424435614566194);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9664424435614566194);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2584658034384630260 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      576 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2584658034384630260(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 576} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2584658034384630260(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2584658034384630260(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2584658034384630260(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2584658034384630260(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2584658034384630260(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2584658034384630260(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2584658034384630260);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2584658034384630260);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2584658034384630260);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10365513986194798311 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      2048 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      304 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      18 /* PadHeight */, \
      18 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      18 /* DilationWidth */, \
      18 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10365513986194798311(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 304} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 18} /* PadHeight */, 
      {"pad_width", 18} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 18} /* DilationWidth */, 
      {"dilation_width", 18} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10365513986194798311(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10365513986194798311(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10365513986194798311(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10365513986194798311(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10365513986194798311(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10365513986194798311(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10365513986194798311);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10365513986194798311);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10365513986194798311);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__11307590596450747825 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__11307590596450747825(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__11307590596450747825(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__11307590596450747825(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__11307590596450747825(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__11307590596450747825(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__11307590596450747825(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__11307590596450747825(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__11307590596450747825);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__11307590596450747825);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__11307590596450747825);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14215627713887287837 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      272 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14215627713887287837(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 272} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14215627713887287837(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14215627713887287837(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14215627713887287837(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14215627713887287837(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__14215627713887287837(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14215627713887287837(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14215627713887287837);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14215627713887287837);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__14215627713887287837);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__16273333040728204016 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      4503599627370496 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16273333040728204016(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4503599627370496} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__16273333040728204016(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16273333040728204016(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16273333040728204016(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16273333040728204016(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__16273333040728204016(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16273333040728204016(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__16273333040728204016);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16273333040728204016);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__16273333040728204016);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__6620720274870378721 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      32768 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6620720274870378721(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 32768} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6620720274870378721(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6620720274870378721(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6620720274870378721(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6620720274870378721(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6620720274870378721(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6620720274870378721(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6620720274870378721);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6620720274870378721);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6620720274870378721);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__18245232030457775330 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      16777216 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18245232030457775330(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16777216} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__18245232030457775330(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18245232030457775330(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18245232030457775330(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18245232030457775330(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18245232030457775330(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18245232030457775330(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__18245232030457775330);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18245232030457775330);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18245232030457775330);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__4322124907708576563 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      8192 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4322124907708576563(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 8192} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4322124907708576563(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4322124907708576563(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4322124907708576563(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4322124907708576563(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4322124907708576563(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4322124907708576563(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4322124907708576563);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4322124907708576563);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4322124907708576563);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11135640839213908029 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      32 /* Input0 */, \
      128 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11135640839213908029(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 32} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11135640839213908029(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11135640839213908029(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11135640839213908029(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11135640839213908029(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11135640839213908029(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11135640839213908029(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11135640839213908029);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11135640839213908029);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11135640839213908029);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__10609601243470722281 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      3072 /* Input0 */, \
      192 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10609601243470722281(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 3072} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__10609601243470722281(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10609601243470722281(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10609601243470722281(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10609601243470722281(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__10609601243470722281(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10609601243470722281(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__10609601243470722281);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10609601243470722281);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__10609601243470722281);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace CUDNN_POOLING_FWD__12560304980725834327 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__12560304980725834327(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__12560304980725834327(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__12560304980725834327(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__12560304980725834327(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__12560304980725834327(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__12560304980725834327(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__12560304980725834327(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__12560304980725834327);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__12560304980725834327);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__12560304980725834327);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__13713319501332909600 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1024 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13713319501332909600(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13713319501332909600(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13713319501332909600(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13713319501332909600(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13713319501332909600(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__13713319501332909600(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13713319501332909600(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13713319501332909600);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13713319501332909600);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__13713319501332909600);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12748226607727853913 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12748226607727853913(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12748226607727853913(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12748226607727853913(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12748226607727853913(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12748226607727853913(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__12748226607727853913(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12748226607727853913(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12748226607727853913);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12748226607727853913);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__12748226607727853913);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__7684071975151067717 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      384 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7684071975151067717(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7684071975151067717(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7684071975151067717(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7684071975151067717(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7684071975151067717(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7684071975151067717(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7684071975151067717(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7684071975151067717);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7684071975151067717);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7684071975151067717);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__13779835688777344837 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      96 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      96 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13779835688777344837(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13779835688777344837(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13779835688777344837(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13779835688777344837(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13779835688777344837(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13779835688777344837(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13779835688777344837(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13779835688777344837);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13779835688777344837);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13779835688777344837);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__3946527980509248159 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      113 /* Input2 */, \
      113 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__3946527980509248159(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 113} /* Input2 */, 
      {"input[3]", 113} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__3946527980509248159(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__3946527980509248159(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__3946527980509248159(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__3946527980509248159(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__3946527980509248159(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__3946527980509248159(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__3946527980509248159);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__3946527980509248159);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__3946527980509248159);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__5051402869941322273 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1099511627776 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5051402869941322273(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1099511627776} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__5051402869941322273(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5051402869941322273(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5051402869941322273(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5051402869941322273(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__5051402869941322273(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5051402869941322273(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__5051402869941322273);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5051402869941322273);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__5051402869941322273);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__9425059079054548626 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9425059079054548626(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__9425059079054548626(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9425059079054548626(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9425059079054548626(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9425059079054548626(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__9425059079054548626(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9425059079054548626(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__9425059079054548626);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9425059079054548626);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__9425059079054548626);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__15339746672715693844 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15339746672715693844(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15339746672715693844(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15339746672715693844(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15339746672715693844(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15339746672715693844(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15339746672715693844(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15339746672715693844(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15339746672715693844);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15339746672715693844);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15339746672715693844);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__7771360959227830460 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      288230376151711744 /* Input0 */, \
      512 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7771360959227830460(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 288230376151711744} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7771360959227830460(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7771360959227830460(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7771360959227830460(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7771360959227830460(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7771360959227830460(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7771360959227830460(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7771360959227830460);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7771360959227830460);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7771360959227830460);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__6407215479462882063 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6407215479462882063(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6407215479462882063(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6407215479462882063(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6407215479462882063(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6407215479462882063(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6407215479462882063(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6407215479462882063(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6407215479462882063);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6407215479462882063);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6407215479462882063);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__16964756741235058751 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      12288 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16964756741235058751(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 12288} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16964756741235058751(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16964756741235058751(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16964756741235058751(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16964756741235058751(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16964756741235058751(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16964756741235058751(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16964756741235058751);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16964756741235058751);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16964756741235058751);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__8865626807462998854 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      65536 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8865626807462998854(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__8865626807462998854(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8865626807462998854(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8865626807462998854(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8865626807462998854(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__8865626807462998854(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8865626807462998854(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__8865626807462998854);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8865626807462998854);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__8865626807462998854);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__6960170675217277259 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      4294967296 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6960170675217277259(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4294967296} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__6960170675217277259(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6960170675217277259(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6960170675217277259(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6960170675217277259(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__6960170675217277259(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6960170675217277259(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__6960170675217277259);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6960170675217277259);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__6960170675217277259);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__2403718942223608611 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      16 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2403718942223608611(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2403718942223608611(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2403718942223608611(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2403718942223608611(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2403718942223608611(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2403718942223608611(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2403718942223608611(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2403718942223608611);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2403718942223608611);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2403718942223608611);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__1780898040261865370 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      147456 /* Input0 */, \
      352 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1780898040261865370(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 147456} /* Input0 */, 
      {"input[1]", 352} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__1780898040261865370(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1780898040261865370(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1780898040261865370(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1780898040261865370(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__1780898040261865370(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1780898040261865370(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__1780898040261865370);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1780898040261865370);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__1780898040261865370);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9517389333854358352 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      128 /* Input0 */, \
      64 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9517389333854358352(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 128} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9517389333854358352(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9517389333854358352(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9517389333854358352(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9517389333854358352(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9517389333854358352(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9517389333854358352(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9517389333854358352);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9517389333854358352);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9517389333854358352);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__8789930440786207474 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      12288 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8789930440786207474(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 12288} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8789930440786207474(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8789930440786207474(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8789930440786207474(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8789930440786207474(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8789930440786207474(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8789930440786207474(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8789930440786207474);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8789930440786207474);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8789930440786207474);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__8005188750117647731 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__8005188750117647731(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__8005188750117647731(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__8005188750117647731(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__8005188750117647731(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__8005188750117647731(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__8005188750117647731(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__8005188750117647731(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__8005188750117647731);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__8005188750117647731);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__8005188750117647731);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12520458054090372392 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      17179869184 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12520458054090372392(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 17179869184} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12520458054090372392(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12520458054090372392(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12520458054090372392(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12520458054090372392(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__12520458054090372392(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12520458054090372392(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12520458054090372392);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12520458054090372392);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__12520458054090372392);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__17678781982088015611 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17678781982088015611(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17678781982088015611(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17678781982088015611(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17678781982088015611(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17678781982088015611(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17678781982088015611(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17678781982088015611(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17678781982088015611);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17678781982088015611);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17678781982088015611);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__407293111033910293 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      48 /* Input0 */, \
      64 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__407293111033910293(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 48} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__407293111033910293(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__407293111033910293(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__407293111033910293(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__407293111033910293(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__407293111033910293(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__407293111033910293(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__407293111033910293);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__407293111033910293);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__407293111033910293);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__14489811092418645213 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      64 /* Input0 */, \
      192 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14489811092418645213(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14489811092418645213(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14489811092418645213(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14489811092418645213(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14489811092418645213(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14489811092418645213(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14489811092418645213(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14489811092418645213);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14489811092418645213);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14489811092418645213);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__9253235393275048340 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      16 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9253235393275048340(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9253235393275048340(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9253235393275048340(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9253235393275048340(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9253235393275048340(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9253235393275048340(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9253235393275048340(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9253235393275048340);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9253235393275048340);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9253235393275048340);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__15750020629800872409 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      67108864 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15750020629800872409(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 67108864} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15750020629800872409(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15750020629800872409(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15750020629800872409(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15750020629800872409(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__15750020629800872409(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15750020629800872409(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15750020629800872409);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15750020629800872409);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__15750020629800872409);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__8352427061216989404 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      32 /* Input0 */, \
      192 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8352427061216989404(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 32} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8352427061216989404(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8352427061216989404(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8352427061216989404(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8352427061216989404(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8352427061216989404(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8352427061216989404(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8352427061216989404);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8352427061216989404);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8352427061216989404);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14887258662648526068 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      281474976710656 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14887258662648526068(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 281474976710656} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14887258662648526068(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14887258662648526068(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14887258662648526068(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14887258662648526068(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__14887258662648526068(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14887258662648526068(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14887258662648526068);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14887258662648526068);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__14887258662648526068);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__15922013049157630608 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      12288 /* Input0 */, \
      256 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15922013049157630608(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 12288} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15922013049157630608(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15922013049157630608(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15922013049157630608(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15922013049157630608(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__15922013049157630608(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15922013049157630608(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15922013049157630608);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15922013049157630608);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__15922013049157630608);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__2227691131149342533 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      36864 /* Input0 */, \
      128 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2227691131149342533(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 36864} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__2227691131149342533(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2227691131149342533(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2227691131149342533(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2227691131149342533(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__2227691131149342533(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2227691131149342533(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__2227691131149342533);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2227691131149342533);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__2227691131149342533);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__4597386566957697920 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      34359738368 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4597386566957697920(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 34359738368} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__4597386566957697920(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4597386566957697920(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4597386566957697920(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4597386566957697920(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__4597386566957697920(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4597386566957697920(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__4597386566957697920);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4597386566957697920);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__4597386566957697920);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__297381060854555202 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      48 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__297381060854555202(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 48} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__297381060854555202(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__297381060854555202(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__297381060854555202(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__297381060854555202(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__297381060854555202(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__297381060854555202(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__297381060854555202);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__297381060854555202);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__297381060854555202);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__5730962585163262634 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2097152 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5730962585163262634(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2097152} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5730962585163262634(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5730962585163262634(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5730962585163262634(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5730962585163262634(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5730962585163262634(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5730962585163262634(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5730962585163262634);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5730962585163262634);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5730962585163262634);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__11230656647148802316 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      147456 /* Input0 */, \
      352 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11230656647148802316(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 147456} /* Input0 */, 
      {"input[1]", 352} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11230656647148802316(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11230656647148802316(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11230656647148802316(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11230656647148802316(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11230656647148802316(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11230656647148802316(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11230656647148802316);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11230656647148802316);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11230656647148802316);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__16804066418679194145 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      192 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16804066418679194145(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 192} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__16804066418679194145(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16804066418679194145(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16804066418679194145(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16804066418679194145(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__16804066418679194145(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16804066418679194145(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__16804066418679194145);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16804066418679194145);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__16804066418679194145);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__5039782560672933715 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      144 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5039782560672933715(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__5039782560672933715(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5039782560672933715(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5039782560672933715(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5039782560672933715(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__5039782560672933715(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5039782560672933715(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__5039782560672933715);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5039782560672933715);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__5039782560672933715);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__9320966949310507568 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      192 /* Input0 */, \
      224 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9320966949310507568(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 192} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9320966949310507568(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9320966949310507568(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9320966949310507568(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9320966949310507568(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9320966949310507568(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9320966949310507568(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9320966949310507568);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9320966949310507568);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9320966949310507568);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__13166213926934369387 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13166213926934369387(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13166213926934369387(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13166213926934369387(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13166213926934369387(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13166213926934369387(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13166213926934369387(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13166213926934369387(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13166213926934369387);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13166213926934369387);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13166213926934369387);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4388697270336099763 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      64 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4388697270336099763(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4388697270336099763(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4388697270336099763(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4388697270336099763(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4388697270336099763(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4388697270336099763(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4388697270336099763(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4388697270336099763);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4388697270336099763);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4388697270336099763);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__4405422640982061238 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1024 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4405422640982061238(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4405422640982061238(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4405422640982061238(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4405422640982061238(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4405422640982061238(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4405422640982061238(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4405422640982061238(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4405422640982061238);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4405422640982061238);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4405422640982061238);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__4920831774247316997 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4920831774247316997(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4920831774247316997(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4920831774247316997(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4920831774247316997(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4920831774247316997(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4920831774247316997(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4920831774247316997(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4920831774247316997);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4920831774247316997);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4920831774247316997);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__17255726049061801485 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1048576 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17255726049061801485(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1048576} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17255726049061801485(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17255726049061801485(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17255726049061801485(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17255726049061801485(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17255726049061801485(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17255726049061801485(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17255726049061801485);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17255726049061801485);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17255726049061801485);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__3761829803643680247 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      113 /* Input2 */, \
      113 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3761829803643680247(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 113} /* Input2 */, 
      {"input[3]", 113} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__3761829803643680247(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3761829803643680247(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3761829803643680247(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3761829803643680247(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__3761829803643680247(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3761829803643680247(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__3761829803643680247);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3761829803643680247);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__3761829803643680247);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace CUDNN_POOLING_FWD__5354295476941134909 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__5354295476941134909(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__5354295476941134909(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__5354295476941134909(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__5354295476941134909(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__5354295476941134909(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__5354295476941134909(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__5354295476941134909(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__5354295476941134909);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__5354295476941134909);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__5354295476941134909);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__7217889319705327543 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      12288 /* Input0 */, \
      192 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7217889319705327543(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 12288} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7217889319705327543(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7217889319705327543(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7217889319705327543(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7217889319705327543(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7217889319705327543(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7217889319705327543(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7217889319705327543);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7217889319705327543);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7217889319705327543);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__3565542181019276480 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3565542181019276480(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3565542181019276480(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3565542181019276480(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3565542181019276480(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3565542181019276480(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3565542181019276480(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3565542181019276480(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3565542181019276480);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3565542181019276480);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3565542181019276480);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__1405028433773198762 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      64 /* Input0 */, \
      192 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1405028433773198762(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1405028433773198762(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1405028433773198762(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1405028433773198762(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1405028433773198762(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1405028433773198762(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1405028433773198762(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1405028433773198762);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1405028433773198762);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1405028433773198762);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5366393086785912001 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      96 /* FilterCount */, \
      11 /* FilterHeight */, \
      11 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      4 /* StrideHeight */, \
      4 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5366393086785912001(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 11} /* FilterHeight */, 
      {"filter_width", 11} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 4} /* StrideHeight */, 
      {"stride_width", 4} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5366393086785912001(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5366393086785912001(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5366393086785912001(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5366393086785912001(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5366393086785912001(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5366393086785912001(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5366393086785912001);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5366393086785912001);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5366393086785912001);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__1608392511481186337 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      12288 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__1608392511481186337(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 12288} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__1608392511481186337(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__1608392511481186337(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__1608392511481186337(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__1608392511481186337(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__1608392511481186337(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__1608392511481186337(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__1608392511481186337);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__1608392511481186337);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__1608392511481186337);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2192309203456471198 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2192309203456471198(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2192309203456471198(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2192309203456471198(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2192309203456471198(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2192309203456471198(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2192309203456471198(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2192309203456471198(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2192309203456471198);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2192309203456471198);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2192309203456471198);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__13266056775856364897 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13266056775856364897(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13266056775856364897(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13266056775856364897(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13266056775856364897(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13266056775856364897(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13266056775856364897(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13266056775856364897(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13266056775856364897);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13266056775856364897);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13266056775856364897);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__82072831508324055 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      48 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__82072831508324055(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 48} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__82072831508324055(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__82072831508324055(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__82072831508324055(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__82072831508324055(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__82072831508324055(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__82072831508324055(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__82072831508324055);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__82072831508324055);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__82072831508324055);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__6802837762227367988 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      562949953421312 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6802837762227367988(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 562949953421312} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6802837762227367988(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6802837762227367988(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6802837762227367988(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6802837762227367988(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6802837762227367988(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6802837762227367988(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6802837762227367988);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6802837762227367988);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6802837762227367988);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8655757863901543148 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      128 /* Input0 */, \
      64 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8655757863901543148(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 128} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8655757863901543148(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8655757863901543148(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8655757863901543148(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8655757863901543148(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8655757863901543148(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8655757863901543148(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8655757863901543148);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8655757863901543148);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8655757863901543148);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__9458825591017171220 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      524288 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9458825591017171220(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 524288} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9458825591017171220(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9458825591017171220(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9458825591017171220(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9458825591017171220(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9458825591017171220(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9458825591017171220(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9458825591017171220);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9458825591017171220);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9458825591017171220);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_DROPOUT_FWD__17779027193153028564 {

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS() \
  Args({{ \
      256 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__17779027193153028564(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


static void LAYER_CUDNN_DROPOUT_FWD_FLOAT16__17779027193153028564(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<__half, batchnorm_mode, is_training>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__17779027193153028564(state);
}

static void LAYER_CUDNN_DROPOUT_FWD_FLOAT32__17779027193153028564(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<float, batchnorm_mode, is_training>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__17779027193153028564(state);
}

static void LAYER_CUDNN_DROPOUT_FWD_TENSORCOREHALF__17779027193153028564(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<__half, batchnorm_mode, is_training>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__17779027193153028564(state);
}




BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT16__17779027193153028564);

BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT32__17779027193153028564);

BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_TENSORCOREHALF__17779027193153028564);

#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES
}

namespace LAYER_CUDNN_ACTIVATION_FWD__1002433645399762284 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      262144 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1002433645399762284(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 262144} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1002433645399762284(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1002433645399762284(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1002433645399762284(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1002433645399762284(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1002433645399762284(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1002433645399762284(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1002433645399762284);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1002433645399762284);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1002433645399762284);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11221556613953088006 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      57 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11221556613953088006(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11221556613953088006(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11221556613953088006(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11221556613953088006(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11221556613953088006(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11221556613953088006(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11221556613953088006(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11221556613953088006);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11221556613953088006);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11221556613953088006);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__3841713278929955612 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      112 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3841713278929955612(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3841713278929955612(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3841713278929955612(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3841713278929955612(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3841713278929955612(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3841713278929955612(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3841713278929955612(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3841713278929955612);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3841713278929955612);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3841713278929955612);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__4089496564252578481 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4089496564252578481(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4089496564252578481(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4089496564252578481(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4089496564252578481(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4089496564252578481(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4089496564252578481(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4089496564252578481(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4089496564252578481);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4089496564252578481);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4089496564252578481);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__11098242710275693140 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      131072 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11098242710275693140(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 131072} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11098242710275693140(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11098242710275693140(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11098242710275693140(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11098242710275693140(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11098242710275693140(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11098242710275693140(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11098242710275693140);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11098242710275693140);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11098242710275693140);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__15286598053047680724 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      17592186044416 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15286598053047680724(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 17592186044416} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15286598053047680724(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15286598053047680724(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15286598053047680724(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15286598053047680724(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15286598053047680724(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15286598053047680724(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15286598053047680724);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15286598053047680724);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15286598053047680724);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14369965003954337411 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      16 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14369965003954337411(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14369965003954337411(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14369965003954337411(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14369965003954337411(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14369965003954337411(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14369965003954337411(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14369965003954337411(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14369965003954337411);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14369965003954337411);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14369965003954337411);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__9516206706028465084 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      256 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9516206706028465084(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9516206706028465084(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9516206706028465084(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9516206706028465084(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9516206706028465084(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9516206706028465084(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9516206706028465084(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9516206706028465084);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9516206706028465084);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9516206706028465084);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__275778139438682559 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      48 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__275778139438682559(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 48} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__275778139438682559(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__275778139438682559(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__275778139438682559(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__275778139438682559(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__275778139438682559(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__275778139438682559(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__275778139438682559);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__275778139438682559);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__275778139438682559);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12540304554273859113 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      3072 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12540304554273859113(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 3072} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12540304554273859113(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12540304554273859113(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12540304554273859113(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12540304554273859113(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__12540304554273859113(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12540304554273859113(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12540304554273859113);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12540304554273859113);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__12540304554273859113);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__11350780410111044051 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      36864 /* Input0 */, \
      128 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11350780410111044051(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 36864} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11350780410111044051(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11350780410111044051(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11350780410111044051(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11350780410111044051(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11350780410111044051(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11350780410111044051(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11350780410111044051);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11350780410111044051);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11350780410111044051);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6747703657588446902 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      36864 /* Input0 */, \
      192 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      320 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6747703657588446902(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 36864} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 320} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6747703657588446902(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6747703657588446902(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6747703657588446902(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6747703657588446902(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6747703657588446902(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6747703657588446902(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6747703657588446902);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6747703657588446902);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6747703657588446902);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6678057746326470081 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      262144 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6678057746326470081(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 262144} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6678057746326470081(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6678057746326470081(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6678057746326470081(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6678057746326470081(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6678057746326470081(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6678057746326470081(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6678057746326470081);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6678057746326470081);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6678057746326470081);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8096784436491291010 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      281474976710656 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8096784436491291010(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 281474976710656} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8096784436491291010(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8096784436491291010(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8096784436491291010(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8096784436491291010(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8096784436491291010(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8096784436491291010(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8096784436491291010);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8096784436491291010);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8096784436491291010);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__2299993332840428661 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      18014398509481984 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2299993332840428661(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 18014398509481984} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2299993332840428661(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2299993332840428661(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2299993332840428661(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2299993332840428661(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2299993332840428661(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2299993332840428661(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2299993332840428661);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2299993332840428661);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2299993332840428661);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__15263633532097929288 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      112 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      112 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15263633532097929288(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 112} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15263633532097929288(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15263633532097929288(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15263633532097929288(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15263633532097929288(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15263633532097929288(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15263633532097929288(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15263633532097929288);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15263633532097929288);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15263633532097929288);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__3447688412795220271 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__3447688412795220271(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__3447688412795220271(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__3447688412795220271(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__3447688412795220271(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__3447688412795220271(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__3447688412795220271(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__3447688412795220271(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__3447688412795220271);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__3447688412795220271);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__3447688412795220271);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__6899181686698475887 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6899181686698475887(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6899181686698475887(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6899181686698475887(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6899181686698475887(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6899181686698475887(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6899181686698475887(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6899181686698475887(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6899181686698475887);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6899181686698475887);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6899181686698475887);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__3712803848064942031 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3712803848064942031(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3712803848064942031(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3712803848064942031(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3712803848064942031(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3712803848064942031(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3712803848064942031(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3712803848064942031(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3712803848064942031);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3712803848064942031);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3712803848064942031);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__6829980159518547193 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      96 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6829980159518547193(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__6829980159518547193(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6829980159518547193(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6829980159518547193(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6829980159518547193(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__6829980159518547193(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6829980159518547193(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__6829980159518547193);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6829980159518547193);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__6829980159518547193);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__11265013007748398978 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      768 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11265013007748398978(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 768} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__11265013007748398978(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11265013007748398978(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11265013007748398978(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11265013007748398978(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__11265013007748398978(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11265013007748398978(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__11265013007748398978);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11265013007748398978);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__11265013007748398978);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__9534624682206767500 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      128 /* Input0 */, \
      64 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9534624682206767500(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 128} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9534624682206767500(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9534624682206767500(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9534624682206767500(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9534624682206767500(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9534624682206767500(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9534624682206767500(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9534624682206767500);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9534624682206767500);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9534624682206767500);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4524747795653081339 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      33554432 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4524747795653081339(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 33554432} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4524747795653081339(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4524747795653081339(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4524747795653081339(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4524747795653081339(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4524747795653081339(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4524747795653081339(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4524747795653081339);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4524747795653081339);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4524747795653081339);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__7357773125161024820 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      65536 /* Input0 */, \
      48 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7357773125161024820(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7357773125161024820(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7357773125161024820(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7357773125161024820(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7357773125161024820(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7357773125161024820(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7357773125161024820(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7357773125161024820);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7357773125161024820);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7357773125161024820);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__18315675101082491532 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      128 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18315675101082491532(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18315675101082491532(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18315675101082491532(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18315675101082491532(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18315675101082491532(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__18315675101082491532(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18315675101082491532(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18315675101082491532);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18315675101082491532);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__18315675101082491532);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__14414372110732348732 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      68719476736 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14414372110732348732(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 68719476736} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14414372110732348732(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14414372110732348732(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14414372110732348732(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14414372110732348732(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14414372110732348732(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14414372110732348732(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14414372110732348732);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14414372110732348732);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14414372110732348732);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__8097601368122251127 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      112 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8097601368122251127(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8097601368122251127(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8097601368122251127(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8097601368122251127(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8097601368122251127(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8097601368122251127(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8097601368122251127(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8097601368122251127);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8097601368122251127);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8097601368122251127);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__10597833503999254613 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      12288 /* Input0 */, \
      192 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10597833503999254613(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 12288} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__10597833503999254613(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10597833503999254613(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10597833503999254613(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10597833503999254613(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__10597833503999254613(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10597833503999254613(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__10597833503999254613);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10597833503999254613);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__10597833503999254613);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__4363449296233128570 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4363449296233128570(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4363449296233128570(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4363449296233128570(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4363449296233128570(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4363449296233128570(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4363449296233128570(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4363449296233128570(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4363449296233128570);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4363449296233128570);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4363449296233128570);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12311971014252716959 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      32 /* Input0 */, \
      192 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      48 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12311971014252716959(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 32} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12311971014252716959(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12311971014252716959(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12311971014252716959(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12311971014252716959(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12311971014252716959(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12311971014252716959(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12311971014252716959);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12311971014252716959);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12311971014252716959);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__9025797036715107354 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      128 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9025797036715107354(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__9025797036715107354(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9025797036715107354(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9025797036715107354(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9025797036715107354(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__9025797036715107354(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9025797036715107354(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__9025797036715107354);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9025797036715107354);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__9025797036715107354);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__12816687056561553363 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      64 /* Input0 */, \
      24 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12816687056561553363(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12816687056561553363(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12816687056561553363(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12816687056561553363(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12816687056561553363(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12816687056561553363(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12816687056561553363(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12816687056561553363);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12816687056561553363);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12816687056561553363);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__3363574984565607358 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      17179869184 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3363574984565607358(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 17179869184} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3363574984565607358(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3363574984565607358(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3363574984565607358(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3363574984565607358(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3363574984565607358(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3363574984565607358(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3363574984565607358);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3363574984565607358);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3363574984565607358);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__18437691963492759572 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      96 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18437691963492759572(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__18437691963492759572(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18437691963492759572(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18437691963492759572(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18437691963492759572(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18437691963492759572(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18437691963492759572(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__18437691963492759572);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18437691963492759572);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18437691963492759572);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12328117435450172137 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      3072 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12328117435450172137(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 3072} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12328117435450172137(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12328117435450172137(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12328117435450172137(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12328117435450172137(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12328117435450172137(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12328117435450172137(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12328117435450172137);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12328117435450172137);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12328117435450172137);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11364830047598812542 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      128 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11364830047598812542(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 128} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11364830047598812542(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11364830047598812542(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11364830047598812542(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11364830047598812542(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11364830047598812542(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11364830047598812542(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11364830047598812542);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11364830047598812542);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11364830047598812542);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__16180798846947037101 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      256 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16180798846947037101(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16180798846947037101(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16180798846947037101(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16180798846947037101(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16180798846947037101(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16180798846947037101(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16180798846947037101(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16180798846947037101);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16180798846947037101);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16180798846947037101);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__1703787912541473102 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      17179869184 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1703787912541473102(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 17179869184} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1703787912541473102(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1703787912541473102(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1703787912541473102(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1703787912541473102(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1703787912541473102(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1703787912541473102(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1703787912541473102);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1703787912541473102);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1703787912541473102);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__8119630370505607067 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      48 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8119630370505607067(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 48} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__8119630370505607067(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8119630370505607067(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8119630370505607067(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8119630370505607067(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__8119630370505607067(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8119630370505607067(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__8119630370505607067);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8119630370505607067);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__8119630370505607067);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__8014471916080716907 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8014471916080716907(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__8014471916080716907(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8014471916080716907(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8014471916080716907(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8014471916080716907(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__8014471916080716907(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8014471916080716907(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__8014471916080716907);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8014471916080716907);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__8014471916080716907);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__5096481279428734828 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      14 /* Input3 */, \
      384 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5096481279428734828(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5096481279428734828(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5096481279428734828(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5096481279428734828(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5096481279428734828(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5096481279428734828(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5096481279428734828(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5096481279428734828);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5096481279428734828);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5096481279428734828);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__14651091710050527114 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      36864 /* Input0 */, \
      224 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14651091710050527114(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 36864} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14651091710050527114(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14651091710050527114(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14651091710050527114(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14651091710050527114(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14651091710050527114(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14651091710050527114(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14651091710050527114);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14651091710050527114);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14651091710050527114);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__15604354646899973751 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      32768 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15604354646899973751(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 32768} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15604354646899973751(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15604354646899973751(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15604354646899973751(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15604354646899973751(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__15604354646899973751(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15604354646899973751(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15604354646899973751);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15604354646899973751);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__15604354646899973751);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__6706427800708133317 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      36028797018963968 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6706427800708133317(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 36028797018963968} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6706427800708133317(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6706427800708133317(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6706427800708133317(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6706427800708133317(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6706427800708133317(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6706427800708133317(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6706427800708133317);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6706427800708133317);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6706427800708133317);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__13356782153454117711 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__13356782153454117711(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__13356782153454117711(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__13356782153454117711(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__13356782153454117711(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__13356782153454117711(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__13356782153454117711(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__13356782153454117711(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__13356782153454117711);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__13356782153454117711);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__13356782153454117711);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14061612534742206392 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14061612534742206392(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14061612534742206392(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14061612534742206392(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14061612534742206392(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14061612534742206392(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14061612534742206392(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14061612534742206392(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14061612534742206392);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14061612534742206392);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14061612534742206392);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11274389816593092323 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      4398046511104 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11274389816593092323(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4398046511104} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11274389816593092323(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11274389816593092323(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11274389816593092323(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11274389816593092323(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11274389816593092323(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11274389816593092323(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11274389816593092323);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11274389816593092323);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11274389816593092323);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4204747075385250447 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2199023255552 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4204747075385250447(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2199023255552} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4204747075385250447(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4204747075385250447(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4204747075385250447(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4204747075385250447(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4204747075385250447(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4204747075385250447(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4204747075385250447);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4204747075385250447);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4204747075385250447);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12937296503356461428 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      200 /* Input2 */, \
      200 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12937296503356461428(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 200} /* Input2 */, 
      {"input[3]", 200} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12937296503356461428(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12937296503356461428(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12937296503356461428(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12937296503356461428(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12937296503356461428(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12937296503356461428(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12937296503356461428);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12937296503356461428);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12937296503356461428);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__18399244130207701921 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18399244130207701921(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18399244130207701921(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18399244130207701921(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18399244130207701921(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18399244130207701921(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__18399244130207701921(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18399244130207701921(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18399244130207701921);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18399244130207701921);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__18399244130207701921);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__16388336239713007064 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      384 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16388336239713007064(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16388336239713007064(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16388336239713007064(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16388336239713007064(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16388336239713007064(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16388336239713007064(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16388336239713007064(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16388336239713007064);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16388336239713007064);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16388336239713007064);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5742646041131479218 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5742646041131479218(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5742646041131479218(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5742646041131479218(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5742646041131479218(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5742646041131479218(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5742646041131479218(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5742646041131479218(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5742646041131479218);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5742646041131479218);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5742646041131479218);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__452360821537151075 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__452360821537151075(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__452360821537151075(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__452360821537151075(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__452360821537151075(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__452360821537151075(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__452360821537151075(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__452360821537151075(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__452360821537151075);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__452360821537151075);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__452360821537151075);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__14390166043040606333 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      96 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14390166043040606333(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14390166043040606333(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14390166043040606333(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14390166043040606333(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14390166043040606333(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14390166043040606333(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14390166043040606333(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14390166043040606333);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14390166043040606333);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14390166043040606333);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__11279120282157583075 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      18014398509481984 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11279120282157583075(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 18014398509481984} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__11279120282157583075(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11279120282157583075(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11279120282157583075(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11279120282157583075(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__11279120282157583075(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11279120282157583075(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__11279120282157583075);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11279120282157583075);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__11279120282157583075);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace CUDNN_POOLING_FWD__16555695108537545364 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      64 /* Input1 */, \
      104 /* Input2 */, \
      104 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__16555695108537545364(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 104} /* Input2 */, 
      {"input[3]", 104} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__16555695108537545364(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__16555695108537545364(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__16555695108537545364(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__16555695108537545364(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__16555695108537545364(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__16555695108537545364(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__16555695108537545364);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__16555695108537545364);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__16555695108537545364);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace CUDNN_POOLING_FWD__1895319401089177154 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__1895319401089177154(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__1895319401089177154(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__1895319401089177154(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__1895319401089177154(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__1895319401089177154(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__1895319401089177154(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__1895319401089177154(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__1895319401089177154);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__1895319401089177154);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__1895319401089177154);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__5124877110649499345 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      16384 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5124877110649499345(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5124877110649499345(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5124877110649499345(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5124877110649499345(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5124877110649499345(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5124877110649499345(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5124877110649499345(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5124877110649499345);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5124877110649499345);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5124877110649499345);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9739250583524800298 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9739250583524800298(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9739250583524800298(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9739250583524800298(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9739250583524800298(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9739250583524800298(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9739250583524800298(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9739250583524800298(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9739250583524800298);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9739250583524800298);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9739250583524800298);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__16662498195990872282 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16662498195990872282(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16662498195990872282(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16662498195990872282(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16662498195990872282(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16662498195990872282(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16662498195990872282(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16662498195990872282(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16662498195990872282);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16662498195990872282);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16662498195990872282);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__8951877252663534903 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8951877252663534903(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__8951877252663534903(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8951877252663534903(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8951877252663534903(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8951877252663534903(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__8951877252663534903(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8951877252663534903(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__8951877252663534903);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8951877252663534903);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__8951877252663534903);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__10700378419537727889 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10700378419537727889(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10700378419537727889(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10700378419537727889(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10700378419537727889(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10700378419537727889(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10700378419537727889(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10700378419537727889(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10700378419537727889);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10700378419537727889);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10700378419537727889);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__8936627370767615007 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      256 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8936627370767615007(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__8936627370767615007(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8936627370767615007(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8936627370767615007(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8936627370767615007(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__8936627370767615007(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8936627370767615007(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__8936627370767615007);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8936627370767615007);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__8936627370767615007);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__3102203540803586070 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      36864 /* Input0 */, \
      192 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3102203540803586070(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 36864} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3102203540803586070(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3102203540803586070(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3102203540803586070(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3102203540803586070(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3102203540803586070(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3102203540803586070(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3102203540803586070);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3102203540803586070);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3102203540803586070);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__1618986998254318739 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      96 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1618986998254318739(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__1618986998254318739(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1618986998254318739(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1618986998254318739(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1618986998254318739(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__1618986998254318739(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1618986998254318739(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__1618986998254318739);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1618986998254318739);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__1618986998254318739);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__16838043435438407865 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      262144 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16838043435438407865(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 262144} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16838043435438407865(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16838043435438407865(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16838043435438407865(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16838043435438407865(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16838043435438407865(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16838043435438407865(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16838043435438407865);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16838043435438407865);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16838043435438407865);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__5446795974066621561 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5446795974066621561(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5446795974066621561(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5446795974066621561(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5446795974066621561(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5446795974066621561(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5446795974066621561(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5446795974066621561(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5446795974066621561);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5446795974066621561);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5446795974066621561);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__810569457766651350 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      589824 /* Input0 */, \
      352 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__810569457766651350(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 589824} /* Input0 */, 
      {"input[1]", 352} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__810569457766651350(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__810569457766651350(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__810569457766651350(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__810569457766651350(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__810569457766651350(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__810569457766651350(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__810569457766651350);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__810569457766651350);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__810569457766651350);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__7203799601740921304 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      147456 /* Input0 */, \
      224 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      224 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7203799601740921304(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 147456} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 224} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7203799601740921304(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7203799601740921304(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7203799601740921304(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7203799601740921304(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7203799601740921304(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7203799601740921304(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7203799601740921304);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7203799601740921304);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7203799601740921304);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12632714896492852592 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12632714896492852592(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12632714896492852592(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12632714896492852592(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12632714896492852592(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12632714896492852592(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12632714896492852592(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12632714896492852592(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12632714896492852592);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12632714896492852592);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12632714896492852592);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14774443044856743328 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      8192 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14774443044856743328(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 8192} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14774443044856743328(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14774443044856743328(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14774443044856743328(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14774443044856743328(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14774443044856743328(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14774443044856743328(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14774443044856743328);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14774443044856743328);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14774443044856743328);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10835407164820463326 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10835407164820463326(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10835407164820463326(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10835407164820463326(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10835407164820463326(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10835407164820463326(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10835407164820463326(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10835407164820463326(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10835407164820463326);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10835407164820463326);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10835407164820463326);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__3270835367085119679 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      3072 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3270835367085119679(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 3072} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3270835367085119679(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3270835367085119679(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3270835367085119679(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3270835367085119679(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3270835367085119679(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3270835367085119679(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3270835367085119679);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3270835367085119679);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3270835367085119679);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__6517863916034588085 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__6517863916034588085(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__6517863916034588085(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__6517863916034588085(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__6517863916034588085(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__6517863916034588085(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__6517863916034588085(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__6517863916034588085(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__6517863916034588085);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__6517863916034588085);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__6517863916034588085);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__7881020433013937717 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      48 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7881020433013937717(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7881020433013937717(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7881020433013937717(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7881020433013937717(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7881020433013937717(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7881020433013937717(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7881020433013937717(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7881020433013937717);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7881020433013937717);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7881020433013937717);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__15926068612481978018 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      562949953421312 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15926068612481978018(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 562949953421312} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15926068612481978018(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15926068612481978018(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15926068612481978018(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15926068612481978018(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__15926068612481978018(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15926068612481978018(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15926068612481978018);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15926068612481978018);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__15926068612481978018);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__10712544777580007185 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      1024 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10712544777580007185(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10712544777580007185(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10712544777580007185(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10712544777580007185(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10712544777580007185(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10712544777580007185(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10712544777580007185(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10712544777580007185);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10712544777580007185);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10712544777580007185);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__13720964137071174689 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      160 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13720964137071174689(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13720964137071174689(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13720964137071174689(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13720964137071174689(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13720964137071174689(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13720964137071174689(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13720964137071174689(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13720964137071174689);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13720964137071174689);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13720964137071174689);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__12561930023214627483 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12561930023214627483(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12561930023214627483(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12561930023214627483(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12561930023214627483(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12561930023214627483(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12561930023214627483(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12561930023214627483(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12561930023214627483);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12561930023214627483);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12561930023214627483);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__15269451703654032537 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      36028797018963968 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15269451703654032537(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 36028797018963968} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15269451703654032537(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15269451703654032537(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15269451703654032537(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15269451703654032537(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__15269451703654032537(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15269451703654032537(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15269451703654032537);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15269451703654032537);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__15269451703654032537);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14369088160618618010 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      137438953472 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14369088160618618010(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 137438953472} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14369088160618618010(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14369088160618618010(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14369088160618618010(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14369088160618618010(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__14369088160618618010(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14369088160618618010(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14369088160618618010);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14369088160618618010);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__14369088160618618010);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__5831835717591582988 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      64 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5831835717591582988(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5831835717591582988(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5831835717591582988(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5831835717591582988(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5831835717591582988(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5831835717591582988(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5831835717591582988(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5831835717591582988);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5831835717591582988);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5831835717591582988);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__2409302640648556972 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      9007199254740992 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2409302640648556972(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 9007199254740992} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__2409302640648556972(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2409302640648556972(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2409302640648556972(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2409302640648556972(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__2409302640648556972(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2409302640648556972(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__2409302640648556972);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2409302640648556972);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__2409302640648556972);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__6104745315676708552 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      768 /* Input0 */, \
      192 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6104745315676708552(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 768} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6104745315676708552(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6104745315676708552(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6104745315676708552(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6104745315676708552(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6104745315676708552(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6104745315676708552(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6104745315676708552);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6104745315676708552);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6104745315676708552);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10070408172932557599 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      3072 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10070408172932557599(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 3072} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10070408172932557599(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10070408172932557599(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10070408172932557599(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10070408172932557599(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10070408172932557599(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10070408172932557599(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10070408172932557599);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10070408172932557599);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10070408172932557599);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__10013819964896374623 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      64 /* Input0 */, \
      112 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10013819964896374623(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10013819964896374623(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10013819964896374623(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10013819964896374623(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10013819964896374623(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10013819964896374623(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10013819964896374623(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10013819964896374623);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10013819964896374623);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10013819964896374623);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__8146139807556315178 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      48 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8146139807556315178(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 48} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__8146139807556315178(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8146139807556315178(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8146139807556315178(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8146139807556315178(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__8146139807556315178(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8146139807556315178(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__8146139807556315178);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8146139807556315178);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__8146139807556315178);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__16407813255821131907 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      274877906944 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16407813255821131907(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 274877906944} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__16407813255821131907(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16407813255821131907(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16407813255821131907(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16407813255821131907(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__16407813255821131907(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16407813255821131907(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__16407813255821131907);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16407813255821131907);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__16407813255821131907);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14055393231058866118 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      8388608 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14055393231058866118(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 8388608} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14055393231058866118(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14055393231058866118(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14055393231058866118(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14055393231058866118(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__14055393231058866118(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14055393231058866118(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14055393231058866118);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14055393231058866118);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__14055393231058866118);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__1916824563274200655 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      4294967296 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1916824563274200655(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4294967296} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1916824563274200655(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1916824563274200655(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1916824563274200655(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1916824563274200655(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1916824563274200655(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1916824563274200655(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1916824563274200655);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1916824563274200655);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1916824563274200655);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__7165645549127097909 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2199023255552 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7165645549127097909(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2199023255552} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__7165645549127097909(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7165645549127097909(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7165645549127097909(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7165645549127097909(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__7165645549127097909(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7165645549127097909(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__7165645549127097909);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7165645549127097909);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__7165645549127097909);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__11674557997430326347 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1125899906842624 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11674557997430326347(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1125899906842624} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11674557997430326347(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11674557997430326347(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11674557997430326347(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11674557997430326347(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11674557997430326347(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11674557997430326347(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11674557997430326347);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11674557997430326347);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11674557997430326347);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10446132015701370279 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      18014398509481984 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10446132015701370279(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 18014398509481984} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10446132015701370279(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10446132015701370279(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10446132015701370279(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10446132015701370279(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10446132015701370279(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10446132015701370279(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10446132015701370279);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10446132015701370279);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10446132015701370279);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12588517614440524400 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      8 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      16 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12588517614440524400(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 8} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12588517614440524400(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12588517614440524400(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12588517614440524400(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12588517614440524400(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12588517614440524400(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12588517614440524400(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12588517614440524400);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12588517614440524400);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12588517614440524400);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__1743040015017177309 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      4194304 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1743040015017177309(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4194304} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__1743040015017177309(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1743040015017177309(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1743040015017177309(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1743040015017177309(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__1743040015017177309(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1743040015017177309(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__1743040015017177309);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1743040015017177309);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__1743040015017177309);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__4901335237405041164 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      137438953472 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4901335237405041164(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 137438953472} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4901335237405041164(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4901335237405041164(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4901335237405041164(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4901335237405041164(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4901335237405041164(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4901335237405041164(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4901335237405041164);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4901335237405041164);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4901335237405041164);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__8823071701881851384 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      32 /* Input1 */, \
      208 /* Input2 */, \
      208 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__8823071701881851384(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 208} /* Input2 */, 
      {"input[3]", 208} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__8823071701881851384(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__8823071701881851384(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__8823071701881851384(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__8823071701881851384(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__8823071701881851384(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__8823071701881851384(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__8823071701881851384);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__8823071701881851384);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__8823071701881851384);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__6672530044887954045 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      8 /* Input0 */, \
      128 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6672530044887954045(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 8} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6672530044887954045(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6672530044887954045(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6672530044887954045(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6672530044887954045(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6672530044887954045(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6672530044887954045(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6672530044887954045);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6672530044887954045);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6672530044887954045);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__13166213926934369387 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13166213926934369387(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13166213926934369387(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13166213926934369387(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13166213926934369387(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13166213926934369387(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13166213926934369387(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13166213926934369387(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13166213926934369387);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13166213926934369387);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13166213926934369387);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10239840509892715385 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      64 /* Input0 */, \
      64 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10239840509892715385(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10239840509892715385(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10239840509892715385(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10239840509892715385(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10239840509892715385(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10239840509892715385(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10239840509892715385(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10239840509892715385);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10239840509892715385);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10239840509892715385);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12520473409532553065 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12520473409532553065(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12520473409532553065(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12520473409532553065(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12520473409532553065(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12520473409532553065(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12520473409532553065(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12520473409532553065(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12520473409532553065);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12520473409532553065);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12520473409532553065);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_DROPOUT_FWD__2871951751636546193 {

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      4096 /* Input1 */, \
      -1 /* Input2 */, \
      -1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__2871951751636546193(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 4096} /* Input1 */, 
      {"input[2]", -1} /* Input2 */, 
      {"input[3]", -1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


static void LAYER_CUDNN_DROPOUT_FWD_FLOAT16__2871951751636546193(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<__half, batchnorm_mode, is_training>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__2871951751636546193(state);
}

static void LAYER_CUDNN_DROPOUT_FWD_FLOAT32__2871951751636546193(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<float, batchnorm_mode, is_training>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__2871951751636546193(state);
}

static void LAYER_CUDNN_DROPOUT_FWD_TENSORCOREHALF__2871951751636546193(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<__half, batchnorm_mode, is_training>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__2871951751636546193(state);
}




BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT16__2871951751636546193);

BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT32__2871951751636546193);

BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_TENSORCOREHALF__2871951751636546193);

#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES
}

namespace CUDNN_POOLING_FWD__18022442904521906734 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      16384 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__18022442904521906734(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__18022442904521906734(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__18022442904521906734(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__18022442904521906734(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__18022442904521906734(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__18022442904521906734(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__18022442904521906734(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__18022442904521906734);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__18022442904521906734);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__18022442904521906734);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__16332815700675076780 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      288230376151711744 /* Input0 */, \
      512 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16332815700675076780(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 288230376151711744} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16332815700675076780(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16332815700675076780(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16332815700675076780(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16332815700675076780(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16332815700675076780(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16332815700675076780(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16332815700675076780);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16332815700675076780);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16332815700675076780);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__504231566777163298 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      65536 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__504231566777163298(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__504231566777163298(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__504231566777163298(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__504231566777163298(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__504231566777163298(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__504231566777163298(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__504231566777163298(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__504231566777163298);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__504231566777163298);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__504231566777163298);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__7919706177502132763 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7919706177502132763(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__7919706177502132763(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7919706177502132763(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7919706177502132763(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7919706177502132763(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__7919706177502132763(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7919706177502132763(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__7919706177502132763);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7919706177502132763);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__7919706177502132763);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__16798332293280333607 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2048 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16798332293280333607(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2048} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__16798332293280333607(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16798332293280333607(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16798332293280333607(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16798332293280333607(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__16798332293280333607(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16798332293280333607(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__16798332293280333607);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16798332293280333607);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__16798332293280333607);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__10665926225742682873 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      140737488355328 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10665926225742682873(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 140737488355328} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10665926225742682873(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10665926225742682873(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10665926225742682873(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10665926225742682873(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10665926225742682873(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10665926225742682873(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10665926225742682873);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10665926225742682873);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10665926225742682873);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__10511017113007540481 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      3 /* Input0 */, \
      272 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10511017113007540481(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 3} /* Input0 */, 
      {"input[1]", 272} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__10511017113007540481(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10511017113007540481(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10511017113007540481(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10511017113007540481(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__10511017113007540481(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10511017113007540481(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__10511017113007540481);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10511017113007540481);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__10511017113007540481);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__380821973141987362 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__380821973141987362(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__380821973141987362(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__380821973141987362(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__380821973141987362(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__380821973141987362(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__380821973141987362(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__380821973141987362(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__380821973141987362);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__380821973141987362);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__380821973141987362);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__10269640869839870970 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      262144 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10269640869839870970(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 262144} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__10269640869839870970(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10269640869839870970(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10269640869839870970(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10269640869839870970(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__10269640869839870970(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10269640869839870970(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__10269640869839870970);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10269640869839870970);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__10269640869839870970);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__117307621069660164 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__117307621069660164(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__117307621069660164(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__117307621069660164(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__117307621069660164(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__117307621069660164(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__117307621069660164(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__117307621069660164(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__117307621069660164);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__117307621069660164);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__117307621069660164);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_SOFTMAX_FWD__9374987095908127186 {

#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      8 /* Input1 */, \
      -1 /* Input2 */, \
      -1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_ADD_COUNTERS__9374987095908127186(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 8} /* Input1 */, 
      {"input[2]", -1} /* Input2 */, 
      {"input[3]", -1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnSoftmaxAlgorithm_t softmax_algorithm, cudnnSoftmaxMode_t softmax_mode>
static void LAYER_CUDNN_SOFTMAX_FWD_FLOAT16__9374987095908127186(benchmark::State& state) {
  LAYER_CUDNN_SOFTMAX_FWD_Impl<__half, softmax_algorithm, softmax_mode>(state);
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_ADD_COUNTERS__9374987095908127186(state);
}

template <cudnnSoftmaxAlgorithm_t softmax_algorithm, cudnnSoftmaxMode_t softmax_mode>
static void LAYER_CUDNN_SOFTMAX_FWD_FLOAT32__9374987095908127186(benchmark::State& state) {
  LAYER_CUDNN_SOFTMAX_FWD_Impl<float, softmax_algorithm, softmax_mode>(state);
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_ADD_COUNTERS__9374987095908127186(state);
}

template <cudnnSoftmaxAlgorithm_t softmax_algorithm, cudnnSoftmaxMode_t softmax_mode>
static void LAYER_CUDNN_SOFTMAX_FWD_TENSORCOREHALF__9374987095908127186(benchmark::State& state) {
  LAYER_CUDNN_SOFTMAX_FWD_Impl<__half, softmax_algorithm, softmax_mode>(state);
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_ADD_COUNTERS__9374987095908127186(state);
}

#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, SOFTMAX_MODE) \
 BENCHMARK_TEMPLATE(b, CUDNN_SOFTMAX_MODE_INSTANCE, SOFTMAX_MODE)->BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_SOFTMAX_MODE_CHANNEL, SOFTMAX_MODE)->BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD(b)                                                                                             \
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, CUDNN_SOFTMAX_MODE_INSTANCE);                                                                    \
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, CUDNN_SOFTMAX_MODE_CHANNEL)

BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD(LAYER_CUDNN_SOFTMAX_FWD_FLOAT16__9374987095908127186);
BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD(LAYER_CUDNN_SOFTMAX_FWD_FLOAT32__9374987095908127186);
BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD(LAYER_CUDNN_SOFTMAX_FWD_TENSORCOREHALF__9374987095908127186);

#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0
#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__14221905074641411226 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      134217728 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14221905074641411226(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 134217728} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14221905074641411226(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14221905074641411226(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14221905074641411226(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14221905074641411226(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14221905074641411226(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14221905074641411226(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14221905074641411226);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14221905074641411226);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14221905074641411226);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12514947743014560830 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      140737488355328 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12514947743014560830(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 140737488355328} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12514947743014560830(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12514947743014560830(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12514947743014560830(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12514947743014560830(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__12514947743014560830(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12514947743014560830(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12514947743014560830);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12514947743014560830);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__12514947743014560830);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9294104708343363324 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9294104708343363324(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9294104708343363324(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9294104708343363324(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9294104708343363324(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9294104708343363324(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9294104708343363324(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9294104708343363324(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9294104708343363324);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9294104708343363324);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9294104708343363324);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__15831484865317576288 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15831484865317576288(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15831484865317576288(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15831484865317576288(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15831484865317576288(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15831484865317576288(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15831484865317576288(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15831484865317576288(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15831484865317576288);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15831484865317576288);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15831484865317576288);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__2211045288256924966 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      208 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2211045288256924966(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 208} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2211045288256924966(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2211045288256924966(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2211045288256924966(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2211045288256924966(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2211045288256924966(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2211045288256924966(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2211045288256924966);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2211045288256924966);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2211045288256924966);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__707046102078741182 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      589824 /* Input0 */, \
      352 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__707046102078741182(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 589824} /* Input0 */, 
      {"input[1]", 352} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__707046102078741182(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__707046102078741182(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__707046102078741182(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__707046102078741182(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__707046102078741182(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__707046102078741182(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__707046102078741182);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__707046102078741182);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__707046102078741182);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__16427783136348921821 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      4294967296 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16427783136348921821(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4294967296} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16427783136348921821(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16427783136348921821(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16427783136348921821(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16427783136348921821(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16427783136348921821(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16427783136348921821(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16427783136348921821);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16427783136348921821);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16427783136348921821);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14827422721188260927 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      128 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14827422721188260927(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14827422721188260927(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14827422721188260927(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14827422721188260927(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14827422721188260927(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14827422721188260927(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14827422721188260927(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14827422721188260927);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14827422721188260927);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14827422721188260927);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__7670722921435890097 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2048 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7670722921435890097(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2048} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7670722921435890097(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7670722921435890097(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7670722921435890097(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7670722921435890097(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7670722921435890097(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7670722921435890097(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7670722921435890097);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7670722921435890097);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7670722921435890097);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12511636102878074445 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      3072 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12511636102878074445(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 3072} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12511636102878074445(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12511636102878074445(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12511636102878074445(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12511636102878074445(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12511636102878074445(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12511636102878074445(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12511636102878074445);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12511636102878074445);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12511636102878074445);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12981700766781039282 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      128 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12981700766781039282(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12981700766781039282(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12981700766781039282(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12981700766781039282(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12981700766781039282(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12981700766781039282(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12981700766781039282(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12981700766781039282);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12981700766781039282);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12981700766781039282);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__16368775526401070587 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      4398046511104 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16368775526401070587(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4398046511104} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__16368775526401070587(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16368775526401070587(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16368775526401070587(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16368775526401070587(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__16368775526401070587(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16368775526401070587(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__16368775526401070587);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16368775526401070587);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__16368775526401070587);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__1191357817019767294 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      32 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1191357817019767294(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1191357817019767294(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1191357817019767294(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1191357817019767294(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1191357817019767294(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1191357817019767294(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1191357817019767294(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1191357817019767294);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1191357817019767294);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1191357817019767294);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__15662723875986996484 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      70368744177664 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15662723875986996484(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 70368744177664} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15662723875986996484(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15662723875986996484(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15662723875986996484(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15662723875986996484(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15662723875986996484(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15662723875986996484(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15662723875986996484);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15662723875986996484);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15662723875986996484);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10806489361570273944 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      36864 /* Input0 */, \
      192 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10806489361570273944(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 36864} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10806489361570273944(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10806489361570273944(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10806489361570273944(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10806489361570273944(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10806489361570273944(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10806489361570273944(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10806489361570273944);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10806489361570273944);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10806489361570273944);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__6977744366191320211 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      70368744177664 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6977744366191320211(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 70368744177664} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6977744366191320211(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6977744366191320211(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6977744366191320211(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6977744366191320211(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6977744366191320211(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6977744366191320211(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6977744366191320211);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6977744366191320211);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6977744366191320211);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__17186924947317671053 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17186924947317671053(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17186924947317671053(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17186924947317671053(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17186924947317671053(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17186924947317671053(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17186924947317671053(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17186924947317671053(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17186924947317671053);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17186924947317671053);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17186924947317671053);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__1981657212003295377 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1981657212003295377(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1981657212003295377(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1981657212003295377(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1981657212003295377(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1981657212003295377(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1981657212003295377(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1981657212003295377(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1981657212003295377);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1981657212003295377);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1981657212003295377);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8945485709280879348 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      16 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8945485709280879348(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8945485709280879348(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8945485709280879348(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8945485709280879348(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8945485709280879348(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8945485709280879348(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8945485709280879348(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8945485709280879348);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8945485709280879348);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8945485709280879348);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__16895908438023172011 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      12288 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16895908438023172011(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 12288} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16895908438023172011(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16895908438023172011(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16895908438023172011(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16895908438023172011(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16895908438023172011(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16895908438023172011(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16895908438023172011);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16895908438023172011);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16895908438023172011);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__11315046670144043472 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11315046670144043472(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11315046670144043472(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11315046670144043472(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11315046670144043472(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11315046670144043472(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11315046670144043472(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11315046670144043472(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11315046670144043472);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11315046670144043472);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11315046670144043472);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__9331461204195673987 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      32768 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9331461204195673987(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 32768} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9331461204195673987(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9331461204195673987(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9331461204195673987(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9331461204195673987(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9331461204195673987(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9331461204195673987(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9331461204195673987);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9331461204195673987);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9331461204195673987);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__16897176049575078859 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      48 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      224 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16897176049575078859(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 48} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 224} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16897176049575078859(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16897176049575078859(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16897176049575078859(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16897176049575078859(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16897176049575078859(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16897176049575078859(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16897176049575078859);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16897176049575078859);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16897176049575078859);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8764098600772227725 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8764098600772227725(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8764098600772227725(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8764098600772227725(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8764098600772227725(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8764098600772227725(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8764098600772227725(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8764098600772227725(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8764098600772227725);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8764098600772227725);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8764098600772227725);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9513615094950899147 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      -1 /* Input2 */, \
      -1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9513615094950899147(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", -1} /* Input2 */, 
      {"input[3]", -1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9513615094950899147(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9513615094950899147(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9513615094950899147(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9513615094950899147(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9513615094950899147(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9513615094950899147(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9513615094950899147);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9513615094950899147);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9513615094950899147);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__14751996634496423134 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      8 /* Input0 */, \
      128 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14751996634496423134(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 8} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14751996634496423134(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14751996634496423134(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14751996634496423134(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14751996634496423134(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14751996634496423134(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14751996634496423134(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14751996634496423134);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14751996634496423134);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14751996634496423134);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6654936228453582629 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      48 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6654936228453582629(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6654936228453582629(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6654936228453582629(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6654936228453582629(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6654936228453582629(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6654936228453582629(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6654936228453582629(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6654936228453582629);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6654936228453582629);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6654936228453582629);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__3911178556542384816 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3911178556542384816(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3911178556542384816(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3911178556542384816(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3911178556542384816(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3911178556542384816(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3911178556542384816(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3911178556542384816(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3911178556542384816);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3911178556542384816);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3911178556542384816);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5109621497058672093 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      64 /* Input0 */, \
      192 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5109621497058672093(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5109621497058672093(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5109621497058672093(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5109621497058672093(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5109621497058672093(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5109621497058672093(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5109621497058672093(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5109621497058672093);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5109621497058672093);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5109621497058672093);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__5058123695462277644 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      134217728 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5058123695462277644(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 134217728} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__5058123695462277644(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5058123695462277644(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5058123695462277644(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5058123695462277644(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__5058123695462277644(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5058123695462277644(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__5058123695462277644);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5058123695462277644);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__5058123695462277644);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__4075490229609747071 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4075490229609747071(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4075490229609747071(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4075490229609747071(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4075490229609747071(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4075490229609747071(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4075490229609747071(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4075490229609747071(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4075490229609747071);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4075490229609747071);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4075490229609747071);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6357533878887008385 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6357533878887008385(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6357533878887008385(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6357533878887008385(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6357533878887008385(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6357533878887008385(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6357533878887008385(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6357533878887008385(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6357533878887008385);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6357533878887008385);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6357533878887008385);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__1452685790186346106 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      32 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1452685790186346106(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1452685790186346106(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1452685790186346106(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1452685790186346106(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1452685790186346106(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1452685790186346106(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1452685790186346106(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1452685790186346106);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1452685790186346106);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1452685790186346106);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__6978337357145210389 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      274877906944 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6978337357145210389(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 274877906944} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6978337357145210389(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6978337357145210389(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6978337357145210389(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6978337357145210389(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6978337357145210389(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6978337357145210389(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6978337357145210389);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6978337357145210389);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6978337357145210389);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__7402795222581005264 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7402795222581005264(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7402795222581005264(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7402795222581005264(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7402795222581005264(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7402795222581005264(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7402795222581005264(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7402795222581005264(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7402795222581005264);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7402795222581005264);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7402795222581005264);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__3485889975373327477 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      4398046511104 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3485889975373327477(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4398046511104} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3485889975373327477(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3485889975373327477(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3485889975373327477(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3485889975373327477(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3485889975373327477(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3485889975373327477(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3485889975373327477);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3485889975373327477);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3485889975373327477);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6203280463285081151 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      256 /* Input0 */, \
      160 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      24 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6203280463285081151(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 24} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6203280463285081151(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6203280463285081151(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6203280463285081151(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6203280463285081151(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6203280463285081151(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6203280463285081151(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6203280463285081151);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6203280463285081151);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6203280463285081151);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__11227925876569008060 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11227925876569008060(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11227925876569008060(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11227925876569008060(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11227925876569008060(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11227925876569008060(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11227925876569008060(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11227925876569008060(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11227925876569008060);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11227925876569008060);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11227925876569008060);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__17521376294079736284 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17521376294079736284(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__17521376294079736284(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17521376294079736284(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17521376294079736284(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17521376294079736284(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__17521376294079736284(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17521376294079736284(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__17521376294079736284);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17521376294079736284);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__17521376294079736284);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__3031590442738265767 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3031590442738265767(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__3031590442738265767(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3031590442738265767(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3031590442738265767(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3031590442738265767(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__3031590442738265767(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3031590442738265767(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__3031590442738265767);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3031590442738265767);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__3031590442738265767);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__17821212914715921804 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      3072 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17821212914715921804(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 3072} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__17821212914715921804(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17821212914715921804(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17821212914715921804(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17821212914715921804(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__17821212914715921804(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17821212914715921804(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__17821212914715921804);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17821212914715921804);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__17821212914715921804);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace CUDNN_POOLING_FWD__15670025746818570993 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__15670025746818570993(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__15670025746818570993(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__15670025746818570993(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__15670025746818570993(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__15670025746818570993(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__15670025746818570993(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__15670025746818570993(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__15670025746818570993);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__15670025746818570993);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__15670025746818570993);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__18153382396830186502 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      32 /* Input1 */, \
      208 /* Input2 */, \
      208 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18153382396830186502(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 208} /* Input2 */, 
      {"input[3]", 208} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18153382396830186502(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18153382396830186502(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18153382396830186502(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18153382396830186502(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__18153382396830186502(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18153382396830186502(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18153382396830186502);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18153382396830186502);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__18153382396830186502);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4212363531925157884 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4212363531925157884(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4212363531925157884(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4212363531925157884(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4212363531925157884(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4212363531925157884(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4212363531925157884(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4212363531925157884(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4212363531925157884);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4212363531925157884);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4212363531925157884);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__5424226332452684275 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5424226332452684275(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5424226332452684275(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5424226332452684275(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5424226332452684275(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5424226332452684275(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5424226332452684275(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5424226332452684275(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5424226332452684275);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5424226332452684275);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5424226332452684275);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__2895140415261156653 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      112 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__2895140415261156653(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__2895140415261156653(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__2895140415261156653(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__2895140415261156653(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__2895140415261156653(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__2895140415261156653(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__2895140415261156653(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__2895140415261156653);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__2895140415261156653);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__2895140415261156653);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_DROPOUT_FWD__881731682979928428 {

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS() \
  Args({{ \
      262144 /* Input0 */, \
      384 /* Input1 */, \
      0 /* Input2 */, \
      2 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__881731682979928428(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 262144} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 0} /* Input2 */, 
      {"input[3]", 2} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


static void LAYER_CUDNN_DROPOUT_FWD_FLOAT16__881731682979928428(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<__half, batchnorm_mode, is_training>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__881731682979928428(state);
}

static void LAYER_CUDNN_DROPOUT_FWD_FLOAT32__881731682979928428(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<float, batchnorm_mode, is_training>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__881731682979928428(state);
}

static void LAYER_CUDNN_DROPOUT_FWD_TENSORCOREHALF__881731682979928428(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<__half, batchnorm_mode, is_training>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__881731682979928428(state);
}




BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT16__881731682979928428);

BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT32__881731682979928428);

BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_TENSORCOREHALF__881731682979928428);

#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__368667936609515517 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      144 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__368667936609515517(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__368667936609515517(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__368667936609515517(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__368667936609515517(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__368667936609515517(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__368667936609515517(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__368667936609515517(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__368667936609515517);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__368667936609515517);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__368667936609515517);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9712774618178366083 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      48 /* Input0 */, \
      64 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9712774618178366083(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 48} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9712774618178366083(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9712774618178366083(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9712774618178366083(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9712774618178366083(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9712774618178366083(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9712774618178366083(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9712774618178366083);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9712774618178366083);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9712774618178366083);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__3214398444889927801 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      16 /* Input1 */, \
      113 /* Input2 */, \
      113 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3214398444889927801(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 113} /* Input2 */, 
      {"input[3]", 113} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3214398444889927801(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3214398444889927801(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3214398444889927801(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3214398444889927801(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3214398444889927801(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3214398444889927801(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3214398444889927801);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3214398444889927801);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3214398444889927801);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__18060385221010422550 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      320 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      1280 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18060385221010422550(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 1280} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__18060385221010422550(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18060385221010422550(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18060385221010422550(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18060385221010422550(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18060385221010422550(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18060385221010422550(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__18060385221010422550);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18060385221010422550);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18060385221010422550);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__1558131921650104485 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      128 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1558131921650104485(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1558131921650104485(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1558131921650104485(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1558131921650104485(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1558131921650104485(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1558131921650104485(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1558131921650104485(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1558131921650104485);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1558131921650104485);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1558131921650104485);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__5241646058785903445 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5241646058785903445(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__5241646058785903445(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5241646058785903445(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5241646058785903445(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5241646058785903445(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__5241646058785903445(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5241646058785903445(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__5241646058785903445);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5241646058785903445);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__5241646058785903445);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__4232275272974287367 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4232275272974287367(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4232275272974287367(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4232275272974287367(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4232275272974287367(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4232275272974287367(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4232275272974287367(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4232275272974287367(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4232275272974287367);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4232275272974287367);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4232275272974287367);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_DROPOUT_FWD__3100599422149109794 {

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS() \
  Args({{ \
      262144 /* Input0 */, \
      384 /* Input1 */, \
      0 /* Input2 */, \
      1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__3100599422149109794(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 262144} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 0} /* Input2 */, 
      {"input[3]", 1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


static void LAYER_CUDNN_DROPOUT_FWD_FLOAT16__3100599422149109794(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<__half, batchnorm_mode, is_training>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__3100599422149109794(state);
}

static void LAYER_CUDNN_DROPOUT_FWD_FLOAT32__3100599422149109794(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<float, batchnorm_mode, is_training>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__3100599422149109794(state);
}

static void LAYER_CUDNN_DROPOUT_FWD_TENSORCOREHALF__3100599422149109794(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<__half, batchnorm_mode, is_training>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__3100599422149109794(state);
}




BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT16__3100599422149109794);

BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT32__3100599422149109794);

BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_TENSORCOREHALF__3100599422149109794);

#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES
}

namespace LAYER_CUDNN_ACTIVATION_FWD__16344823026494131163 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      272 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16344823026494131163(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 272} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16344823026494131163(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16344823026494131163(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16344823026494131163(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16344823026494131163(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16344823026494131163(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16344823026494131163(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16344823026494131163);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16344823026494131163);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16344823026494131163);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__16468522438458613516 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__16468522438458613516(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__16468522438458613516(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__16468522438458613516(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__16468522438458613516(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__16468522438458613516(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__16468522438458613516(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__16468522438458613516(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__16468522438458613516);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__16468522438458613516);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__16468522438458613516);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4261241160422401507 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      16 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4261241160422401507(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4261241160422401507(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4261241160422401507(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4261241160422401507(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4261241160422401507(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4261241160422401507(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4261241160422401507(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4261241160422401507);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4261241160422401507);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4261241160422401507);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__220851034545348629 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2147483648 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__220851034545348629(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2147483648} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__220851034545348629(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__220851034545348629(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__220851034545348629(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__220851034545348629(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__220851034545348629(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__220851034545348629(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__220851034545348629);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__220851034545348629);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__220851034545348629);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14108614882704051060 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      16 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      48 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14108614882704051060(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14108614882704051060(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14108614882704051060(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14108614882704051060(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14108614882704051060(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14108614882704051060(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14108614882704051060(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14108614882704051060);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14108614882704051060);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14108614882704051060);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11672337044011801873 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      200 /* Input2 */, \
      200 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11672337044011801873(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 200} /* Input2 */, 
      {"input[3]", 200} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11672337044011801873(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11672337044011801873(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11672337044011801873(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11672337044011801873(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11672337044011801873(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11672337044011801873(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11672337044011801873);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11672337044011801873);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11672337044011801873);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__8065909996253296182 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      16 /* Input1 */, \
      416 /* Input2 */, \
      416 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8065909996253296182(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 416} /* Input2 */, 
      {"input[3]", 416} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8065909996253296182(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8065909996253296182(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8065909996253296182(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8065909996253296182(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8065909996253296182(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8065909996253296182(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8065909996253296182);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8065909996253296182);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8065909996253296182);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__3560554937100131978 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      64 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3560554937100131978(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3560554937100131978(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3560554937100131978(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3560554937100131978(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3560554937100131978(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3560554937100131978(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3560554937100131978(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3560554937100131978);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3560554937100131978);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3560554937100131978);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__10482553123957433361 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      268435456 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10482553123957433361(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 268435456} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10482553123957433361(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10482553123957433361(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10482553123957433361(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10482553123957433361(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10482553123957433361(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10482553123957433361(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10482553123957433361);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10482553123957433361);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10482553123957433361);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__1129252902536427229 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      112 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1129252902536427229(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1129252902536427229(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1129252902536427229(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1129252902536427229(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1129252902536427229(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1129252902536427229(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1129252902536427229(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1129252902536427229);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1129252902536427229);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1129252902536427229);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10453317762990867797 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      48 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10453317762990867797(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10453317762990867797(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10453317762990867797(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10453317762990867797(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10453317762990867797(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10453317762990867797(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10453317762990867797(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10453317762990867797);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10453317762990867797);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10453317762990867797);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__16622333731981073398 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      16 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16622333731981073398(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16622333731981073398(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16622333731981073398(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16622333731981073398(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16622333731981073398(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16622333731981073398(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16622333731981073398(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16622333731981073398);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16622333731981073398);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16622333731981073398);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11666913606190785950 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      48 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11666913606190785950(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 48} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11666913606190785950(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11666913606190785950(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11666913606190785950(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11666913606190785950(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11666913606190785950(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11666913606190785950(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11666913606190785950);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11666913606190785950);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11666913606190785950);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__12013329300830651733 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      320 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12013329300830651733(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12013329300830651733(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12013329300830651733(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12013329300830651733(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12013329300830651733(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12013329300830651733(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12013329300830651733(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12013329300830651733);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12013329300830651733);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12013329300830651733);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10863148915389818704 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10863148915389818704(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10863148915389818704(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10863148915389818704(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10863148915389818704(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10863148915389818704(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10863148915389818704(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10863148915389818704(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10863148915389818704);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10863148915389818704);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10863148915389818704);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4669455272073652364 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      136 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      136 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4669455272073652364(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 136} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 136} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4669455272073652364(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4669455272073652364(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4669455272073652364(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4669455272073652364(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4669455272073652364(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4669455272073652364(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4669455272073652364);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4669455272073652364);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4669455272073652364);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4282639036276696490 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      160 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      320 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4282639036276696490(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 320} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4282639036276696490(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4282639036276696490(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4282639036276696490(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4282639036276696490(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4282639036276696490(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4282639036276696490(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4282639036276696490);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4282639036276696490);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4282639036276696490);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__8699856414614579818 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      64 /* Input0 */, \
      224 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8699856414614579818(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8699856414614579818(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8699856414614579818(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8699856414614579818(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8699856414614579818(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8699856414614579818(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8699856414614579818(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8699856414614579818);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8699856414614579818);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8699856414614579818);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__5728726462302959878 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__5728726462302959878(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__5728726462302959878(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__5728726462302959878(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__5728726462302959878(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__5728726462302959878(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__5728726462302959878(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__5728726462302959878(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__5728726462302959878);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__5728726462302959878);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__5728726462302959878);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12317445851403409713 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      64 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12317445851403409713(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12317445851403409713(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12317445851403409713(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12317445851403409713(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12317445851403409713(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__12317445851403409713(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12317445851403409713(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12317445851403409713);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12317445851403409713);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__12317445851403409713);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__13147726863754437515 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      9007199254740992 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13147726863754437515(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 9007199254740992} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13147726863754437515(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13147726863754437515(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13147726863754437515(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13147726863754437515(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13147726863754437515(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13147726863754437515(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13147726863754437515);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13147726863754437515);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13147726863754437515);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__8821013163963607618 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      160 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8821013163963607618(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8821013163963607618(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8821013163963607618(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8821013163963607618(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8821013163963607618(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8821013163963607618(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8821013163963607618(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8821013163963607618);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8821013163963607618);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8821013163963607618);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__2433507135732761283 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      4096 /* Input1 */, \
      -1 /* Input2 */, \
      -1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2433507135732761283(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 4096} /* Input1 */, 
      {"input[2]", -1} /* Input2 */, 
      {"input[3]", -1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2433507135732761283(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2433507135732761283(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2433507135732761283(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2433507135732761283(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2433507135732761283(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2433507135732761283(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2433507135732761283);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2433507135732761283);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2433507135732761283);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__7041433072895440205 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      272 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7041433072895440205(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 272} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__7041433072895440205(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7041433072895440205(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7041433072895440205(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7041433072895440205(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__7041433072895440205(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7041433072895440205(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__7041433072895440205);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7041433072895440205);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__7041433072895440205);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace CUDNN_POOLING_FWD__1527499785585386247 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      16384 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__1527499785585386247(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__1527499785585386247(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__1527499785585386247(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__1527499785585386247(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__1527499785585386247(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__1527499785585386247(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__1527499785585386247(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__1527499785585386247);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__1527499785585386247);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__1527499785585386247);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__1665883157319617512 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1665883157319617512(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1665883157319617512(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1665883157319617512(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1665883157319617512(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1665883157319617512(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1665883157319617512(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1665883157319617512(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1665883157319617512);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1665883157319617512);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1665883157319617512);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10613154376983862917 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      4503599627370496 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10613154376983862917(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4503599627370496} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10613154376983862917(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10613154376983862917(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10613154376983862917(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10613154376983862917(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10613154376983862917(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10613154376983862917(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10613154376983862917);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10613154376983862917);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10613154376983862917);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8537908043433281137 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      64 /* Input0 */, \
      24 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      64 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8537908043433281137(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8537908043433281137(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8537908043433281137(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8537908043433281137(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8537908043433281137(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8537908043433281137(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8537908043433281137(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8537908043433281137);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8537908043433281137);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8537908043433281137);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12610752668790910936 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      36864 /* Input0 */, \
      192 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12610752668790910936(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 36864} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12610752668790910936(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12610752668790910936(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12610752668790910936(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12610752668790910936(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12610752668790910936(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12610752668790910936(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12610752668790910936);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12610752668790910936);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12610752668790910936);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_DROPOUT_FWD__12369201523487528953 {

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      1024 /* Input1 */, \
      -1 /* Input2 */, \
      -1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__12369201523487528953(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", -1} /* Input2 */, 
      {"input[3]", -1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


static void LAYER_CUDNN_DROPOUT_FWD_FLOAT16__12369201523487528953(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<__half, batchnorm_mode, is_training>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__12369201523487528953(state);
}

static void LAYER_CUDNN_DROPOUT_FWD_FLOAT32__12369201523487528953(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<float, batchnorm_mode, is_training>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__12369201523487528953(state);
}

static void LAYER_CUDNN_DROPOUT_FWD_TENSORCOREHALF__12369201523487528953(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<__half, batchnorm_mode, is_training>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__12369201523487528953(state);
}




BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT16__12369201523487528953);

BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT32__12369201523487528953);

BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_TENSORCOREHALF__12369201523487528953);

#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__8491261726408346078 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      512 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8491261726408346078(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__8491261726408346078(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8491261726408346078(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8491261726408346078(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8491261726408346078(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__8491261726408346078(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8491261726408346078(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__8491261726408346078);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8491261726408346078);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__8491261726408346078);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__302648067509719457 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__302648067509719457(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__302648067509719457(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__302648067509719457(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__302648067509719457(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__302648067509719457(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__302648067509719457(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__302648067509719457(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__302648067509719457);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__302648067509719457);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__302648067509719457);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9392242914189482003 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      65536 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9392242914189482003(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9392242914189482003(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9392242914189482003(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9392242914189482003(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9392242914189482003(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9392242914189482003(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9392242914189482003(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9392242914189482003);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9392242914189482003);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9392242914189482003);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12220585168283306657 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1024 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12220585168283306657(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12220585168283306657(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12220585168283306657(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12220585168283306657(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12220585168283306657(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12220585168283306657(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12220585168283306657(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12220585168283306657);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12220585168283306657);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12220585168283306657);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__923917622765620168 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      128 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__923917622765620168(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 128} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__923917622765620168(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__923917622765620168(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__923917622765620168(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__923917622765620168(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__923917622765620168(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__923917622765620168(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__923917622765620168);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__923917622765620168);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__923917622765620168);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__3603540504620470620 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      136 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      136 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3603540504620470620(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 136} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 136} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3603540504620470620(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3603540504620470620(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3603540504620470620(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3603540504620470620(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3603540504620470620(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3603540504620470620(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3603540504620470620);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3603540504620470620);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3603540504620470620);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__16590222450796321848 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16590222450796321848(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16590222450796321848(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16590222450796321848(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16590222450796321848(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16590222450796321848(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16590222450796321848(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16590222450796321848(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16590222450796321848);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16590222450796321848);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16590222450796321848);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__9970183001732024226 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      2048 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9970183001732024226(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 2048} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9970183001732024226(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9970183001732024226(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9970183001732024226(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9970183001732024226(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9970183001732024226(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9970183001732024226(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9970183001732024226);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9970183001732024226);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9970183001732024226);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__300414407987713452 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      96 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__300414407987713452(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__300414407987713452(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__300414407987713452(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__300414407987713452(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__300414407987713452(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__300414407987713452(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__300414407987713452(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__300414407987713452);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__300414407987713452);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__300414407987713452);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_DROPOUT_FWD__1165970736075942379 {

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      4 /* Input2 */, \
      4 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__1165970736075942379(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 4} /* Input2 */, 
      {"input[3]", 4} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


static void LAYER_CUDNN_DROPOUT_FWD_FLOAT16__1165970736075942379(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<__half, batchnorm_mode, is_training>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__1165970736075942379(state);
}

static void LAYER_CUDNN_DROPOUT_FWD_FLOAT32__1165970736075942379(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<float, batchnorm_mode, is_training>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__1165970736075942379(state);
}

static void LAYER_CUDNN_DROPOUT_FWD_TENSORCOREHALF__1165970736075942379(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<__half, batchnorm_mode, is_training>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__1165970736075942379(state);
}




BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT16__1165970736075942379);

BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT32__1165970736075942379);

BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_TENSORCOREHALF__1165970736075942379);

#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES
}

namespace LAYER_CUDNN_ACTIVATION_FWD__16847950372286172075 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      96 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16847950372286172075(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16847950372286172075(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16847950372286172075(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16847950372286172075(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16847950372286172075(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16847950372286172075(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16847950372286172075(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16847950372286172075);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16847950372286172075);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16847950372286172075);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__10478904113843469615 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      16 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10478904113843469615(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10478904113843469615(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10478904113843469615(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10478904113843469615(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10478904113843469615(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10478904113843469615(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10478904113843469615(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10478904113843469615);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10478904113843469615);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10478904113843469615);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9545520055739833620 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      110 /* Input2 */, \
      110 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9545520055739833620(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 110} /* Input2 */, 
      {"input[3]", 110} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9545520055739833620(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9545520055739833620(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9545520055739833620(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9545520055739833620(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9545520055739833620(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9545520055739833620(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9545520055739833620);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9545520055739833620);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9545520055739833620);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6084668616178784795 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      549755813888 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6084668616178784795(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 549755813888} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6084668616178784795(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6084668616178784795(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6084668616178784795(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6084668616178784795(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6084668616178784795(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6084668616178784795(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6084668616178784795);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6084668616178784795);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6084668616178784795);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11783210266765155345 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      16384 /* Input0 */, \
      32 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      128 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11783210266765155345(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11783210266765155345(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11783210266765155345(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11783210266765155345(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11783210266765155345(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11783210266765155345(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11783210266765155345(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11783210266765155345);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11783210266765155345);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11783210266765155345);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__15166056830211266751 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15166056830211266751(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15166056830211266751(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15166056830211266751(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15166056830211266751(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15166056830211266751(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15166056830211266751(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15166056830211266751(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15166056830211266751);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15166056830211266751);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15166056830211266751);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6823047445792015984 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      113 /* Input2 */, \
      113 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6823047445792015984(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 113} /* Input2 */, 
      {"input[3]", 113} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6823047445792015984(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6823047445792015984(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6823047445792015984(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6823047445792015984(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6823047445792015984(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6823047445792015984(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6823047445792015984);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6823047445792015984);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6823047445792015984);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__14105953770810177171 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1024 /* Input0 */, \
      144 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14105953770810177171(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14105953770810177171(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14105953770810177171(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14105953770810177171(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14105953770810177171(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14105953770810177171(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14105953770810177171(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14105953770810177171);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14105953770810177171);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14105953770810177171);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6424322653422800172 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1024 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      144 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6424322653422800172(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 144} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6424322653422800172(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6424322653422800172(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6424322653422800172(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6424322653422800172(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6424322653422800172(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6424322653422800172(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6424322653422800172);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6424322653422800172);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6424322653422800172);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__18383045495614074351 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      256 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      1000 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18383045495614074351(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 1000} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__18383045495614074351(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18383045495614074351(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18383045495614074351(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18383045495614074351(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18383045495614074351(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18383045495614074351(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__18383045495614074351);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18383045495614074351);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18383045495614074351);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__15440170952454545384 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      256 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15440170952454545384(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15440170952454545384(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15440170952454545384(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15440170952454545384(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15440170952454545384(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15440170952454545384(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15440170952454545384(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15440170952454545384);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15440170952454545384);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15440170952454545384);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__14686901057994550723 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14686901057994550723(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14686901057994550723(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14686901057994550723(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14686901057994550723(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14686901057994550723(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14686901057994550723(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14686901057994550723(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14686901057994550723);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14686901057994550723);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14686901057994550723);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5161873031494948124 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      3 /* Input0 */, \
      136 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      272 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5161873031494948124(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 3} /* Input0 */, 
      {"input[1]", 136} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 272} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5161873031494948124(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5161873031494948124(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5161873031494948124(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5161873031494948124(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5161873031494948124(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5161873031494948124(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5161873031494948124);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5161873031494948124);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5161873031494948124);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__16324042497778750091 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      128 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16324042497778750091(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16324042497778750091(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16324042497778750091(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16324042497778750091(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16324042497778750091(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16324042497778750091(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16324042497778750091(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16324042497778750091);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16324042497778750091);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16324042497778750091);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8237718998752586338 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      960 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8237718998752586338(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 960} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8237718998752586338(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8237718998752586338(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8237718998752586338(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8237718998752586338(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8237718998752586338(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8237718998752586338(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8237718998752586338);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8237718998752586338);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8237718998752586338);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__11942319074223807247 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      136 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11942319074223807247(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 136} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11942319074223807247(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11942319074223807247(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11942319074223807247(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11942319074223807247(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11942319074223807247(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11942319074223807247(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11942319074223807247);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11942319074223807247);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11942319074223807247);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__14311503288406514117 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      144 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14311503288406514117(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14311503288406514117(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14311503288406514117(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14311503288406514117(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14311503288406514117(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14311503288406514117(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14311503288406514117(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14311503288406514117);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14311503288406514117);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14311503288406514117);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__9029172640658183968 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      147456 /* Input0 */, \
      192 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9029172640658183968(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 147456} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__9029172640658183968(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9029172640658183968(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9029172640658183968(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9029172640658183968(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__9029172640658183968(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9029172640658183968(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__9029172640658183968);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9029172640658183968);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__9029172640658183968);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__4822696412851480337 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      401 /* Input2 */, \
      401 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4822696412851480337(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 401} /* Input2 */, 
      {"input[3]", 401} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__4822696412851480337(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4822696412851480337(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4822696412851480337(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4822696412851480337(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__4822696412851480337(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4822696412851480337(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__4822696412851480337);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4822696412851480337);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__4822696412851480337);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__13511340366597993504 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      8 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13511340366597993504(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 8} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13511340366597993504(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13511340366597993504(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13511340366597993504(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13511340366597993504(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13511340366597993504(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13511340366597993504(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13511340366597993504);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13511340366597993504);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13511340366597993504);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__16204072764969862260 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      147456 /* Input0 */, \
      352 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      352 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16204072764969862260(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 147456} /* Input0 */, 
      {"input[1]", 352} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 352} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16204072764969862260(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16204072764969862260(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16204072764969862260(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16204072764969862260(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16204072764969862260(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16204072764969862260(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16204072764969862260);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16204072764969862260);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16204072764969862260);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10624008691834446865 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      24 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10624008691834446865(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 24} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10624008691834446865(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10624008691834446865(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10624008691834446865(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10624008691834446865(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10624008691834446865(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10624008691834446865(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10624008691834446865);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10624008691834446865);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10624008691834446865);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__13238493518282963022 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      262144 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13238493518282963022(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 262144} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13238493518282963022(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13238493518282963022(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13238493518282963022(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13238493518282963022(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13238493518282963022(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13238493518282963022(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13238493518282963022);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13238493518282963022);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13238493518282963022);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__18404255499411487369 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      256 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18404255499411487369(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18404255499411487369(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18404255499411487369(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18404255499411487369(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18404255499411487369(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__18404255499411487369(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18404255499411487369(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18404255499411487369);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18404255499411487369);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__18404255499411487369);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__12367056692684065991 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      256 /* Input0 */, \
      160 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12367056692684065991(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12367056692684065991(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12367056692684065991(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12367056692684065991(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12367056692684065991(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12367056692684065991(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12367056692684065991(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12367056692684065991);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12367056692684065991);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12367056692684065991);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__8485922749039639386 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      544 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8485922749039639386(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 544} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8485922749039639386(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8485922749039639386(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8485922749039639386(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8485922749039639386(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8485922749039639386(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8485922749039639386(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8485922749039639386);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8485922749039639386);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8485922749039639386);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__9972027427649022943 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9972027427649022943(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9972027427649022943(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9972027427649022943(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9972027427649022943(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9972027427649022943(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9972027427649022943(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9972027427649022943(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9972027427649022943);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9972027427649022943);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9972027427649022943);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10442178224088894792 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10442178224088894792(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10442178224088894792(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10442178224088894792(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10442178224088894792(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10442178224088894792(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10442178224088894792(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10442178224088894792(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10442178224088894792);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10442178224088894792);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10442178224088894792);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__7629557526053672280 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      272 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      272 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7629557526053672280(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 272} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 272} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7629557526053672280(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7629557526053672280(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7629557526053672280(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7629557526053672280(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7629557526053672280(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7629557526053672280(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7629557526053672280);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7629557526053672280);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7629557526053672280);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__3498035068437298711 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3498035068437298711(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3498035068437298711(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3498035068437298711(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3498035068437298711(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3498035068437298711(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3498035068437298711(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3498035068437298711(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3498035068437298711);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3498035068437298711);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3498035068437298711);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__7674032825540927811 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      2048 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7674032825540927811(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__7674032825540927811(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7674032825540927811(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7674032825540927811(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7674032825540927811(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__7674032825540927811(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7674032825540927811(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__7674032825540927811);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7674032825540927811);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__7674032825540927811);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__11079644735875146738 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      256 /* Input0 */, \
      64 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11079644735875146738(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11079644735875146738(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11079644735875146738(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11079644735875146738(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11079644735875146738(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11079644735875146738(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11079644735875146738(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11079644735875146738);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11079644735875146738);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11079644735875146738);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__10697693457900967348 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10697693457900967348(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10697693457900967348(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10697693457900967348(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10697693457900967348(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10697693457900967348(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10697693457900967348(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10697693457900967348(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10697693457900967348);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10697693457900967348);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10697693457900967348);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__3941605766650770817 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3941605766650770817(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__3941605766650770817(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3941605766650770817(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3941605766650770817(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3941605766650770817(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__3941605766650770817(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3941605766650770817(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__3941605766650770817);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3941605766650770817);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__3941605766650770817);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9990010145270801448 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      589824 /* Input0 */, \
      352 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9990010145270801448(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 589824} /* Input0 */, 
      {"input[1]", 352} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9990010145270801448(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9990010145270801448(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9990010145270801448(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9990010145270801448(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9990010145270801448(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9990010145270801448(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9990010145270801448);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9990010145270801448);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9990010145270801448);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__3167876644871931609 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3167876644871931609(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3167876644871931609(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3167876644871931609(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3167876644871931609(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3167876644871931609(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3167876644871931609(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3167876644871931609(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3167876644871931609);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3167876644871931609);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3167876644871931609);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__13943675745650891143 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      401 /* Input2 */, \
      401 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13943675745650891143(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 401} /* Input2 */, 
      {"input[3]", 401} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13943675745650891143(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13943675745650891143(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13943675745650891143(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13943675745650891143(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13943675745650891143(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13943675745650891143(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13943675745650891143);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13943675745650891143);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13943675745650891143);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5188812311678468242 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      64 /* Input0 */, \
      192 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5188812311678468242(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5188812311678468242(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5188812311678468242(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5188812311678468242(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5188812311678468242(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5188812311678468242(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5188812311678468242(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5188812311678468242);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5188812311678468242);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5188812311678468242);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__115247575927930839 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      320 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__115247575927930839(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__115247575927930839(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__115247575927930839(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__115247575927930839(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__115247575927930839(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__115247575927930839(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__115247575927930839(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__115247575927930839);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__115247575927930839);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__115247575927930839);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__11536629186350160353 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1024 /* Input0 */, \
      64 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11536629186350160353(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11536629186350160353(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11536629186350160353(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11536629186350160353(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11536629186350160353(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11536629186350160353(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11536629186350160353(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11536629186350160353);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11536629186350160353);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11536629186350160353);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__1571876359932225957 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      64 /* Input0 */, \
      192 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1571876359932225957(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1571876359932225957(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1571876359932225957(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1571876359932225957(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1571876359932225957(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1571876359932225957(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1571876359932225957(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1571876359932225957);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1571876359932225957);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1571876359932225957);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__5571049003840015588 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2251799813685248 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5571049003840015588(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2251799813685248} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__5571049003840015588(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5571049003840015588(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5571049003840015588(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5571049003840015588(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__5571049003840015588(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5571049003840015588(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__5571049003840015588);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5571049003840015588);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__5571049003840015588);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__5038960630669201576 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      544 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5038960630669201576(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 544} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__5038960630669201576(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5038960630669201576(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5038960630669201576(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5038960630669201576(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__5038960630669201576(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5038960630669201576(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__5038960630669201576);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5038960630669201576);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__5038960630669201576);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace CUDNN_POOLING_FWD__677862920749638700 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      112 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__677862920749638700(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__677862920749638700(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__677862920749638700(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__677862920749638700(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__677862920749638700(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__677862920749638700(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__677862920749638700(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__677862920749638700);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__677862920749638700);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__677862920749638700);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6233775122211103303 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      2048 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      304 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      12 /* PadHeight */, \
      12 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      12 /* DilationWidth */, \
      12 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6233775122211103303(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 304} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 12} /* PadHeight */, 
      {"pad_width", 12} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 12} /* DilationWidth */, 
      {"dilation_width", 12} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6233775122211103303(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6233775122211103303(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6233775122211103303(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6233775122211103303(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6233775122211103303(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6233775122211103303(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6233775122211103303);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6233775122211103303);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6233775122211103303);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4184735121568627255 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      32 /* Input0 */, \
      48 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4184735121568627255(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 32} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4184735121568627255(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4184735121568627255(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4184735121568627255(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4184735121568627255(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4184735121568627255(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4184735121568627255(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4184735121568627255);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4184735121568627255);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4184735121568627255);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8876036559124331266 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      24 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      144 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8876036559124331266(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 144} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8876036559124331266(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8876036559124331266(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8876036559124331266(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8876036559124331266(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8876036559124331266(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8876036559124331266(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8876036559124331266);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8876036559124331266);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8876036559124331266);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2452248540666004558 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      32 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      96 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2452248540666004558(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2452248540666004558(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2452248540666004558(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2452248540666004558(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2452248540666004558(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2452248540666004558(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2452248540666004558(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2452248540666004558);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2452248540666004558);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2452248540666004558);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__16321461558798636768 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      544 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      544 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16321461558798636768(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 544} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 544} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16321461558798636768(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16321461558798636768(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16321461558798636768(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16321461558798636768(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16321461558798636768(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16321461558798636768(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16321461558798636768);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16321461558798636768);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16321461558798636768);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__5164926936956442596 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      65536 /* Input0 */, \
      192 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5164926936956442596(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5164926936956442596(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5164926936956442596(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5164926936956442596(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5164926936956442596(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5164926936956442596(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5164926936956442596(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5164926936956442596);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5164926936956442596);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5164926936956442596);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9864868050080708651 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9864868050080708651(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9864868050080708651(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9864868050080708651(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9864868050080708651(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9864868050080708651(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9864868050080708651(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9864868050080708651(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9864868050080708651);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9864868050080708651);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9864868050080708651);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14606746579487915174 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      34359738368 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14606746579487915174(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 34359738368} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14606746579487915174(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14606746579487915174(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14606746579487915174(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14606746579487915174(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14606746579487915174(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14606746579487915174(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14606746579487915174);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14606746579487915174);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14606746579487915174);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__11341288172227452275 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11341288172227452275(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__11341288172227452275(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11341288172227452275(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11341288172227452275(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11341288172227452275(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__11341288172227452275(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11341288172227452275(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__11341288172227452275);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11341288172227452275);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__11341288172227452275);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__10007565838180282878 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      262144 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10007565838180282878(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 262144} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__10007565838180282878(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10007565838180282878(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10007565838180282878(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10007565838180282878(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__10007565838180282878(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10007565838180282878(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__10007565838180282878);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10007565838180282878);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__10007565838180282878);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__12883260294921856530 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      16384 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12883260294921856530(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12883260294921856530(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12883260294921856530(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12883260294921856530(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12883260294921856530(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12883260294921856530(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12883260294921856530(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12883260294921856530);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12883260294921856530);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12883260294921856530);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__6591752544203407946 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      128 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6591752544203407946(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6591752544203407946(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6591752544203407946(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6591752544203407946(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6591752544203407946(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6591752544203407946(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6591752544203407946(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6591752544203407946);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6591752544203407946);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6591752544203407946);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10383502438697752407 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10383502438697752407(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10383502438697752407(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10383502438697752407(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10383502438697752407(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10383502438697752407(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10383502438697752407(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10383502438697752407(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10383502438697752407);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10383502438697752407);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10383502438697752407);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__7266417581424125919 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      2048 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      304 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      6 /* PadHeight */, \
      6 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      6 /* DilationWidth */, \
      6 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7266417581424125919(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 304} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 6} /* PadHeight */, 
      {"pad_width", 6} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 6} /* DilationWidth */, 
      {"dilation_width", 6} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7266417581424125919(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7266417581424125919(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7266417581424125919(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7266417581424125919(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7266417581424125919(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7266417581424125919(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7266417581424125919);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7266417581424125919);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7266417581424125919);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__17459305545519455646 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      33554432 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17459305545519455646(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 33554432} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17459305545519455646(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17459305545519455646(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17459305545519455646(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17459305545519455646(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17459305545519455646(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17459305545519455646(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17459305545519455646);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17459305545519455646);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17459305545519455646);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8468696306950643777 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8468696306950643777(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8468696306950643777(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8468696306950643777(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8468696306950643777(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8468696306950643777(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8468696306950643777(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8468696306950643777(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8468696306950643777);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8468696306950643777);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8468696306950643777);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11975635359135607606 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      112 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11975635359135607606(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11975635359135607606(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11975635359135607606(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11975635359135607606(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11975635359135607606(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11975635359135607606(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11975635359135607606(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11975635359135607606);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11975635359135607606);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11975635359135607606);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__14214453378224016102 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      16384 /* Input0 */, \
      128 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14214453378224016102(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14214453378224016102(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14214453378224016102(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14214453378224016102(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14214453378224016102(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14214453378224016102(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14214453378224016102(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14214453378224016102);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14214453378224016102);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14214453378224016102);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6998582793065432067 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      768 /* Input0 */, \
      192 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6998582793065432067(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 768} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6998582793065432067(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6998582793065432067(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6998582793065432067(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6998582793065432067(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6998582793065432067(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6998582793065432067(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6998582793065432067);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6998582793065432067);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6998582793065432067);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__14502608992983663052 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      65536 /* Input0 */, \
      384 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14502608992983663052(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14502608992983663052(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14502608992983663052(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14502608992983663052(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14502608992983663052(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14502608992983663052(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14502608992983663052(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14502608992983663052);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14502608992983663052);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14502608992983663052);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4736387825172028229 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      32 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      128 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4736387825172028229(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4736387825172028229(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4736387825172028229(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4736387825172028229(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4736387825172028229(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4736387825172028229(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4736387825172028229(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4736387825172028229);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4736387825172028229);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4736387825172028229);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__10601792516364502728 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      96 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10601792516364502728(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10601792516364502728(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10601792516364502728(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10601792516364502728(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10601792516364502728(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10601792516364502728(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10601792516364502728(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10601792516364502728);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10601792516364502728);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10601792516364502728);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6162010582878397466 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      65536 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      48 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6162010582878397466(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6162010582878397466(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6162010582878397466(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6162010582878397466(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6162010582878397466(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6162010582878397466(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6162010582878397466(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6162010582878397466);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6162010582878397466);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6162010582878397466);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__18048485655459953979 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      57 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18048485655459953979(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__18048485655459953979(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18048485655459953979(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18048485655459953979(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18048485655459953979(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18048485655459953979(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18048485655459953979(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__18048485655459953979);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18048485655459953979);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18048485655459953979);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__14304318042968383279 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      16384 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__14304318042968383279(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__14304318042968383279(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__14304318042968383279(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__14304318042968383279(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__14304318042968383279(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__14304318042968383279(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__14304318042968383279(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__14304318042968383279);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__14304318042968383279);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__14304318042968383279);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__8457975817448103864 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      192 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8457975817448103864(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 192} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__8457975817448103864(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8457975817448103864(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8457975817448103864(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8457975817448103864(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__8457975817448103864(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8457975817448103864(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__8457975817448103864);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8457975817448103864);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__8457975817448103864);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__10665284050567012366 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      549755813888 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10665284050567012366(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 549755813888} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10665284050567012366(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10665284050567012366(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10665284050567012366(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10665284050567012366(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10665284050567012366(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10665284050567012366(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10665284050567012366);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10665284050567012366);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10665284050567012366);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__17483429624650650453 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      32 /* Input0 */, \
      48 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17483429624650650453(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 32} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17483429624650650453(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17483429624650650453(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17483429624650650453(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17483429624650650453(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17483429624650650453(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17483429624650650453(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17483429624650650453);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17483429624650650453);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17483429624650650453);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__13946138248092943722 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      4398046511104 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13946138248092943722(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4398046511104} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13946138248092943722(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13946138248092943722(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13946138248092943722(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13946138248092943722(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13946138248092943722(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13946138248092943722(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13946138248092943722);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13946138248092943722);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13946138248092943722);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__1926900279239758388 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      36864 /* Input0 */, \
      320 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1926900279239758388(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 36864} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__1926900279239758388(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1926900279239758388(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1926900279239758388(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1926900279239758388(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__1926900279239758388(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1926900279239758388(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__1926900279239758388);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1926900279239758388);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__1926900279239758388);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__13529956688111556904 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      48 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13529956688111556904(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 48} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13529956688111556904(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13529956688111556904(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13529956688111556904(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13529956688111556904(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13529956688111556904(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13529956688111556904(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13529956688111556904);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13529956688111556904);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13529956688111556904);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__18212888447807170508 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18212888447807170508(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18212888447807170508(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18212888447807170508(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18212888447807170508(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18212888447807170508(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__18212888447807170508(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18212888447807170508(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18212888447807170508);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18212888447807170508);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__18212888447807170508);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10804823767184537866 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10804823767184537866(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10804823767184537866(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10804823767184537866(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10804823767184537866(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10804823767184537866(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10804823767184537866(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10804823767184537866(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10804823767184537866);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10804823767184537866);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10804823767184537866);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__5757307600434377081 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      8589934592 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5757307600434377081(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 8589934592} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5757307600434377081(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5757307600434377081(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5757307600434377081(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5757307600434377081(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5757307600434377081(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5757307600434377081(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5757307600434377081);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5757307600434377081);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5757307600434377081);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__11504672497642750501 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__11504672497642750501(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__11504672497642750501(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__11504672497642750501(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__11504672497642750501(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__11504672497642750501(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__11504672497642750501(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__11504672497642750501(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__11504672497642750501);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__11504672497642750501);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__11504672497642750501);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4746089987387864624 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      64 /* FilterCount */, \
      7 /* FilterHeight */, \
      7 /* FilterWidth */, \
      3 /* PadHeight */, \
      3 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4746089987387864624(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 7} /* FilterHeight */, 
      {"filter_width", 7} /* FilterWidth */, 
      {"pad_height", 3} /* PadHeight */, 
      {"pad_width", 3} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4746089987387864624(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4746089987387864624(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4746089987387864624(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4746089987387864624(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4746089987387864624(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4746089987387864624(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4746089987387864624);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4746089987387864624);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4746089987387864624);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__16149124631251656867 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2199023255552 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16149124631251656867(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2199023255552} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16149124631251656867(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16149124631251656867(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16149124631251656867(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16149124631251656867(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16149124631251656867(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16149124631251656867(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16149124631251656867);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16149124631251656867);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16149124631251656867);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__5456635239261056825 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5456635239261056825(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5456635239261056825(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5456635239261056825(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5456635239261056825(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5456635239261056825(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5456635239261056825(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5456635239261056825(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5456635239261056825);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5456635239261056825);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5456635239261056825);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5960954592692131696 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      272 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      272 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5960954592692131696(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 272} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 272} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5960954592692131696(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5960954592692131696(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5960954592692131696(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5960954592692131696(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5960954592692131696(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5960954592692131696(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5960954592692131696);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5960954592692131696);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5960954592692131696);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6015097134521740399 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      137438953472 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6015097134521740399(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 137438953472} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6015097134521740399(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6015097134521740399(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6015097134521740399(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6015097134521740399(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6015097134521740399(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6015097134521740399(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6015097134521740399);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6015097134521740399);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6015097134521740399);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__344995516720306438 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__344995516720306438(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__344995516720306438(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__344995516720306438(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__344995516720306438(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__344995516720306438(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__344995516720306438(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__344995516720306438(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__344995516720306438);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__344995516720306438);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__344995516720306438);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12048274526743475563 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      67108864 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12048274526743475563(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 67108864} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12048274526743475563(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12048274526743475563(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12048274526743475563(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12048274526743475563(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12048274526743475563(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12048274526743475563(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12048274526743475563);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12048274526743475563);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12048274526743475563);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12942035327743879114 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      36864 /* Input0 */, \
      192 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12942035327743879114(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 36864} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12942035327743879114(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12942035327743879114(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12942035327743879114(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12942035327743879114(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12942035327743879114(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12942035327743879114(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12942035327743879114);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12942035327743879114);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12942035327743879114);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__1296425422143036732 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      24 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      112 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1296425422143036732(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 112} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1296425422143036732(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1296425422143036732(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1296425422143036732(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1296425422143036732(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1296425422143036732(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1296425422143036732(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1296425422143036732);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1296425422143036732);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1296425422143036732);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__5724420566094041703 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      4398046511104 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__5724420566094041703(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4398046511104} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__5724420566094041703(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__5724420566094041703(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__5724420566094041703(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__5724420566094041703(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__5724420566094041703(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__5724420566094041703(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__5724420566094041703);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__5724420566094041703);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__5724420566094041703);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace CUDNN_POOLING_FWD__1893573820371132658 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      147456 /* Input0 */, \
      352 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__1893573820371132658(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 147456} /* Input0 */, 
      {"input[1]", 352} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__1893573820371132658(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__1893573820371132658(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__1893573820371132658(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__1893573820371132658(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__1893573820371132658(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__1893573820371132658(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__1893573820371132658);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__1893573820371132658);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__1893573820371132658);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__3803316804863399280 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3803316804863399280(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3803316804863399280(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3803316804863399280(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3803316804863399280(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3803316804863399280(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3803316804863399280(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3803316804863399280(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3803316804863399280);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3803316804863399280);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3803316804863399280);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__531456683578422793 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      192 /* Input0 */, \
      224 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__531456683578422793(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 192} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__531456683578422793(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__531456683578422793(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__531456683578422793(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__531456683578422793(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__531456683578422793(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__531456683578422793(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__531456683578422793);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__531456683578422793);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__531456683578422793);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__8273914187275379426 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      401 /* Input2 */, \
      401 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8273914187275379426(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 401} /* Input2 */, 
      {"input[3]", 401} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__8273914187275379426(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8273914187275379426(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8273914187275379426(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8273914187275379426(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__8273914187275379426(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8273914187275379426(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__8273914187275379426);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8273914187275379426);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__8273914187275379426);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__17029610328490877693 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17029610328490877693(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17029610328490877693(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17029610328490877693(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17029610328490877693(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17029610328490877693(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17029610328490877693(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17029610328490877693(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17029610328490877693);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17029610328490877693);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17029610328490877693);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__2586395468412767186 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__2586395468412767186(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__2586395468412767186(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__2586395468412767186(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__2586395468412767186(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__2586395468412767186(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__2586395468412767186(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__2586395468412767186(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__2586395468412767186);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__2586395468412767186);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__2586395468412767186);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__1509827634459623678 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      64 /* Input0 */, \
      64 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1509827634459623678(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__1509827634459623678(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1509827634459623678(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1509827634459623678(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1509827634459623678(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__1509827634459623678(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1509827634459623678(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__1509827634459623678);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1509827634459623678);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__1509827634459623678);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__4523426111830274362 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      562949953421312 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4523426111830274362(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 562949953421312} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4523426111830274362(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4523426111830274362(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4523426111830274362(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4523426111830274362(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4523426111830274362(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4523426111830274362(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4523426111830274362);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4523426111830274362);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4523426111830274362);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__13684272468683845527 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      576 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13684272468683845527(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 576} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13684272468683845527(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13684272468683845527(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13684272468683845527(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13684272468683845527(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13684272468683845527(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13684272468683845527(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13684272468683845527);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13684272468683845527);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13684272468683845527);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__14860784130622091890 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2251799813685248 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14860784130622091890(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2251799813685248} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14860784130622091890(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14860784130622091890(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14860784130622091890(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14860784130622091890(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14860784130622091890(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14860784130622091890(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14860784130622091890);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14860784130622091890);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14860784130622091890);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__7059401947092985785 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      56 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7059401947092985785(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7059401947092985785(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7059401947092985785(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7059401947092985785(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7059401947092985785(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7059401947092985785(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7059401947092985785(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7059401947092985785);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7059401947092985785);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7059401947092985785);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__3577761469940783236 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      16384 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3577761469940783236(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__3577761469940783236(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3577761469940783236(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3577761469940783236(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3577761469940783236(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__3577761469940783236(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3577761469940783236(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__3577761469940783236);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3577761469940783236);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__3577761469940783236);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__11707288741229902380 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11707288741229902380(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11707288741229902380(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11707288741229902380(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11707288741229902380(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11707288741229902380(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11707288741229902380(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11707288741229902380(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11707288741229902380);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11707288741229902380);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11707288741229902380);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__929846030719021995 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      128 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__929846030719021995(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 128} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__929846030719021995(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__929846030719021995(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__929846030719021995(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__929846030719021995(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__929846030719021995(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__929846030719021995(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__929846030719021995);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__929846030719021995);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__929846030719021995);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__8491397372905389890 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      256 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8491397372905389890(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8491397372905389890(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8491397372905389890(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8491397372905389890(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8491397372905389890(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8491397372905389890(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8491397372905389890(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8491397372905389890);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8491397372905389890);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8491397372905389890);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__15748872946452709808 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      576 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15748872946452709808(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 576} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15748872946452709808(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15748872946452709808(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15748872946452709808(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15748872946452709808(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15748872946452709808(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15748872946452709808(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15748872946452709808);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15748872946452709808);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15748872946452709808);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__8153663774899938056 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      33554432 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8153663774899938056(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 33554432} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__8153663774899938056(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8153663774899938056(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8153663774899938056(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8153663774899938056(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__8153663774899938056(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8153663774899938056(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__8153663774899938056);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8153663774899938056);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__8153663774899938056);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__2767214238919195553 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1024 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2767214238919195553(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2767214238919195553(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2767214238919195553(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2767214238919195553(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2767214238919195553(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2767214238919195553(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2767214238919195553(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2767214238919195553);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2767214238919195553);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2767214238919195553);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__16257305389364143414 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      48 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16257305389364143414(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16257305389364143414(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16257305389364143414(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16257305389364143414(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16257305389364143414(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16257305389364143414(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16257305389364143414(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16257305389364143414);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16257305389364143414);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16257305389364143414);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__1242759927424124150 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      320 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1242759927424124150(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1242759927424124150(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1242759927424124150(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1242759927424124150(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1242759927424124150(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1242759927424124150(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1242759927424124150(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1242759927424124150);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1242759927424124150);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1242759927424124150);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__2249853980591980820 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      768 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2249853980591980820(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 768} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2249853980591980820(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2249853980591980820(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2249853980591980820(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2249853980591980820(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2249853980591980820(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2249853980591980820(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2249853980591980820);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2249853980591980820);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2249853980591980820);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4696420654802048879 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4696420654802048879(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4696420654802048879(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4696420654802048879(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4696420654802048879(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4696420654802048879(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4696420654802048879(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4696420654802048879(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4696420654802048879);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4696420654802048879);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4696420654802048879);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__14851916856216701176 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14851916856216701176(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14851916856216701176(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14851916856216701176(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14851916856216701176(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14851916856216701176(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14851916856216701176(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14851916856216701176(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14851916856216701176);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14851916856216701176);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14851916856216701176);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__4184246411943982165 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4184246411943982165(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__4184246411943982165(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4184246411943982165(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4184246411943982165(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4184246411943982165(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__4184246411943982165(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4184246411943982165(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__4184246411943982165);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4184246411943982165);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__4184246411943982165);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__2253745202065747016 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      128 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2253745202065747016(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 128} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2253745202065747016(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2253745202065747016(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2253745202065747016(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2253745202065747016(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2253745202065747016(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2253745202065747016(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2253745202065747016);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2253745202065747016);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2253745202065747016);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14664554858885838842 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      65536 /* Input0 */, \
      192 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      384 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14664554858885838842(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14664554858885838842(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14664554858885838842(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14664554858885838842(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14664554858885838842(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14664554858885838842(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14664554858885838842(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14664554858885838842);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14664554858885838842);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14664554858885838842);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__322793322210176929 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__322793322210176929(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__322793322210176929(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__322793322210176929(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__322793322210176929(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__322793322210176929(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__322793322210176929(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__322793322210176929(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__322793322210176929);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__322793322210176929);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__322793322210176929);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__13351008413858621432 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      65536 /* Input0 */, \
      128 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13351008413858621432(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13351008413858621432(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13351008413858621432(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13351008413858621432(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13351008413858621432(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13351008413858621432(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13351008413858621432(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13351008413858621432);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13351008413858621432);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13351008413858621432);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__11284725068635801256 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1024 /* Input0 */, \
      32 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11284725068635801256(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11284725068635801256(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11284725068635801256(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11284725068635801256(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11284725068635801256(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11284725068635801256(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11284725068635801256(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11284725068635801256);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11284725068635801256);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11284725068635801256);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__2407199598227995357 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1125899906842624 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2407199598227995357(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1125899906842624} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__2407199598227995357(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2407199598227995357(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2407199598227995357(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2407199598227995357(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__2407199598227995357(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2407199598227995357(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__2407199598227995357);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2407199598227995357);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__2407199598227995357);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__11745091834038179088 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      113 /* Input2 */, \
      113 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11745091834038179088(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 113} /* Input2 */, 
      {"input[3]", 113} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11745091834038179088(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11745091834038179088(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11745091834038179088(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11745091834038179088(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11745091834038179088(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11745091834038179088(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11745091834038179088);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11745091834038179088);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11745091834038179088);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__10256074791763079830 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      65536 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10256074791763079830(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10256074791763079830(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10256074791763079830(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10256074791763079830(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10256074791763079830(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10256074791763079830(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10256074791763079830(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10256074791763079830);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10256074791763079830);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10256074791763079830);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__1347396235716048791 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      3 /* Input0 */, \
      272 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1347396235716048791(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 3} /* Input0 */, 
      {"input[1]", 272} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1347396235716048791(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1347396235716048791(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1347396235716048791(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1347396235716048791(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1347396235716048791(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1347396235716048791(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1347396235716048791);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1347396235716048791);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1347396235716048791);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__9659383004367529164 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9659383004367529164(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9659383004367529164(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9659383004367529164(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9659383004367529164(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9659383004367529164(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9659383004367529164(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9659383004367529164(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9659383004367529164);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9659383004367529164);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9659383004367529164);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5007981215488386016 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5007981215488386016(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5007981215488386016(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5007981215488386016(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5007981215488386016(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5007981215488386016(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5007981215488386016(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5007981215488386016(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5007981215488386016);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5007981215488386016);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5007981215488386016);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14865537619099157052 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      136 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      136 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14865537619099157052(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 136} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 136} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14865537619099157052(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14865537619099157052(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14865537619099157052(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14865537619099157052(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14865537619099157052(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14865537619099157052(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14865537619099157052);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14865537619099157052);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14865537619099157052);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__15590199560045644816 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      17 /* PadHeight */, \
      17 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      17 /* DilationWidth */, \
      17 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15590199560045644816(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 17} /* PadHeight */, 
      {"pad_width", 17} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 17} /* DilationWidth */, 
      {"dilation_width", 17} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15590199560045644816(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15590199560045644816(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15590199560045644816(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15590199560045644816(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15590199560045644816(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15590199560045644816(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15590199560045644816);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15590199560045644816);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15590199560045644816);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__563164703380424515 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      64 /* Input2 */, \
      64 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__563164703380424515(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 64} /* Input2 */, 
      {"input[3]", 64} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__563164703380424515(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__563164703380424515(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__563164703380424515(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__563164703380424515(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__563164703380424515(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__563164703380424515(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__563164703380424515);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__563164703380424515);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__563164703380424515);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__6700523398334236663 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      960 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6700523398334236663(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 960} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__6700523398334236663(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6700523398334236663(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6700523398334236663(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6700523398334236663(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__6700523398334236663(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6700523398334236663(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__6700523398334236663);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6700523398334236663);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__6700523398334236663);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__4639413846168170832 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      8388608 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4639413846168170832(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 8388608} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4639413846168170832(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4639413846168170832(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4639413846168170832(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4639413846168170832(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4639413846168170832(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4639413846168170832(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4639413846168170832);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4639413846168170832);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4639413846168170832);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__2176596800611267547 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2176596800611267547(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2176596800611267547(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2176596800611267547(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2176596800611267547(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2176596800611267547(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2176596800611267547(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2176596800611267547(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2176596800611267547);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2176596800611267547);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2176596800611267547);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__15066595224623157342 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      128 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      160 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15066595224623157342(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15066595224623157342(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15066595224623157342(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15066595224623157342(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15066595224623157342(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15066595224623157342(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15066595224623157342(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15066595224623157342);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15066595224623157342);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15066595224623157342);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__3358742332551916031 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3358742332551916031(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3358742332551916031(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3358742332551916031(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3358742332551916031(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3358742332551916031(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3358742332551916031(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3358742332551916031(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3358742332551916031);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3358742332551916031);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3358742332551916031);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6189153574142127990 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      1024 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      1024 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6189153574142127990(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6189153574142127990(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6189153574142127990(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6189153574142127990(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6189153574142127990(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6189153574142127990(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6189153574142127990(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6189153574142127990);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6189153574142127990);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6189153574142127990);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11045546135996421228 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      147456 /* Input0 */, \
      352 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11045546135996421228(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 147456} /* Input0 */, 
      {"input[1]", 352} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11045546135996421228(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11045546135996421228(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11045546135996421228(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11045546135996421228(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11045546135996421228(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11045546135996421228(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11045546135996421228);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11045546135996421228);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11045546135996421228);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__559827208636914868 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      16384 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__559827208636914868(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__559827208636914868(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__559827208636914868(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__559827208636914868(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__559827208636914868(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__559827208636914868(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__559827208636914868(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__559827208636914868);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__559827208636914868);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__559827208636914868);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__4332655453383843464 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      64 /* Input0 */, \
      24 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4332655453383843464(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4332655453383843464(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4332655453383843464(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4332655453383843464(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4332655453383843464(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4332655453383843464(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4332655453383843464(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4332655453383843464);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4332655453383843464);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4332655453383843464);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8024735633449325977 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8024735633449325977(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8024735633449325977(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8024735633449325977(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8024735633449325977(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8024735633449325977(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8024735633449325977(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8024735633449325977(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8024735633449325977);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8024735633449325977);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8024735633449325977);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__2798960848652674457 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      136 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2798960848652674457(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 136} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__2798960848652674457(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2798960848652674457(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2798960848652674457(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2798960848652674457(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__2798960848652674457(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2798960848652674457(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__2798960848652674457);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2798960848652674457);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__2798960848652674457);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__17028562306284522112 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      36864 /* Input0 */, \
      192 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      352 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17028562306284522112(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 36864} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 352} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17028562306284522112(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17028562306284522112(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17028562306284522112(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17028562306284522112(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17028562306284522112(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17028562306284522112(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17028562306284522112);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17028562306284522112);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17028562306284522112);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__3317165572865895619 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      3072 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3317165572865895619(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 3072} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3317165572865895619(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3317165572865895619(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3317165572865895619(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3317165572865895619(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3317165572865895619(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3317165572865895619(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3317165572865895619);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3317165572865895619);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3317165572865895619);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2017279428645824320 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2017279428645824320(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2017279428645824320(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2017279428645824320(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2017279428645824320(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2017279428645824320(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2017279428645824320(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2017279428645824320(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2017279428645824320);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2017279428645824320);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2017279428645824320);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8584361682468920400 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      5 /* PadHeight */, \
      5 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      5 /* DilationWidth */, \
      5 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8584361682468920400(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 5} /* PadHeight */, 
      {"pad_width", 5} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 5} /* DilationWidth */, 
      {"dilation_width", 5} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8584361682468920400(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8584361682468920400(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8584361682468920400(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8584361682468920400(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8584361682468920400(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8584361682468920400(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8584361682468920400);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8584361682468920400);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8584361682468920400);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2609686270873279149 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      16 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2609686270873279149(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2609686270873279149(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2609686270873279149(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2609686270873279149(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2609686270873279149(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2609686270873279149(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2609686270873279149(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2609686270873279149);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2609686270873279149);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2609686270873279149);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__1688560366851710616 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      549755813888 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1688560366851710616(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 549755813888} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__1688560366851710616(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1688560366851710616(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1688560366851710616(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1688560366851710616(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__1688560366851710616(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1688560366851710616(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__1688560366851710616);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1688560366851710616);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__1688560366851710616);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9997427891970641933 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      16777216 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9997427891970641933(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16777216} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9997427891970641933(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9997427891970641933(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9997427891970641933(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9997427891970641933(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9997427891970641933(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9997427891970641933(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9997427891970641933);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9997427891970641933);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9997427891970641933);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__7189308919723700734 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__7189308919723700734(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__7189308919723700734(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__7189308919723700734(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__7189308919723700734(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__7189308919723700734(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__7189308919723700734(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__7189308919723700734(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__7189308919723700734);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__7189308919723700734);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__7189308919723700734);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace CUDNN_POOLING_FWD__14717729230969685952 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1024 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__14717729230969685952(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__14717729230969685952(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__14717729230969685952(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__14717729230969685952(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__14717729230969685952(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__14717729230969685952(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__14717729230969685952(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__14717729230969685952);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__14717729230969685952);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__14717729230969685952);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12281026278197441823 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1073741824 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12281026278197441823(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1073741824} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12281026278197441823(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12281026278197441823(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12281026278197441823(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12281026278197441823(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__12281026278197441823(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12281026278197441823(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12281026278197441823);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12281026278197441823);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__12281026278197441823);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__10250960441077778463 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10250960441077778463(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__10250960441077778463(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10250960441077778463(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10250960441077778463(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10250960441077778463(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__10250960441077778463(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10250960441077778463(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__10250960441077778463);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10250960441077778463);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__10250960441077778463);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__13232793519627201510 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13232793519627201510(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13232793519627201510(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13232793519627201510(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13232793519627201510(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13232793519627201510(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__13232793519627201510(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13232793519627201510(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13232793519627201510);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13232793519627201510);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__13232793519627201510);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_SOFTMAX_FWD__5431290219690225716 {

#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      19 /* Input1 */, \
      -1 /* Input2 */, \
      -1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_ADD_COUNTERS__5431290219690225716(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 19} /* Input1 */, 
      {"input[2]", -1} /* Input2 */, 
      {"input[3]", -1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnSoftmaxAlgorithm_t softmax_algorithm, cudnnSoftmaxMode_t softmax_mode>
static void LAYER_CUDNN_SOFTMAX_FWD_FLOAT16__5431290219690225716(benchmark::State& state) {
  LAYER_CUDNN_SOFTMAX_FWD_Impl<__half, softmax_algorithm, softmax_mode>(state);
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_ADD_COUNTERS__5431290219690225716(state);
}

template <cudnnSoftmaxAlgorithm_t softmax_algorithm, cudnnSoftmaxMode_t softmax_mode>
static void LAYER_CUDNN_SOFTMAX_FWD_FLOAT32__5431290219690225716(benchmark::State& state) {
  LAYER_CUDNN_SOFTMAX_FWD_Impl<float, softmax_algorithm, softmax_mode>(state);
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_ADD_COUNTERS__5431290219690225716(state);
}

template <cudnnSoftmaxAlgorithm_t softmax_algorithm, cudnnSoftmaxMode_t softmax_mode>
static void LAYER_CUDNN_SOFTMAX_FWD_TENSORCOREHALF__5431290219690225716(benchmark::State& state) {
  LAYER_CUDNN_SOFTMAX_FWD_Impl<__half, softmax_algorithm, softmax_mode>(state);
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_ADD_COUNTERS__5431290219690225716(state);
}

#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, SOFTMAX_MODE) \
 BENCHMARK_TEMPLATE(b, CUDNN_SOFTMAX_MODE_INSTANCE, SOFTMAX_MODE)->BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_SOFTMAX_MODE_CHANNEL, SOFTMAX_MODE)->BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD(b)                                                                                             \
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, CUDNN_SOFTMAX_MODE_INSTANCE);                                                                    \
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, CUDNN_SOFTMAX_MODE_CHANNEL)

BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD(LAYER_CUDNN_SOFTMAX_FWD_FLOAT16__5431290219690225716);
BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD(LAYER_CUDNN_SOFTMAX_FWD_FLOAT32__5431290219690225716);
BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD(LAYER_CUDNN_SOFTMAX_FWD_TENSORCOREHALF__5431290219690225716);

#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0
#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__6587322338349955310 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6587322338349955310(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__6587322338349955310(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6587322338349955310(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6587322338349955310(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6587322338349955310(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__6587322338349955310(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6587322338349955310(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__6587322338349955310);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6587322338349955310);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__6587322338349955310);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__13389276514195684497 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13389276514195684497(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13389276514195684497(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13389276514195684497(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13389276514195684497(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13389276514195684497(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__13389276514195684497(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13389276514195684497(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13389276514195684497);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13389276514195684497);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__13389276514195684497);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__400902856783100257 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      192 /* Input0 */, \
      224 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__400902856783100257(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 192} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__400902856783100257(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__400902856783100257(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__400902856783100257(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__400902856783100257(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__400902856783100257(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__400902856783100257(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__400902856783100257);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__400902856783100257);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__400902856783100257);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__1402644738247025198 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      128 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1402644738247025198(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__1402644738247025198(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1402644738247025198(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1402644738247025198(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1402644738247025198(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__1402644738247025198(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1402644738247025198(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__1402644738247025198);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1402644738247025198);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__1402644738247025198);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__13826717052703284460 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13826717052703284460(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13826717052703284460(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13826717052703284460(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13826717052703284460(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13826717052703284460(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__13826717052703284460(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13826717052703284460(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13826717052703284460);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13826717052703284460);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__13826717052703284460);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__17846997608818076267 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      65536 /* Input0 */, \
      192 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      384 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17846997608818076267(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17846997608818076267(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17846997608818076267(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17846997608818076267(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17846997608818076267(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17846997608818076267(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17846997608818076267(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17846997608818076267);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17846997608818076267);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17846997608818076267);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4830328438837449332 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      13 /* Input2 */, \
      14 /* Input3 */, \
      384 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4830328438837449332(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4830328438837449332(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4830328438837449332(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4830328438837449332(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4830328438837449332(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4830328438837449332(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4830328438837449332(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4830328438837449332);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4830328438837449332);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4830328438837449332);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14423607935759941477 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14423607935759941477(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14423607935759941477(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14423607935759941477(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14423607935759941477(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14423607935759941477(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__14423607935759941477(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14423607935759941477(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14423607935759941477);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14423607935759941477);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__14423607935759941477);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__14969726060953044630 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14969726060953044630(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14969726060953044630(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14969726060953044630(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14969726060953044630(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14969726060953044630(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14969726060953044630(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14969726060953044630(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14969726060953044630);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14969726060953044630);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14969726060953044630);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9754641982578257719 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9754641982578257719(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9754641982578257719(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9754641982578257719(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9754641982578257719(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9754641982578257719(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9754641982578257719(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9754641982578257719(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9754641982578257719);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9754641982578257719);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9754641982578257719);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__15287945354445804899 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      147456 /* Input0 */, \
      128 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15287945354445804899(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 147456} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15287945354445804899(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15287945354445804899(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15287945354445804899(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15287945354445804899(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15287945354445804899(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15287945354445804899(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15287945354445804899);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15287945354445804899);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15287945354445804899);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14410813508407374226 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      64 /* Input0 */, \
      192 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14410813508407374226(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14410813508407374226(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14410813508407374226(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14410813508407374226(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14410813508407374226(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14410813508407374226(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14410813508407374226(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14410813508407374226);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14410813508407374226);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14410813508407374226);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5581606760375094591 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      3072 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5581606760375094591(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 3072} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5581606760375094591(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5581606760375094591(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5581606760375094591(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5581606760375094591(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5581606760375094591(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5581606760375094591(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5581606760375094591);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5581606760375094591);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5581606760375094591);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__13086186896193184571 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      112 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13086186896193184571(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13086186896193184571(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13086186896193184571(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13086186896193184571(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13086186896193184571(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__13086186896193184571(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13086186896193184571(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13086186896193184571);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13086186896193184571);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__13086186896193184571);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9595851555439306485 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9595851555439306485(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9595851555439306485(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9595851555439306485(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9595851555439306485(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9595851555439306485(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9595851555439306485(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9595851555439306485(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9595851555439306485);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9595851555439306485);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9595851555439306485);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__15265427960135398155 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1048576 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15265427960135398155(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1048576} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15265427960135398155(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15265427960135398155(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15265427960135398155(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15265427960135398155(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__15265427960135398155(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15265427960135398155(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15265427960135398155);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15265427960135398155);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__15265427960135398155);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__11801108119322466810 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      768 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11801108119322466810(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 768} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__11801108119322466810(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11801108119322466810(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11801108119322466810(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11801108119322466810(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__11801108119322466810(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11801108119322466810(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__11801108119322466810);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11801108119322466810);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__11801108119322466810);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__655617116379613940 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      113 /* Input2 */, \
      113 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__655617116379613940(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 113} /* Input2 */, 
      {"input[3]", 113} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__655617116379613940(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__655617116379613940(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__655617116379613940(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__655617116379613940(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__655617116379613940(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__655617116379613940(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__655617116379613940);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__655617116379613940);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__655617116379613940);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__7960622227350057186 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      4398046511104 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7960622227350057186(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4398046511104} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7960622227350057186(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7960622227350057186(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7960622227350057186(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7960622227350057186(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7960622227350057186(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7960622227350057186(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7960622227350057186);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7960622227350057186);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7960622227350057186);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__16768484298576729075 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      128 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16768484298576729075(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16768484298576729075(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16768484298576729075(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16768484298576729075(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16768484298576729075(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16768484298576729075(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16768484298576729075(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16768484298576729075);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16768484298576729075);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16768484298576729075);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__1045204185803365888 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      112 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1045204185803365888(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1045204185803365888(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1045204185803365888(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1045204185803365888(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1045204185803365888(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1045204185803365888(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1045204185803365888(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1045204185803365888);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1045204185803365888);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1045204185803365888);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11343909968865375830 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11343909968865375830(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11343909968865375830(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11343909968865375830(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11343909968865375830(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11343909968865375830(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11343909968865375830(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11343909968865375830(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11343909968865375830);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11343909968865375830);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11343909968865375830);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11154317111988684996 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11154317111988684996(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11154317111988684996(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11154317111988684996(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11154317111988684996(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11154317111988684996(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11154317111988684996(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11154317111988684996(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11154317111988684996);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11154317111988684996);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11154317111988684996);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__7019768816879069827 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      536870912 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7019768816879069827(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 536870912} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7019768816879069827(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7019768816879069827(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7019768816879069827(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7019768816879069827(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7019768816879069827(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7019768816879069827(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7019768816879069827);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7019768816879069827);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7019768816879069827);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14249369528291734672 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      72057594037927936 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14249369528291734672(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 72057594037927936} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14249369528291734672(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14249369528291734672(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14249369528291734672(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14249369528291734672(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__14249369528291734672(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14249369528291734672(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14249369528291734672);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14249369528291734672);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__14249369528291734672);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__6616160412211859941 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6616160412211859941(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6616160412211859941(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6616160412211859941(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6616160412211859941(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6616160412211859941(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6616160412211859941(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6616160412211859941(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6616160412211859941);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6616160412211859941);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6616160412211859941);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9677354602501715645 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      64 /* Input2 */, \
      64 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9677354602501715645(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 64} /* Input2 */, 
      {"input[3]", 64} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9677354602501715645(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9677354602501715645(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9677354602501715645(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9677354602501715645(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9677354602501715645(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9677354602501715645(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9677354602501715645);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9677354602501715645);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9677354602501715645);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10054633226223086376 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10054633226223086376(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10054633226223086376(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10054633226223086376(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10054633226223086376(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10054633226223086376(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10054633226223086376(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10054633226223086376(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10054633226223086376);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10054633226223086376);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10054633226223086376);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__13073471456554292191 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      2048 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      304 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      24 /* PadHeight */, \
      24 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      24 /* DilationWidth */, \
      24 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13073471456554292191(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 304} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 24} /* PadHeight */, 
      {"pad_width", 24} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 24} /* DilationWidth */, 
      {"dilation_width", 24} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13073471456554292191(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13073471456554292191(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13073471456554292191(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13073471456554292191(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13073471456554292191(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13073471456554292191(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13073471456554292191);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13073471456554292191);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13073471456554292191);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__1166150958227095925 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      256 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1166150958227095925(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__1166150958227095925(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1166150958227095925(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1166150958227095925(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1166150958227095925(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__1166150958227095925(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1166150958227095925(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__1166150958227095925);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1166150958227095925);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__1166150958227095925);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14738692929490800623 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      8589934592 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14738692929490800623(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 8589934592} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14738692929490800623(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14738692929490800623(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14738692929490800623(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14738692929490800623(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__14738692929490800623(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14738692929490800623(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14738692929490800623);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14738692929490800623);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__14738692929490800623);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__921194561464388717 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      32 /* Input0 */, \
      128 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__921194561464388717(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 32} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__921194561464388717(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__921194561464388717(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__921194561464388717(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__921194561464388717(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__921194561464388717(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__921194561464388717(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__921194561464388717);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__921194561464388717);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__921194561464388717);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2580687081091458903 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      65536 /* Input0 */, \
      48 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      128 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2580687081091458903(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2580687081091458903(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2580687081091458903(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2580687081091458903(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2580687081091458903(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2580687081091458903(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2580687081091458903(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2580687081091458903);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2580687081091458903);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2580687081091458903);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__13166292911169973044 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      16384 /* Input0 */, \
      32 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13166292911169973044(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13166292911169973044(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13166292911169973044(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13166292911169973044(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13166292911169973044(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13166292911169973044(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13166292911169973044(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13166292911169973044);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13166292911169973044);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13166292911169973044);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__18205288761369174264 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18205288761369174264(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__18205288761369174264(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18205288761369174264(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18205288761369174264(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18205288761369174264(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18205288761369174264(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18205288761369174264(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__18205288761369174264);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18205288761369174264);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18205288761369174264);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__5910038520356559784 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      136 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5910038520356559784(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 136} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__5910038520356559784(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5910038520356559784(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5910038520356559784(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5910038520356559784(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__5910038520356559784(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5910038520356559784(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__5910038520356559784);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5910038520356559784);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__5910038520356559784);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__16759335886785363168 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      64 /* Input0 */, \
      192 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      112 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16759335886785363168(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 112} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16759335886785363168(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16759335886785363168(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16759335886785363168(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16759335886785363168(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16759335886785363168(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16759335886785363168(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16759335886785363168);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16759335886785363168);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16759335886785363168);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10425853395156755740 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      272 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      272 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10425853395156755740(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 272} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 272} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10425853395156755740(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10425853395156755740(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10425853395156755740(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10425853395156755740(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10425853395156755740(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10425853395156755740(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10425853395156755740);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10425853395156755740);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10425853395156755740);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6960240713304893200 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      24 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6960240713304893200(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 24} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6960240713304893200(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6960240713304893200(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6960240713304893200(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6960240713304893200(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6960240713304893200(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6960240713304893200(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6960240713304893200);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6960240713304893200);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6960240713304893200);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__3369340449262525096 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      140737488355328 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3369340449262525096(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 140737488355328} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3369340449262525096(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3369340449262525096(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3369340449262525096(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3369340449262525096(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3369340449262525096(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3369340449262525096(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3369340449262525096);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3369340449262525096);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3369340449262525096);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__8531068197930228513 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      64 /* Input0 */, \
      128 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__8531068197930228513(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__8531068197930228513(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__8531068197930228513(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__8531068197930228513(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__8531068197930228513(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__8531068197930228513(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__8531068197930228513(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__8531068197930228513);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__8531068197930228513);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__8531068197930228513);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__4505006876393903544 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      16 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4505006876393903544(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4505006876393903544(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4505006876393903544(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4505006876393903544(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4505006876393903544(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4505006876393903544(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4505006876393903544(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4505006876393903544);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4505006876393903544);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4505006876393903544);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9630831414261982704 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      144115188075855872 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9630831414261982704(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 144115188075855872} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9630831414261982704(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9630831414261982704(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9630831414261982704(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9630831414261982704(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9630831414261982704(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9630831414261982704(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9630831414261982704);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9630831414261982704);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9630831414261982704);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__101516499968122821 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      3072 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__101516499968122821(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 3072} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__101516499968122821(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__101516499968122821(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__101516499968122821(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__101516499968122821(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__101516499968122821(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__101516499968122821(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__101516499968122821);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__101516499968122821);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__101516499968122821);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__12227703116167435475 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      112 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12227703116167435475(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12227703116167435475(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12227703116167435475(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12227703116167435475(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12227703116167435475(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12227703116167435475(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12227703116167435475(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12227703116167435475);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12227703116167435475);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12227703116167435475);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8590430140272131569 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      200 /* Input2 */, \
      200 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8590430140272131569(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 200} /* Input2 */, 
      {"input[3]", 200} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8590430140272131569(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8590430140272131569(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8590430140272131569(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8590430140272131569(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8590430140272131569(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8590430140272131569(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8590430140272131569);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8590430140272131569);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8590430140272131569);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__18318123974799177623 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18318123974799177623(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__18318123974799177623(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18318123974799177623(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18318123974799177623(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18318123974799177623(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18318123974799177623(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18318123974799177623(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__18318123974799177623);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18318123974799177623);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18318123974799177623);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__688575722595389288 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      262144 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__688575722595389288(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 262144} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__688575722595389288(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__688575722595389288(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__688575722595389288(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__688575722595389288(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__688575722595389288(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__688575722595389288(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__688575722595389288);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__688575722595389288);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__688575722595389288);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__17513441674430556641 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      112 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17513441674430556641(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__17513441674430556641(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17513441674430556641(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17513441674430556641(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17513441674430556641(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__17513441674430556641(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17513441674430556641(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__17513441674430556641);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17513441674430556641);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__17513441674430556641);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12144603391211286616 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      17592186044416 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12144603391211286616(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 17592186044416} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12144603391211286616(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12144603391211286616(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12144603391211286616(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12144603391211286616(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__12144603391211286616(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12144603391211286616(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12144603391211286616);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12144603391211286616);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__12144603391211286616);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__18055019730382888036 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      12288 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__18055019730382888036(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 12288} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__18055019730382888036(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__18055019730382888036(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__18055019730382888036(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__18055019730382888036(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__18055019730382888036(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__18055019730382888036(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__18055019730382888036);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__18055019730382888036);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__18055019730382888036);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__4916437551457201187 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      2048 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4916437551457201187(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 2048} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4916437551457201187(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4916437551457201187(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4916437551457201187(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4916437551457201187(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4916437551457201187(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4916437551457201187(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4916437551457201187);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4916437551457201187);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4916437551457201187);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8635128834358667164 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      64 /* Input0 */, \
      192 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      24 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8635128834358667164(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 24} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8635128834358667164(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8635128834358667164(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8635128834358667164(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8635128834358667164(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8635128834358667164(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8635128834358667164(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8635128834358667164);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8635128834358667164);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8635128834358667164);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11504986725474845056 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      16384 /* Input0 */, \
      32 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      128 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11504986725474845056(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11504986725474845056(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11504986725474845056(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11504986725474845056(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11504986725474845056(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11504986725474845056(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11504986725474845056(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11504986725474845056);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11504986725474845056);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11504986725474845056);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__12751304857625931636 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      64 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__12751304857625931636(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__12751304857625931636(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__12751304857625931636(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__12751304857625931636(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__12751304857625931636(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__12751304857625931636(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__12751304857625931636(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__12751304857625931636);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__12751304857625931636);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__12751304857625931636);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__11948425713608109062 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11948425713608109062(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__11948425713608109062(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11948425713608109062(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11948425713608109062(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11948425713608109062(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__11948425713608109062(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11948425713608109062(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__11948425713608109062);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11948425713608109062);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__11948425713608109062);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__1383596300876944974 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      112 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1383596300876944974(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1383596300876944974(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1383596300876944974(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1383596300876944974(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1383596300876944974(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1383596300876944974(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1383596300876944974(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1383596300876944974);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1383596300876944974);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1383596300876944974);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__17765940260938667560 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      256 /* Input0 */, \
      160 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17765940260938667560(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17765940260938667560(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17765940260938667560(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17765940260938667560(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17765940260938667560(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17765940260938667560(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17765940260938667560(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17765940260938667560);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17765940260938667560);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17765940260938667560);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__7119952269216754336 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      36864 /* Input0 */, \
      224 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      224 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7119952269216754336(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 36864} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 224} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7119952269216754336(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7119952269216754336(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7119952269216754336(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7119952269216754336(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7119952269216754336(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7119952269216754336(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7119952269216754336);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7119952269216754336);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7119952269216754336);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__7872598735042137847 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7872598735042137847(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__7872598735042137847(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7872598735042137847(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7872598735042137847(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7872598735042137847(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__7872598735042137847(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7872598735042137847(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__7872598735042137847);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7872598735042137847);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__7872598735042137847);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__1303169106277866119 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      268435456 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1303169106277866119(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 268435456} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__1303169106277866119(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1303169106277866119(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1303169106277866119(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1303169106277866119(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__1303169106277866119(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1303169106277866119(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__1303169106277866119);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1303169106277866119);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__1303169106277866119);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__1890897755105558190 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      1024 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1890897755105558190(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1890897755105558190(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1890897755105558190(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1890897755105558190(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1890897755105558190(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1890897755105558190(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1890897755105558190(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1890897755105558190);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1890897755105558190);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__1890897755105558190);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__10090487539852790079 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10090487539852790079(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__10090487539852790079(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10090487539852790079(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10090487539852790079(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10090487539852790079(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__10090487539852790079(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10090487539852790079(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__10090487539852790079);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10090487539852790079);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__10090487539852790079);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__18193188769337621316 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      536870912 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18193188769337621316(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 536870912} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__18193188769337621316(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18193188769337621316(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18193188769337621316(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18193188769337621316(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18193188769337621316(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18193188769337621316(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__18193188769337621316);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18193188769337621316);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18193188769337621316);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__3761829803643680247 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      113 /* Input2 */, \
      113 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3761829803643680247(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 113} /* Input2 */, 
      {"input[3]", 113} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__3761829803643680247(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3761829803643680247(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3761829803643680247(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3761829803643680247(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__3761829803643680247(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3761829803643680247(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__3761829803643680247);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3761829803643680247);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__3761829803643680247);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__13188557898215905940 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      64 /* Input0 */, \
      112 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13188557898215905940(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13188557898215905940(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13188557898215905940(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13188557898215905940(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13188557898215905940(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13188557898215905940(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13188557898215905940(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13188557898215905940);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13188557898215905940);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13188557898215905940);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5087961383746567240 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      113 /* Input2 */, \
      113 /* Input3 */, \
      96 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5087961383746567240(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 113} /* Input2 */, 
      {"input[3]", 113} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5087961383746567240(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5087961383746567240(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5087961383746567240(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5087961383746567240(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5087961383746567240(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5087961383746567240(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5087961383746567240);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5087961383746567240);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5087961383746567240);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__16100062131416821315 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1024 /* Input0 */, \
      144 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      288 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16100062131416821315(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 288} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16100062131416821315(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16100062131416821315(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16100062131416821315(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16100062131416821315(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16100062131416821315(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16100062131416821315(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16100062131416821315);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16100062131416821315);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16100062131416821315);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__258676733556230250 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__258676733556230250(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__258676733556230250(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__258676733556230250(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__258676733556230250(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__258676733556230250(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__258676733556230250(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__258676733556230250(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__258676733556230250);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__258676733556230250);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__258676733556230250);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__4324963748548897634 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      65536 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4324963748548897634(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4324963748548897634(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4324963748548897634(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4324963748548897634(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4324963748548897634(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4324963748548897634(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4324963748548897634(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4324963748548897634);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4324963748548897634);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4324963748548897634);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__2769585096052858245 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      256 /* Input0 */, \
      1000 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2769585096052858245(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 1000} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2769585096052858245(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2769585096052858245(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2769585096052858245(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2769585096052858245(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2769585096052858245(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2769585096052858245(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2769585096052858245);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2769585096052858245);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2769585096052858245);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__5480237417219601125 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      -1 /* Input2 */, \
      -1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5480237417219601125(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", -1} /* Input2 */, 
      {"input[3]", -1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__5480237417219601125(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5480237417219601125(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5480237417219601125(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5480237417219601125(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__5480237417219601125(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5480237417219601125(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__5480237417219601125);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5480237417219601125);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__5480237417219601125);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__10711361837099719838 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      16384 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10711361837099719838(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10711361837099719838(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10711361837099719838(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10711361837099719838(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10711361837099719838(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10711361837099719838(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10711361837099719838(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10711361837099719838);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10711361837099719838);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10711361837099719838);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__16795031116328050645 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      2048 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16795031116328050645(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16795031116328050645(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16795031116328050645(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16795031116328050645(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16795031116328050645(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16795031116328050645(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16795031116328050645(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16795031116328050645);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16795031116328050645);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16795031116328050645);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__6222155912323183797 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      65536 /* Input0 */, \
      192 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6222155912323183797(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6222155912323183797(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6222155912323183797(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6222155912323183797(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6222155912323183797(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6222155912323183797(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6222155912323183797(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6222155912323183797);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6222155912323183797);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6222155912323183797);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__7596193724664908279 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      524288 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7596193724664908279(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 524288} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7596193724664908279(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7596193724664908279(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7596193724664908279(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7596193724664908279(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7596193724664908279(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7596193724664908279(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7596193724664908279);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7596193724664908279);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7596193724664908279);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14183598413523043295 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      768 /* Input0 */, \
      192 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14183598413523043295(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 768} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14183598413523043295(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14183598413523043295(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14183598413523043295(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14183598413523043295(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14183598413523043295(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14183598413523043295(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14183598413523043295);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14183598413523043295);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14183598413523043295);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__879058190776514970 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      136 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__879058190776514970(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 136} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__879058190776514970(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__879058190776514970(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__879058190776514970(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__879058190776514970(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__879058190776514970(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__879058190776514970(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__879058190776514970);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__879058190776514970);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__879058190776514970);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__17690150119706826932 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      131072 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17690150119706826932(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 131072} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17690150119706826932(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17690150119706826932(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17690150119706826932(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17690150119706826932(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17690150119706826932(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17690150119706826932(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17690150119706826932);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17690150119706826932);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17690150119706826932);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8999064973855723859 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      65536 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      384 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8999064973855723859(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8999064973855723859(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8999064973855723859(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8999064973855723859(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8999064973855723859(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8999064973855723859(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8999064973855723859(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8999064973855723859);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8999064973855723859);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8999064973855723859);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__1330582812273498927 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1024 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1330582812273498927(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1330582812273498927(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1330582812273498927(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1330582812273498927(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1330582812273498927(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1330582812273498927(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1330582812273498927(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1330582812273498927);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1330582812273498927);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1330582812273498927);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__9774465385270552464 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9774465385270552464(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__9774465385270552464(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9774465385270552464(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9774465385270552464(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9774465385270552464(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__9774465385270552464(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9774465385270552464(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__9774465385270552464);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9774465385270552464);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__9774465385270552464);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__9425059079054548626 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9425059079054548626(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__9425059079054548626(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9425059079054548626(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9425059079054548626(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9425059079054548626(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__9425059079054548626(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9425059079054548626(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__9425059079054548626);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9425059079054548626);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__9425059079054548626);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__14228696165765725367 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1099511627776 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14228696165765725367(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1099511627776} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14228696165765725367(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14228696165765725367(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14228696165765725367(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14228696165765725367(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14228696165765725367(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14228696165765725367(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14228696165765725367);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14228696165765725367);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14228696165765725367);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__16169071959264581191 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      56 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__16169071959264581191(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__16169071959264581191(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__16169071959264581191(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__16169071959264581191(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__16169071959264581191(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__16169071959264581191(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__16169071959264581191(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__16169071959264581191);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__16169071959264581191);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__16169071959264581191);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__15061484471065749110 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      147456 /* Input0 */, \
      320 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15061484471065749110(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 147456} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15061484471065749110(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15061484471065749110(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15061484471065749110(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15061484471065749110(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15061484471065749110(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15061484471065749110(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15061484471065749110);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15061484471065749110);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15061484471065749110);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__2791420341158761104 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2791420341158761104(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2791420341158761104(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2791420341158761104(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2791420341158761104(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2791420341158761104(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2791420341158761104(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2791420341158761104(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2791420341158761104);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2791420341158761104);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2791420341158761104);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__1450987090372482704 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      401 /* Input2 */, \
      401 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1450987090372482704(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 401} /* Input2 */, 
      {"input[3]", 401} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1450987090372482704(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1450987090372482704(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1450987090372482704(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1450987090372482704(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1450987090372482704(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1450987090372482704(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1450987090372482704);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1450987090372482704);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1450987090372482704);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__7589569730387731672 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__7589569730387731672(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__7589569730387731672(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__7589569730387731672(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__7589569730387731672(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__7589569730387731672(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__7589569730387731672(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__7589569730387731672(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__7589569730387731672);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__7589569730387731672);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__7589569730387731672);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__7316469572888256707 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      36864 /* Input0 */, \
      160 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      224 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7316469572888256707(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 36864} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 224} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7316469572888256707(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7316469572888256707(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7316469572888256707(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7316469572888256707(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7316469572888256707(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7316469572888256707(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7316469572888256707);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7316469572888256707);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7316469572888256707);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__489728827704105830 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      144115188075855872 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__489728827704105830(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 144115188075855872} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__489728827704105830(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__489728827704105830(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__489728827704105830(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__489728827704105830(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__489728827704105830(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__489728827704105830(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__489728827704105830);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__489728827704105830);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__489728827704105830);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__17898786480140275152 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      65536 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17898786480140275152(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17898786480140275152(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17898786480140275152(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17898786480140275152(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17898786480140275152(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17898786480140275152(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17898786480140275152(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17898786480140275152);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17898786480140275152);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17898786480140275152);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6487948375738963180 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6487948375738963180(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6487948375738963180(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6487948375738963180(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6487948375738963180(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6487948375738963180(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6487948375738963180(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6487948375738963180(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6487948375738963180);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6487948375738963180);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6487948375738963180);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6789528350087717393 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      192 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6789528350087717393(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 192} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6789528350087717393(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6789528350087717393(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6789528350087717393(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6789528350087717393(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6789528350087717393(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6789528350087717393(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6789528350087717393);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6789528350087717393);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6789528350087717393);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__8519664154636636269 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8519664154636636269(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__8519664154636636269(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8519664154636636269(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8519664154636636269(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8519664154636636269(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__8519664154636636269(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8519664154636636269(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__8519664154636636269);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8519664154636636269);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__8519664154636636269);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__3446158860206199721 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3446158860206199721(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3446158860206199721(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3446158860206199721(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3446158860206199721(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3446158860206199721(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3446158860206199721(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3446158860206199721(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3446158860206199721);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3446158860206199721);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3446158860206199721);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4664422036270033969 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4664422036270033969(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4664422036270033969(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4664422036270033969(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4664422036270033969(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4664422036270033969(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4664422036270033969(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4664422036270033969(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4664422036270033969);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4664422036270033969);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4664422036270033969);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__18127569558886495492 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      16 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      32 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18127569558886495492(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__18127569558886495492(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18127569558886495492(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18127569558886495492(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18127569558886495492(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18127569558886495492(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18127569558886495492(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__18127569558886495492);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18127569558886495492);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18127569558886495492);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__7637634467444500316 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      384 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7637634467444500316(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7637634467444500316(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7637634467444500316(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7637634467444500316(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7637634467444500316(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7637634467444500316(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7637634467444500316(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7637634467444500316);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7637634467444500316);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7637634467444500316);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__3443262638960266085 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__3443262638960266085(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__3443262638960266085(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__3443262638960266085(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__3443262638960266085(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__3443262638960266085(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__3443262638960266085(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__3443262638960266085(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__3443262638960266085);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__3443262638960266085);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__3443262638960266085);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14293735964714789207 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      32 /* Input0 */, \
      48 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14293735964714789207(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 32} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14293735964714789207(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14293735964714789207(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14293735964714789207(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14293735964714789207(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14293735964714789207(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14293735964714789207(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14293735964714789207);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14293735964714789207);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14293735964714789207);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6060582916895716642 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      113 /* Input2 */, \
      113 /* Input3 */, \
      16 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6060582916895716642(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 113} /* Input2 */, 
      {"input[3]", 113} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6060582916895716642(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6060582916895716642(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6060582916895716642(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6060582916895716642(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6060582916895716642(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6060582916895716642(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6060582916895716642);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6060582916895716642);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6060582916895716642);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__10664802252779045040 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      64 /* Input0 */, \
      64 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10664802252779045040(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10664802252779045040(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10664802252779045040(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10664802252779045040(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10664802252779045040(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10664802252779045040(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10664802252779045040(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10664802252779045040);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10664802252779045040);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10664802252779045040);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__17465149697176917692 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      48 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17465149697176917692(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 48} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17465149697176917692(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17465149697176917692(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17465149697176917692(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17465149697176917692(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17465149697176917692(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17465149697176917692(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17465149697176917692);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17465149697176917692);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17465149697176917692);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__14343970683553129929 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      16 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14343970683553129929(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14343970683553129929(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14343970683553129929(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14343970683553129929(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14343970683553129929(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14343970683553129929(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14343970683553129929(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14343970683553129929);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14343970683553129929);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14343970683553129929);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__9258509386417577603 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2147483648 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9258509386417577603(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2147483648} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__9258509386417577603(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9258509386417577603(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9258509386417577603(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9258509386417577603(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__9258509386417577603(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9258509386417577603(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__9258509386417577603);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9258509386417577603);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__9258509386417577603);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__7063077225780329122 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1280 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7063077225780329122(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1280} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7063077225780329122(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7063077225780329122(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7063077225780329122(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7063077225780329122(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7063077225780329122(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7063077225780329122(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7063077225780329122);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7063077225780329122);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7063077225780329122);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__17601959682292921301 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      96 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17601959682292921301(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17601959682292921301(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17601959682292921301(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17601959682292921301(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17601959682292921301(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17601959682292921301(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17601959682292921301(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17601959682292921301);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17601959682292921301);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17601959682292921301);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_DROPOUT_FWD__1442993235232086249 {

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__1442993235232086249(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


static void LAYER_CUDNN_DROPOUT_FWD_FLOAT16__1442993235232086249(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<__half, batchnorm_mode, is_training>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__1442993235232086249(state);
}

static void LAYER_CUDNN_DROPOUT_FWD_FLOAT32__1442993235232086249(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<float, batchnorm_mode, is_training>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__1442993235232086249(state);
}

static void LAYER_CUDNN_DROPOUT_FWD_TENSORCOREHALF__1442993235232086249(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<__half, batchnorm_mode, is_training>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__1442993235232086249(state);
}




BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT16__1442993235232086249);

BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT32__1442993235232086249);

BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_TENSORCOREHALF__1442993235232086249);

#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES
}

namespace LAYER_CUDNN_ACTIVATION_FWD__8894999483379210561 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      32 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8894999483379210561(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8894999483379210561(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8894999483379210561(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8894999483379210561(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8894999483379210561(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8894999483379210561(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8894999483379210561(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8894999483379210561);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8894999483379210561);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8894999483379210561);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__519235219889222913 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__519235219889222913(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__519235219889222913(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__519235219889222913(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__519235219889222913(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__519235219889222913(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__519235219889222913(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__519235219889222913(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__519235219889222913);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__519235219889222913);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__519235219889222913);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4923603254596275713 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4923603254596275713(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4923603254596275713(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4923603254596275713(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4923603254596275713(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4923603254596275713(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4923603254596275713(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4923603254596275713(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4923603254596275713);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4923603254596275713);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4923603254596275713);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__3523507296020240283 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3523507296020240283(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3523507296020240283(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3523507296020240283(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3523507296020240283(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3523507296020240283(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3523507296020240283(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3523507296020240283(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3523507296020240283);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3523507296020240283);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3523507296020240283);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__3284491421497725231 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      8 /* Input0 */, \
      32 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3284491421497725231(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 8} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3284491421497725231(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3284491421497725231(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3284491421497725231(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3284491421497725231(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3284491421497725231(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3284491421497725231(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3284491421497725231);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3284491421497725231);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3284491421497725231);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__16777737308428631934 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      256 /* Input0 */, \
      160 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16777737308428631934(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16777737308428631934(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16777737308428631934(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16777737308428631934(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16777737308428631934(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16777737308428631934(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16777737308428631934(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16777737308428631934);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16777737308428631934);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16777737308428631934);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__15481375148636093484 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1024 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      144 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15481375148636093484(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 144} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15481375148636093484(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15481375148636093484(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15481375148636093484(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15481375148636093484(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15481375148636093484(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15481375148636093484(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15481375148636093484);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15481375148636093484);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15481375148636093484);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11314371024559246582 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      8388608 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11314371024559246582(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 8388608} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11314371024559246582(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11314371024559246582(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11314371024559246582(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11314371024559246582(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11314371024559246582(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11314371024559246582(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11314371024559246582);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11314371024559246582);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11314371024559246582);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__17729703967917007150 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      192 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17729703967917007150(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 192} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17729703967917007150(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17729703967917007150(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17729703967917007150(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17729703967917007150(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17729703967917007150(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17729703967917007150(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17729703967917007150);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17729703967917007150);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17729703967917007150);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__15570870121400761564 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      128 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15570870121400761564(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15570870121400761564(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15570870121400761564(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15570870121400761564(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15570870121400761564(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__15570870121400761564(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15570870121400761564(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15570870121400761564);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15570870121400761564);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__15570870121400761564);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__12919349795400796019 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      12288 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12919349795400796019(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 12288} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12919349795400796019(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12919349795400796019(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12919349795400796019(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12919349795400796019(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12919349795400796019(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12919349795400796019(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12919349795400796019);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12919349795400796019);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12919349795400796019);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2083878996556236490 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1125899906842624 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2083878996556236490(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1125899906842624} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2083878996556236490(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2083878996556236490(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2083878996556236490(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2083878996556236490(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2083878996556236490(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2083878996556236490(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2083878996556236490);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2083878996556236490);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2083878996556236490);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_DROPOUT_FWD__11721117464311544917 {

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      4096 /* Input1 */, \
      -1 /* Input2 */, \
      -1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__11721117464311544917(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 4096} /* Input1 */, 
      {"input[2]", -1} /* Input2 */, 
      {"input[3]", -1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


static void LAYER_CUDNN_DROPOUT_FWD_FLOAT16__11721117464311544917(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<__half, batchnorm_mode, is_training>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__11721117464311544917(state);
}

static void LAYER_CUDNN_DROPOUT_FWD_FLOAT32__11721117464311544917(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<float, batchnorm_mode, is_training>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__11721117464311544917(state);
}

static void LAYER_CUDNN_DROPOUT_FWD_TENSORCOREHALF__11721117464311544917(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<__half, batchnorm_mode, is_training>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__11721117464311544917(state);
}




BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT16__11721117464311544917);

BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT32__11721117464311544917);

BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_TENSORCOREHALF__11721117464311544917);

#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__1486889881584011081 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      12288 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1486889881584011081(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 12288} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__1486889881584011081(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1486889881584011081(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1486889881584011081(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1486889881584011081(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__1486889881584011081(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1486889881584011081(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__1486889881584011081);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1486889881584011081);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__1486889881584011081);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__12961704932029173940 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      144 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      144 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12961704932029173940(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 144} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12961704932029173940(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12961704932029173940(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12961704932029173940(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12961704932029173940(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12961704932029173940(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12961704932029173940(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12961704932029173940);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12961704932029173940);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12961704932029173940);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__17476428975646754491 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      768 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17476428975646754491(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 768} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__17476428975646754491(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17476428975646754491(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17476428975646754491(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17476428975646754491(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__17476428975646754491(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17476428975646754491(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__17476428975646754491);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17476428975646754491);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__17476428975646754491);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__3880838630052801277 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3880838630052801277(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__3880838630052801277(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3880838630052801277(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3880838630052801277(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3880838630052801277(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__3880838630052801277(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3880838630052801277(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__3880838630052801277);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3880838630052801277);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__3880838630052801277);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__13105244484689054487 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13105244484689054487(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13105244484689054487(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13105244484689054487(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13105244484689054487(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13105244484689054487(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13105244484689054487(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13105244484689054487(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13105244484689054487);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13105244484689054487);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13105244484689054487);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4178688587727465505 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4178688587727465505(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4178688587727465505(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4178688587727465505(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4178688587727465505(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4178688587727465505(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4178688587727465505(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4178688587727465505(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4178688587727465505);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4178688587727465505);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4178688587727465505);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5381953008580894960 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      144 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      144 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5381953008580894960(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 144} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5381953008580894960(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5381953008580894960(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5381953008580894960(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5381953008580894960(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5381953008580894960(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5381953008580894960(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5381953008580894960);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5381953008580894960);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5381953008580894960);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__15475430425982850683 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      24 /* Input1 */, \
      113 /* Input2 */, \
      113 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__15475430425982850683(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 113} /* Input2 */, 
      {"input[3]", 113} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__15475430425982850683(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__15475430425982850683(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__15475430425982850683(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__15475430425982850683(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__15475430425982850683(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__15475430425982850683(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__15475430425982850683);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__15475430425982850683);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__15475430425982850683);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__9244299531816066388 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9244299531816066388(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9244299531816066388(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9244299531816066388(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9244299531816066388(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9244299531816066388(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9244299531816066388(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9244299531816066388(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9244299531816066388);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9244299531816066388);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9244299531816066388);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__11144054690993717989 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      12288 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11144054690993717989(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 12288} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11144054690993717989(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11144054690993717989(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11144054690993717989(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11144054690993717989(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11144054690993717989(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11144054690993717989(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11144054690993717989);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11144054690993717989);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11144054690993717989);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14873640375546212815 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14873640375546212815(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14873640375546212815(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14873640375546212815(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14873640375546212815(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14873640375546212815(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14873640375546212815(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14873640375546212815(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14873640375546212815);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14873640375546212815);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14873640375546212815);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__3854234356944245752 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      96 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      96 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3854234356944245752(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3854234356944245752(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3854234356944245752(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3854234356944245752(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3854234356944245752(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3854234356944245752(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3854234356944245752(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3854234356944245752);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3854234356944245752);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3854234356944245752);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__12760837051084705769 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__12760837051084705769(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__12760837051084705769(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__12760837051084705769(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__12760837051084705769(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__12760837051084705769(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__12760837051084705769(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__12760837051084705769(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__12760837051084705769);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__12760837051084705769);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__12760837051084705769);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__17564868877004159245 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      48 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17564868877004159245(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 48} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17564868877004159245(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17564868877004159245(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17564868877004159245(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17564868877004159245(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17564868877004159245(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17564868877004159245(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17564868877004159245);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17564868877004159245);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17564868877004159245);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__16067766790818557095 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16067766790818557095(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16067766790818557095(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16067766790818557095(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16067766790818557095(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16067766790818557095(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16067766790818557095(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16067766790818557095(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16067766790818557095);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16067766790818557095);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16067766790818557095);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__772041063381112436 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__772041063381112436(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__772041063381112436(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__772041063381112436(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__772041063381112436(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__772041063381112436(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__772041063381112436(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__772041063381112436(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__772041063381112436);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__772041063381112436);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__772041063381112436);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10414865547066468214 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      35184372088832 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10414865547066468214(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 35184372088832} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10414865547066468214(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10414865547066468214(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10414865547066468214(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10414865547066468214(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10414865547066468214(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10414865547066468214(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10414865547066468214);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10414865547066468214);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10414865547066468214);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__18189649383147767377 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18189649383147767377(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__18189649383147767377(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18189649383147767377(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18189649383147767377(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18189649383147767377(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18189649383147767377(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18189649383147767377(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__18189649383147767377);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18189649383147767377);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18189649383147767377);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__2937596702399506799 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      1024 /* Input1 */, \
      -1 /* Input2 */, \
      -1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2937596702399506799(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", -1} /* Input2 */, 
      {"input[3]", -1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2937596702399506799(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2937596702399506799(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2937596702399506799(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2937596702399506799(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2937596702399506799(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2937596702399506799(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2937596702399506799);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2937596702399506799);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2937596702399506799);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8675801029464927196 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8675801029464927196(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8675801029464927196(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8675801029464927196(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8675801029464927196(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8675801029464927196(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8675801029464927196(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8675801029464927196(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8675801029464927196);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8675801029464927196);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8675801029464927196);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__13549981734977722898 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1024 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      112 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13549981734977722898(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 112} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13549981734977722898(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13549981734977722898(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13549981734977722898(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13549981734977722898(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13549981734977722898(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13549981734977722898(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13549981734977722898);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13549981734977722898);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13549981734977722898);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__16242615457483965492 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1280 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16242615457483965492(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1280} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__16242615457483965492(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16242615457483965492(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16242615457483965492(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16242615457483965492(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__16242615457483965492(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16242615457483965492(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__16242615457483965492);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16242615457483965492);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__16242615457483965492);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__14458244032596799629 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      4194304 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14458244032596799629(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4194304} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14458244032596799629(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14458244032596799629(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14458244032596799629(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14458244032596799629(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14458244032596799629(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14458244032596799629(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14458244032596799629);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14458244032596799629);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14458244032596799629);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__323033451159353508 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__323033451159353508(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__323033451159353508(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__323033451159353508(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__323033451159353508(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__323033451159353508(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__323033451159353508(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__323033451159353508(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__323033451159353508);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__323033451159353508);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__323033451159353508);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__16659363625798041084 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      64 /* Input1 */, \
      104 /* Input2 */, \
      104 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16659363625798041084(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 104} /* Input2 */, 
      {"input[3]", 104} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__16659363625798041084(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16659363625798041084(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16659363625798041084(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16659363625798041084(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__16659363625798041084(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16659363625798041084(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__16659363625798041084);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16659363625798041084);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__16659363625798041084);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace CUDNN_POOLING_FWD__13770004410891166790 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      16 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__13770004410891166790(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__13770004410891166790(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__13770004410891166790(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__13770004410891166790(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__13770004410891166790(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__13770004410891166790(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__13770004410891166790(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__13770004410891166790);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__13770004410891166790);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__13770004410891166790);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12195223777201359961 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      272 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12195223777201359961(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 272} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12195223777201359961(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12195223777201359961(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12195223777201359961(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12195223777201359961(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__12195223777201359961(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12195223777201359961(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12195223777201359961);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12195223777201359961);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__12195223777201359961);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__11795932228439580972 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      147456 /* Input0 */, \
      352 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11795932228439580972(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 147456} /* Input0 */, 
      {"input[1]", 352} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11795932228439580972(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11795932228439580972(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11795932228439580972(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11795932228439580972(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11795932228439580972(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11795932228439580972(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11795932228439580972);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11795932228439580972);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11795932228439580972);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__273574743954549229 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      65536 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__273574743954549229(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__273574743954549229(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__273574743954549229(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__273574743954549229(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__273574743954549229(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__273574743954549229(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__273574743954549229(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__273574743954549229);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__273574743954549229);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__273574743954549229);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__1209137880701152534 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      16 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      32 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1209137880701152534(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1209137880701152534(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1209137880701152534(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1209137880701152534(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1209137880701152534(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1209137880701152534(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1209137880701152534(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1209137880701152534);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1209137880701152534);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1209137880701152534);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__12463793896951441729 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      3072 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__12463793896951441729(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 3072} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__12463793896951441729(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__12463793896951441729(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__12463793896951441729(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__12463793896951441729(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__12463793896951441729(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__12463793896951441729(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__12463793896951441729);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__12463793896951441729);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__12463793896951441729);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__6378307348110242525 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6378307348110242525(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__6378307348110242525(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6378307348110242525(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6378307348110242525(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6378307348110242525(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__6378307348110242525(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6378307348110242525(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__6378307348110242525);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6378307348110242525);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__6378307348110242525);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__15566293130197616248 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15566293130197616248(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15566293130197616248(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15566293130197616248(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15566293130197616248(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15566293130197616248(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15566293130197616248(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15566293130197616248(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15566293130197616248);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15566293130197616248);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15566293130197616248);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__8079708691494638307 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      192 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8079708691494638307(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8079708691494638307(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8079708691494638307(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8079708691494638307(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8079708691494638307(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8079708691494638307(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8079708691494638307(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8079708691494638307);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8079708691494638307);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8079708691494638307);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10325219894246243566 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10325219894246243566(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10325219894246243566(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10325219894246243566(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10325219894246243566(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10325219894246243566(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10325219894246243566(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10325219894246243566(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10325219894246243566);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10325219894246243566);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10325219894246243566);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__15660042212839170814 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      16384 /* Input0 */, \
      160 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      320 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15660042212839170814(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 320} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15660042212839170814(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15660042212839170814(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15660042212839170814(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15660042212839170814(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15660042212839170814(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15660042212839170814(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15660042212839170814);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15660042212839170814);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15660042212839170814);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10769150322559465585 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10769150322559465585(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10769150322559465585(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10769150322559465585(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10769150322559465585(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10769150322559465585(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10769150322559465585(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10769150322559465585(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10769150322559465585);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10769150322559465585);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10769150322559465585);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__1158743023656342204 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      256 /* Input0 */, \
      24 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      64 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1158743023656342204(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1158743023656342204(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1158743023656342204(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1158743023656342204(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1158743023656342204(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1158743023656342204(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1158743023656342204(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1158743023656342204);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1158743023656342204);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1158743023656342204);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__10686040071813224539 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      64 /* Input0 */, \
      192 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__10686040071813224539(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__10686040071813224539(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__10686040071813224539(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__10686040071813224539(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__10686040071813224539(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__10686040071813224539(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__10686040071813224539(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__10686040071813224539);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__10686040071813224539);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__10686040071813224539);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__5617784253318548066 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      281474976710656 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5617784253318548066(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 281474976710656} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5617784253318548066(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5617784253318548066(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5617784253318548066(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5617784253318548066(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5617784253318548066(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5617784253318548066(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5617784253318548066);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5617784253318548066);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5617784253318548066);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__10611408697060753379 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      256 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10611408697060753379(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10611408697060753379(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10611408697060753379(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10611408697060753379(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10611408697060753379(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10611408697060753379(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10611408697060753379(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10611408697060753379);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10611408697060753379);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10611408697060753379);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__1460637636623546236 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1460637636623546236(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1460637636623546236(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1460637636623546236(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1460637636623546236(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1460637636623546236(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1460637636623546236(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1460637636623546236(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1460637636623546236);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1460637636623546236);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1460637636623546236);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2682917798289634125 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      16 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2682917798289634125(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2682917798289634125(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2682917798289634125(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2682917798289634125(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2682917798289634125(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2682917798289634125(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2682917798289634125(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2682917798289634125);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2682917798289634125);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2682917798289634125);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__12213221853060288561 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12213221853060288561(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12213221853060288561(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12213221853060288561(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12213221853060288561(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12213221853060288561(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12213221853060288561(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12213221853060288561(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12213221853060288561);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12213221853060288561);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12213221853060288561);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__10309715824978311078 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      2048 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__10309715824978311078(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__10309715824978311078(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__10309715824978311078(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__10309715824978311078(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__10309715824978311078(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__10309715824978311078(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__10309715824978311078(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__10309715824978311078);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__10309715824978311078);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__10309715824978311078);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace CUDNN_POOLING_FWD__3215367446274285998 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      262144 /* Input0 */, \
      384 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__3215367446274285998(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 262144} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__3215367446274285998(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__3215367446274285998(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__3215367446274285998(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__3215367446274285998(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__3215367446274285998(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__3215367446274285998(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__3215367446274285998);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__3215367446274285998);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__3215367446274285998);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_DROPOUT_FWD__302648067509719457 {

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__302648067509719457(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


static void LAYER_CUDNN_DROPOUT_FWD_FLOAT16__302648067509719457(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<__half, batchnorm_mode, is_training>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__302648067509719457(state);
}

static void LAYER_CUDNN_DROPOUT_FWD_FLOAT32__302648067509719457(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<float, batchnorm_mode, is_training>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__302648067509719457(state);
}

static void LAYER_CUDNN_DROPOUT_FWD_TENSORCOREHALF__302648067509719457(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<__half, batchnorm_mode, is_training>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__302648067509719457(state);
}




BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT16__302648067509719457);

BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT32__302648067509719457);

BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_TENSORCOREHALF__302648067509719457);

#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES
}

namespace LAYER_CUDNN_CONV_FWD__12077771894585075 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      65536 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12077771894585075(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12077771894585075(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12077771894585075(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12077771894585075(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12077771894585075(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12077771894585075(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12077771894585075(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12077771894585075);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12077771894585075);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12077771894585075);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6525672358251397502 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      256 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6525672358251397502(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6525672358251397502(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6525672358251397502(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6525672358251397502(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6525672358251397502(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6525672358251397502(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6525672358251397502(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6525672358251397502);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6525672358251397502);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6525672358251397502);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__4916817844940158353 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      200 /* Input2 */, \
      200 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4916817844940158353(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 200} /* Input2 */, 
      {"input[3]", 200} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__4916817844940158353(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4916817844940158353(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4916817844940158353(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4916817844940158353(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__4916817844940158353(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4916817844940158353(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__4916817844940158353);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4916817844940158353);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__4916817844940158353);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__3141933005227993501 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      144 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3141933005227993501(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3141933005227993501(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3141933005227993501(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3141933005227993501(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3141933005227993501(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3141933005227993501(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3141933005227993501(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3141933005227993501);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3141933005227993501);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3141933005227993501);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__7140100715684815733 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      128 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__7140100715684815733(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__7140100715684815733(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__7140100715684815733(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__7140100715684815733(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__7140100715684815733(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__7140100715684815733(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__7140100715684815733(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__7140100715684815733);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__7140100715684815733);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__7140100715684815733);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__18126899082542985677 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      65536 /* Input0 */, \
      128 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18126899082542985677(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18126899082542985677(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18126899082542985677(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18126899082542985677(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18126899082542985677(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__18126899082542985677(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18126899082542985677(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18126899082542985677);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18126899082542985677);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__18126899082542985677);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__8668363251223716825 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      128 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8668363251223716825(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8668363251223716825(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8668363251223716825(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8668363251223716825(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8668363251223716825(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8668363251223716825(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8668363251223716825(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8668363251223716825);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8668363251223716825);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8668363251223716825);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__16775781729113543974 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16775781729113543974(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16775781729113543974(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16775781729113543974(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16775781729113543974(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16775781729113543974(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16775781729113543974(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16775781729113543974(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16775781729113543974);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16775781729113543974);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16775781729113543974);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__15952645869473475879 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      16384 /* Input0 */, \
      128 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15952645869473475879(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15952645869473475879(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15952645869473475879(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15952645869473475879(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15952645869473475879(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15952645869473475879(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15952645869473475879(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15952645869473475879);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15952645869473475879);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15952645869473475879);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12698614221717716460 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      13 /* Input2 */, \
      14 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12698614221717716460(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12698614221717716460(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12698614221717716460(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12698614221717716460(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12698614221717716460(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12698614221717716460(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12698614221717716460(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12698614221717716460);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12698614221717716460);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12698614221717716460);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__2088178385280461989 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      256 /* Input0 */, \
      64 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2088178385280461989(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2088178385280461989(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2088178385280461989(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2088178385280461989(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2088178385280461989(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2088178385280461989(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2088178385280461989(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2088178385280461989);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2088178385280461989);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2088178385280461989);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_SOFTMAX_FWD__9950488291444976510 {

#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARGS() \
  Args({{ \
      256 /* Input0 */, \
      1000 /* Input1 */, \
      1 /* Input2 */, \
      1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_ADD_COUNTERS__9950488291444976510(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 1000} /* Input1 */, 
      {"input[2]", 1} /* Input2 */, 
      {"input[3]", 1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnSoftmaxAlgorithm_t softmax_algorithm, cudnnSoftmaxMode_t softmax_mode>
static void LAYER_CUDNN_SOFTMAX_FWD_FLOAT16__9950488291444976510(benchmark::State& state) {
  LAYER_CUDNN_SOFTMAX_FWD_Impl<__half, softmax_algorithm, softmax_mode>(state);
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_ADD_COUNTERS__9950488291444976510(state);
}

template <cudnnSoftmaxAlgorithm_t softmax_algorithm, cudnnSoftmaxMode_t softmax_mode>
static void LAYER_CUDNN_SOFTMAX_FWD_FLOAT32__9950488291444976510(benchmark::State& state) {
  LAYER_CUDNN_SOFTMAX_FWD_Impl<float, softmax_algorithm, softmax_mode>(state);
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_ADD_COUNTERS__9950488291444976510(state);
}

template <cudnnSoftmaxAlgorithm_t softmax_algorithm, cudnnSoftmaxMode_t softmax_mode>
static void LAYER_CUDNN_SOFTMAX_FWD_TENSORCOREHALF__9950488291444976510(benchmark::State& state) {
  LAYER_CUDNN_SOFTMAX_FWD_Impl<__half, softmax_algorithm, softmax_mode>(state);
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_ADD_COUNTERS__9950488291444976510(state);
}

#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, SOFTMAX_MODE) \
 BENCHMARK_TEMPLATE(b, CUDNN_SOFTMAX_MODE_INSTANCE, SOFTMAX_MODE)->BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_SOFTMAX_MODE_CHANNEL, SOFTMAX_MODE)->BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD(b)                                                                                             \
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, CUDNN_SOFTMAX_MODE_INSTANCE);                                                                    \
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, CUDNN_SOFTMAX_MODE_CHANNEL)

BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD(LAYER_CUDNN_SOFTMAX_FWD_FLOAT16__9950488291444976510);
BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD(LAYER_CUDNN_SOFTMAX_FWD_FLOAT32__9950488291444976510);
BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD(LAYER_CUDNN_SOFTMAX_FWD_TENSORCOREHALF__9950488291444976510);

#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARGS
$undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0
#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__3427783105963289306 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1024 /* Input0 */, \
      64 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3427783105963289306(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3427783105963289306(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3427783105963289306(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3427783105963289306(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3427783105963289306(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3427783105963289306(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3427783105963289306(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3427783105963289306);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3427783105963289306);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3427783105963289306);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__7232140058932872042 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      64 /* Input1 */, \
      104 /* Input2 */, \
      104 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7232140058932872042(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 104} /* Input2 */, 
      {"input[3]", 104} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7232140058932872042(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7232140058932872042(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7232140058932872042(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7232140058932872042(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7232140058932872042(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7232140058932872042(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7232140058932872042);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7232140058932872042);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7232140058932872042);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12673431412319924031 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      96 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      208 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12673431412319924031(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 208} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12673431412319924031(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12673431412319924031(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12673431412319924031(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12673431412319924031(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12673431412319924031(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12673431412319924031(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12673431412319924031);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12673431412319924031);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12673431412319924031);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__16310229925127897088 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16310229925127897088(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16310229925127897088(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16310229925127897088(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16310229925127897088(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16310229925127897088(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16310229925127897088(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16310229925127897088(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16310229925127897088);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16310229925127897088);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16310229925127897088);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__9089984983275446371 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9089984983275446371(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9089984983275446371(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9089984983275446371(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9089984983275446371(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9089984983275446371(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9089984983275446371(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9089984983275446371(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9089984983275446371);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9089984983275446371);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9089984983275446371);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__1582817266075738534 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__1582817266075738534(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__1582817266075738534(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__1582817266075738534(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__1582817266075738534(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__1582817266075738534(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__1582817266075738534(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__1582817266075738534(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__1582817266075738534);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__1582817266075738534);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__1582817266075738534);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__3531658906111798901 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      128 /* Input1 */, \
      52 /* Input2 */, \
      52 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3531658906111798901(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 52} /* Input2 */, 
      {"input[3]", 52} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3531658906111798901(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3531658906111798901(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3531658906111798901(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3531658906111798901(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3531658906111798901(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3531658906111798901(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3531658906111798901);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3531658906111798901);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3531658906111798901);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4669294775958113474 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      65536 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      384 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4669294775958113474(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4669294775958113474(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4669294775958113474(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4669294775958113474(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4669294775958113474(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4669294775958113474(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4669294775958113474(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4669294775958113474);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4669294775958113474);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4669294775958113474);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__9105095390211712792 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      9 /* PadHeight */, \
      9 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      9 /* DilationWidth */, \
      9 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9105095390211712792(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 9} /* PadHeight */, 
      {"pad_width", 9} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 9} /* DilationWidth */, 
      {"dilation_width", 9} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9105095390211712792(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9105095390211712792(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9105095390211712792(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9105095390211712792(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9105095390211712792(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9105095390211712792(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9105095390211712792);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9105095390211712792);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9105095390211712792);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5491292824644893300 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      16 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      48 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5491292824644893300(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5491292824644893300(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5491292824644893300(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5491292824644893300(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5491292824644893300(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5491292824644893300(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5491292824644893300(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5491292824644893300);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5491292824644893300);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5491292824644893300);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__7481702161992970198 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7481702161992970198(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7481702161992970198(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7481702161992970198(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7481702161992970198(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7481702161992970198(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7481702161992970198(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7481702161992970198(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7481702161992970198);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7481702161992970198);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7481702161992970198);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9687680867035498859 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      144 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9687680867035498859(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9687680867035498859(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9687680867035498859(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9687680867035498859(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9687680867035498859(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9687680867035498859(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9687680867035498859(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9687680867035498859);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9687680867035498859);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9687680867035498859);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__10964807029248072129 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      256 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10964807029248072129(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10964807029248072129(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10964807029248072129(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10964807029248072129(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10964807029248072129(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10964807029248072129(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10964807029248072129(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10964807029248072129);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10964807029248072129);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10964807029248072129);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__17364363547411602481 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17364363547411602481(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17364363547411602481(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17364363547411602481(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17364363547411602481(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17364363547411602481(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17364363547411602481(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17364363547411602481(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17364363547411602481);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17364363547411602481);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17364363547411602481);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__16304555172727641619 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      64 /* Input0 */, \
      64 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16304555172727641619(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16304555172727641619(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16304555172727641619(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16304555172727641619(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16304555172727641619(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16304555172727641619(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16304555172727641619(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16304555172727641619);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16304555172727641619);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16304555172727641619);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2959315207773715624 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      200 /* Input2 */, \
      200 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2959315207773715624(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 200} /* Input2 */, 
      {"input[3]", 200} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2959315207773715624(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2959315207773715624(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2959315207773715624(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2959315207773715624(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2959315207773715624(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2959315207773715624(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2959315207773715624);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2959315207773715624);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2959315207773715624);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__9033429619432281650 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__9033429619432281650(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__9033429619432281650(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__9033429619432281650(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__9033429619432281650(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__9033429619432281650(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__9033429619432281650(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__9033429619432281650(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__9033429619432281650);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__9033429619432281650);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__9033429619432281650);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__1072308382308033959 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      8 /* Input0 */, \
      128 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1072308382308033959(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 8} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1072308382308033959(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1072308382308033959(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1072308382308033959(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1072308382308033959(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1072308382308033959(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1072308382308033959(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1072308382308033959);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1072308382308033959);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1072308382308033959);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4200461661441999254 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4200461661441999254(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4200461661441999254(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4200461661441999254(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4200461661441999254(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4200461661441999254(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4200461661441999254(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4200461661441999254(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4200461661441999254);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4200461661441999254);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4200461661441999254);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11031039547586859208 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      2 /* DilationWidth */, \
      2 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11031039547586859208(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 2} /* DilationWidth */, 
      {"dilation_width", 2} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11031039547586859208(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11031039547586859208(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11031039547586859208(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11031039547586859208(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11031039547586859208(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11031039547586859208(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11031039547586859208);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11031039547586859208);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11031039547586859208);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__17433144035549251921 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      64 /* Input0 */, \
      112 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      224 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17433144035549251921(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 224} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17433144035549251921(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17433144035549251921(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17433144035549251921(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17433144035549251921(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17433144035549251921(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17433144035549251921(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17433144035549251921);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17433144035549251921);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17433144035549251921);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6023261935327604114 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      3 /* Input1 */, \
      416 /* Input2 */, \
      416 /* Input3 */, \
      16 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6023261935327604114(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 416} /* Input2 */, 
      {"input[3]", 416} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6023261935327604114(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6023261935327604114(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6023261935327604114(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6023261935327604114(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6023261935327604114(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6023261935327604114(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6023261935327604114);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6023261935327604114);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6023261935327604114);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__5103605278155917830 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      72057594037927936 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5103605278155917830(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 72057594037927936} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5103605278155917830(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5103605278155917830(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5103605278155917830(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5103605278155917830(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5103605278155917830(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5103605278155917830(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5103605278155917830);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5103605278155917830);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5103605278155917830);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2305774891387058886 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      65536 /* Input0 */, \
      48 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      128 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2305774891387058886(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2305774891387058886(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2305774891387058886(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2305774891387058886(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2305774891387058886(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2305774891387058886(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2305774891387058886(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2305774891387058886);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2305774891387058886);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2305774891387058886);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__2353621928123688812 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      768 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2353621928123688812(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 768} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2353621928123688812(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2353621928123688812(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2353621928123688812(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2353621928123688812(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2353621928123688812(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2353621928123688812(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2353621928123688812);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2353621928123688812);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2353621928123688812);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__17042629839764539552 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      16 /* Input1 */, \
      416 /* Input2 */, \
      416 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17042629839764539552(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 416} /* Input2 */, 
      {"input[3]", 416} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__17042629839764539552(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17042629839764539552(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17042629839764539552(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17042629839764539552(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__17042629839764539552(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17042629839764539552(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__17042629839764539552);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17042629839764539552);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__17042629839764539552);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__3563758548250716408 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3563758548250716408(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3563758548250716408(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3563758548250716408(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3563758548250716408(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3563758548250716408(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3563758548250716408(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3563758548250716408(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3563758548250716408);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3563758548250716408);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3563758548250716408);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11523459527500293052 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      256 /* Input0 */, \
      24 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      64 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11523459527500293052(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11523459527500293052(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11523459527500293052(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11523459527500293052(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11523459527500293052(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11523459527500293052(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11523459527500293052(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11523459527500293052);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11523459527500293052);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11523459527500293052);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__18359540421217132727 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      128 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18359540421217132727(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__18359540421217132727(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18359540421217132727(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18359540421217132727(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18359540421217132727(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18359540421217132727(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18359540421217132727(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__18359540421217132727);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18359540421217132727);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18359540421217132727);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__17778890188330694472 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      512 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17778890188330694472(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17778890188330694472(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17778890188330694472(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17778890188330694472(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17778890188330694472(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17778890188330694472(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17778890188330694472(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17778890188330694472);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17778890188330694472);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17778890188330694472);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10938535623703072432 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10938535623703072432(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10938535623703072432(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10938535623703072432(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10938535623703072432(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10938535623703072432(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10938535623703072432(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10938535623703072432(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10938535623703072432);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10938535623703072432);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10938535623703072432);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__13429663731860826977 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13429663731860826977(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13429663731860826977(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13429663731860826977(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13429663731860826977(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13429663731860826977(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13429663731860826977(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13429663731860826977(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13429663731860826977);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13429663731860826977);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13429663731860826977);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__10312489190873624574 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      112 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__10312489190873624574(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__10312489190873624574(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__10312489190873624574(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__10312489190873624574(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__10312489190873624574(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__10312489190873624574(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__10312489190873624574(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__10312489190873624574);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__10312489190873624574);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__10312489190873624574);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__16414942702135382240 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1024 /* Input0 */, \
      288 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16414942702135382240(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 288} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16414942702135382240(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16414942702135382240(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16414942702135382240(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16414942702135382240(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16414942702135382240(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16414942702135382240(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16414942702135382240);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16414942702135382240);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16414942702135382240);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__1137431874847376232 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      65536 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__1137431874847376232(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__1137431874847376232(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__1137431874847376232(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__1137431874847376232(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__1137431874847376232(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__1137431874847376232(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__1137431874847376232(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__1137431874847376232);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__1137431874847376232);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__1137431874847376232);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__4831716919749773337 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      208 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4831716919749773337(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 208} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4831716919749773337(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4831716919749773337(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4831716919749773337(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4831716919749773337(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4831716919749773337(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4831716919749773337(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4831716919749773337);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4831716919749773337);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4831716919749773337);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__2541525115050742197 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      16 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2541525115050742197(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2541525115050742197(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2541525115050742197(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2541525115050742197(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2541525115050742197(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2541525115050742197(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2541525115050742197(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2541525115050742197);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2541525115050742197);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2541525115050742197);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__15839582640988889432 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      960 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      960 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15839582640988889432(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 960} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 960} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15839582640988889432(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15839582640988889432(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15839582640988889432(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15839582640988889432(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15839582640988889432(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15839582640988889432(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15839582640988889432);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15839582640988889432);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15839582640988889432);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11206028001459029239 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      512 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      1024 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11206028001459029239(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11206028001459029239(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11206028001459029239(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11206028001459029239(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11206028001459029239(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11206028001459029239(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11206028001459029239(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11206028001459029239);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11206028001459029239);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11206028001459029239);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__16715485664703554457 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      1024 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16715485664703554457(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16715485664703554457(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16715485664703554457(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16715485664703554457(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16715485664703554457(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16715485664703554457(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16715485664703554457(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16715485664703554457);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16715485664703554457);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16715485664703554457);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__8749833570355640134 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      16384 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8749833570355640134(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8749833570355640134(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8749833570355640134(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8749833570355640134(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8749833570355640134(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8749833570355640134(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8749833570355640134(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8749833570355640134);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8749833570355640134);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8749833570355640134);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2866173528408703340 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2866173528408703340(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2866173528408703340(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2866173528408703340(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2866173528408703340(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2866173528408703340(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2866173528408703340(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2866173528408703340(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2866173528408703340);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2866173528408703340);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2866173528408703340);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__5903470542815116867 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      262144 /* Input0 */, \
      384 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__5903470542815116867(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 262144} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__5903470542815116867(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__5903470542815116867(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__5903470542815116867(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__5903470542815116867(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__5903470542815116867(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__5903470542815116867(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__5903470542815116867);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__5903470542815116867);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__5903470542815116867);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__17014728314582939701 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      200 /* Input2 */, \
      200 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17014728314582939701(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 200} /* Input2 */, 
      {"input[3]", 200} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17014728314582939701(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17014728314582939701(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17014728314582939701(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17014728314582939701(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17014728314582939701(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17014728314582939701(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17014728314582939701);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17014728314582939701);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17014728314582939701);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5721244730378805412 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5721244730378805412(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5721244730378805412(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5721244730378805412(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5721244730378805412(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5721244730378805412(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5721244730378805412(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5721244730378805412(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5721244730378805412);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5721244730378805412);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5721244730378805412);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__5594593533194974782 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1024 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5594593533194974782(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5594593533194974782(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5594593533194974782(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5594593533194974782(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5594593533194974782(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5594593533194974782(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5594593533194974782(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5594593533194974782);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5594593533194974782);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5594593533194974782);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__4053264191753648895 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1024 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__4053264191753648895(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__4053264191753648895(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__4053264191753648895(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__4053264191753648895(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__4053264191753648895(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__4053264191753648895(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__4053264191753648895(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__4053264191753648895);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__4053264191753648895);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__4053264191753648895);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__10938739485212441055 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      12288 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10938739485212441055(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 12288} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10938739485212441055(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10938739485212441055(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10938739485212441055(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10938739485212441055(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10938739485212441055(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10938739485212441055(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10938739485212441055);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10938739485212441055);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10938739485212441055);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__16536116508441460591 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      16384 /* Input0 */, \
      160 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      320 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16536116508441460591(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 320} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16536116508441460591(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16536116508441460591(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16536116508441460591(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16536116508441460591(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16536116508441460591(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16536116508441460591(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16536116508441460591);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16536116508441460591);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16536116508441460591);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__8585923035246921398 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      512 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__8585923035246921398(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__8585923035246921398(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__8585923035246921398(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__8585923035246921398(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__8585923035246921398(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__8585923035246921398(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__8585923035246921398(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__8585923035246921398);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__8585923035246921398);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__8585923035246921398);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4317637952032227621 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      16384 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4317637952032227621(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4317637952032227621(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4317637952032227621(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4317637952032227621(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4317637952032227621(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4317637952032227621(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4317637952032227621(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4317637952032227621);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4317637952032227621);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4317637952032227621);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14943752604183265983 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      16 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14943752604183265983(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14943752604183265983(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14943752604183265983(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14943752604183265983(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14943752604183265983(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14943752604183265983(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14943752604183265983(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14943752604183265983);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14943752604183265983);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14943752604183265983);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__9084283697223661649 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      64 /* Input0 */, \
      112 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      224 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9084283697223661649(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 224} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9084283697223661649(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9084283697223661649(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9084283697223661649(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9084283697223661649(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9084283697223661649(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9084283697223661649(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9084283697223661649);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9084283697223661649);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9084283697223661649);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__9975130275254721858 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      2048 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9975130275254721858(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9975130275254721858(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9975130275254721858(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9975130275254721858(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9975130275254721858(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9975130275254721858(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9975130275254721858(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9975130275254721858);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9975130275254721858);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9975130275254721858);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__10736388641452512131 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__10736388641452512131(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__10736388641452512131(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__10736388641452512131(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__10736388641452512131(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__10736388641452512131(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__10736388641452512131(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__10736388641452512131(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__10736388641452512131);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__10736388641452512131);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__10736388641452512131);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__3966603363610566520 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      32 /* Input1 */, \
      104 /* Input2 */, \
      104 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3966603363610566520(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 104} /* Input2 */, 
      {"input[3]", 104} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3966603363610566520(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3966603363610566520(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3966603363610566520(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3966603363610566520(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3966603363610566520(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3966603363610566520(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3966603363610566520);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3966603363610566520);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3966603363610566520);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__6937189938679917421 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      4398046511104 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6937189938679917421(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4398046511104} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6937189938679917421(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6937189938679917421(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6937189938679917421(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6937189938679917421(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6937189938679917421(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6937189938679917421(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6937189938679917421);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6937189938679917421);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6937189938679917421);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__12466907444566872144 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      262144 /* Input0 */, \
      384 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12466907444566872144(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 262144} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12466907444566872144(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12466907444566872144(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12466907444566872144(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12466907444566872144(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12466907444566872144(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12466907444566872144(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12466907444566872144);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12466907444566872144);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12466907444566872144);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__12720953924397298726 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      16384 /* Input0 */, \
      160 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12720953924397298726(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12720953924397298726(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12720953924397298726(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12720953924397298726(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12720953924397298726(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12720953924397298726(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12720953924397298726(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12720953924397298726);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12720953924397298726);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12720953924397298726);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__5938400389509404896 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      147456 /* Input0 */, \
      320 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5938400389509404896(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 147456} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__5938400389509404896(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5938400389509404896(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5938400389509404896(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5938400389509404896(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__5938400389509404896(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5938400389509404896(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__5938400389509404896);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5938400389509404896);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__5938400389509404896);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__14863044695081904709 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      32 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      128 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14863044695081904709(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14863044695081904709(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14863044695081904709(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14863044695081904709(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14863044695081904709(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14863044695081904709(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14863044695081904709(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14863044695081904709);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14863044695081904709);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14863044695081904709);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__15860485702322528452 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      16 /* Input1 */, \
      113 /* Input2 */, \
      113 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15860485702322528452(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 113} /* Input2 */, 
      {"input[3]", 113} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15860485702322528452(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15860485702322528452(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15860485702322528452(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15860485702322528452(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__15860485702322528452(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15860485702322528452(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15860485702322528452);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15860485702322528452);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__15860485702322528452);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12283401426526458325 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      200 /* Input2 */, \
      200 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12283401426526458325(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 200} /* Input2 */, 
      {"input[3]", 200} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12283401426526458325(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12283401426526458325(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12283401426526458325(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12283401426526458325(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__12283401426526458325(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12283401426526458325(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12283401426526458325);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12283401426526458325);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__12283401426526458325);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__2210264934446436477 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2210264934446436477(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2210264934446436477(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2210264934446436477(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2210264934446436477(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2210264934446436477(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2210264934446436477(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2210264934446436477(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2210264934446436477);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2210264934446436477);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2210264934446436477);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__7803828526831344196 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      384 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7803828526831344196(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7803828526831344196(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7803828526831344196(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7803828526831344196(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7803828526831344196(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7803828526831344196(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7803828526831344196(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7803828526831344196);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7803828526831344196);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7803828526831344196);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__8590893722642714799 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      32 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8590893722642714799(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8590893722642714799(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8590893722642714799(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8590893722642714799(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8590893722642714799(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8590893722642714799(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8590893722642714799(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8590893722642714799);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8590893722642714799);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8590893722642714799);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5805644535422787395 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1024 /* Input0 */, \
      144 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      288 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5805644535422787395(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 288} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5805644535422787395(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5805644535422787395(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5805644535422787395(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5805644535422787395(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5805644535422787395(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5805644535422787395(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5805644535422787395);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5805644535422787395);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5805644535422787395);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__150914675167651632 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      200 /* Input2 */, \
      200 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__150914675167651632(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 200} /* Input2 */, 
      {"input[3]", 200} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__150914675167651632(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__150914675167651632(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__150914675167651632(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__150914675167651632(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__150914675167651632(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__150914675167651632(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__150914675167651632);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__150914675167651632);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__150914675167651632);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__15290727842463537427 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      24 /* Input1 */, \
      113 /* Input2 */, \
      113 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15290727842463537427(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 113} /* Input2 */, 
      {"input[3]", 113} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15290727842463537427(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15290727842463537427(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15290727842463537427(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15290727842463537427(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__15290727842463537427(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15290727842463537427(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15290727842463537427);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15290727842463537427);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__15290727842463537427);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__17773533052379170252 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      544 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17773533052379170252(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 544} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__17773533052379170252(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17773533052379170252(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17773533052379170252(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17773533052379170252(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__17773533052379170252(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17773533052379170252(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__17773533052379170252);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17773533052379170252);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__17773533052379170252);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace CUDNN_POOLING_FWD__7403110082570552941 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      24 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__7403110082570552941(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__7403110082570552941(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__7403110082570552941(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__7403110082570552941(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__7403110082570552941(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__7403110082570552941(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__7403110082570552941(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__7403110082570552941);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__7403110082570552941);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__7403110082570552941);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9868615284022875602 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      112 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9868615284022875602(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9868615284022875602(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9868615284022875602(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9868615284022875602(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9868615284022875602(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9868615284022875602(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9868615284022875602(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9868615284022875602);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9868615284022875602);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9868615284022875602);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11351337989592328751 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1024 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11351337989592328751(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11351337989592328751(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11351337989592328751(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11351337989592328751(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11351337989592328751(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11351337989592328751(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11351337989592328751(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11351337989592328751);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11351337989592328751);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11351337989592328751);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__4620129006792747129 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      401 /* Input2 */, \
      401 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__4620129006792747129(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 401} /* Input2 */, 
      {"input[3]", 401} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__4620129006792747129(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__4620129006792747129(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__4620129006792747129(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__4620129006792747129(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__4620129006792747129(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__4620129006792747129(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__4620129006792747129);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__4620129006792747129);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__4620129006792747129);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8884533835296886597 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8884533835296886597(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8884533835296886597(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8884533835296886597(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8884533835296886597(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8884533835296886597(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8884533835296886597(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8884533835296886597(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8884533835296886597);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8884533835296886597);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8884533835296886597);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6815706674681204708 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      16 /* Input1 */, \
      208 /* Input2 */, \
      208 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6815706674681204708(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 208} /* Input2 */, 
      {"input[3]", 208} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6815706674681204708(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6815706674681204708(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6815706674681204708(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6815706674681204708(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6815706674681204708(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6815706674681204708(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6815706674681204708);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6815706674681204708);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6815706674681204708);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14511335993161070392 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1024 /* Input0 */, \
      32 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      64 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14511335993161070392(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14511335993161070392(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14511335993161070392(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14511335993161070392(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14511335993161070392(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14511335993161070392(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14511335993161070392(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14511335993161070392);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14511335993161070392);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14511335993161070392);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__716677539661403596 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__716677539661403596(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__716677539661403596(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__716677539661403596(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__716677539661403596(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__716677539661403596(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__716677539661403596(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__716677539661403596(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__716677539661403596);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__716677539661403596);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__716677539661403596);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__10343415854128958781 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      128 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10343415854128958781(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 128} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__10343415854128958781(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10343415854128958781(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10343415854128958781(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10343415854128958781(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__10343415854128958781(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10343415854128958781(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__10343415854128958781);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10343415854128958781);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__10343415854128958781);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__5483414137840417464 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      3 /* Input1 */, \
      800 /* Input2 */, \
      800 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5483414137840417464(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 800} /* Input2 */, 
      {"input[3]", 800} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5483414137840417464(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5483414137840417464(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5483414137840417464(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5483414137840417464(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5483414137840417464(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5483414137840417464(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5483414137840417464);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5483414137840417464);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5483414137840417464);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__17938324032856884920 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      16384 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__17938324032856884920(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__17938324032856884920(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__17938324032856884920(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__17938324032856884920(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__17938324032856884920(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__17938324032856884920(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__17938324032856884920(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__17938324032856884920);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__17938324032856884920);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__17938324032856884920);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__15019868433349496765 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      262144 /* Input0 */, \
      384 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15019868433349496765(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 262144} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15019868433349496765(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15019868433349496765(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15019868433349496765(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15019868433349496765(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15019868433349496765(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15019868433349496765(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15019868433349496765);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15019868433349496765);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15019868433349496765);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11032441647660024617 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      272 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      544 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11032441647660024617(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 272} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 544} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11032441647660024617(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11032441647660024617(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11032441647660024617(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11032441647660024617(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11032441647660024617(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11032441647660024617(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11032441647660024617);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11032441647660024617);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11032441647660024617);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__594681294367419292 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__594681294367419292(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__594681294367419292(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__594681294367419292(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__594681294367419292(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__594681294367419292(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__594681294367419292(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__594681294367419292(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__594681294367419292);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__594681294367419292);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__594681294367419292);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__17369714861426319756 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17369714861426319756(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17369714861426319756(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17369714861426319756(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17369714861426319756(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17369714861426319756(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17369714861426319756(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17369714861426319756(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17369714861426319756);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17369714861426319756);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17369714861426319756);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__9585747137689001534 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      200 /* Input2 */, \
      200 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9585747137689001534(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 200} /* Input2 */, 
      {"input[3]", 200} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9585747137689001534(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9585747137689001534(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9585747137689001534(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9585747137689001534(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9585747137689001534(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9585747137689001534(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9585747137689001534);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9585747137689001534);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9585747137689001534);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__7614576295550957963 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      65536 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      48 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7614576295550957963(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7614576295550957963(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7614576295550957963(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7614576295550957963(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7614576295550957963(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7614576295550957963(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7614576295550957963(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7614576295550957963);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7614576295550957963);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7614576295550957963);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__13365881129967536835 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13365881129967536835(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13365881129967536835(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13365881129967536835(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13365881129967536835(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13365881129967536835(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13365881129967536835(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13365881129967536835(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13365881129967536835);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13365881129967536835);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13365881129967536835);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__5416194765340855819 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      16384 /* Input0 */, \
      160 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5416194765340855819(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5416194765340855819(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5416194765340855819(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5416194765340855819(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5416194765340855819(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5416194765340855819(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5416194765340855819(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5416194765340855819);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5416194765340855819);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5416194765340855819);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5737867726387744355 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      256 /* Input0 */, \
      160 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5737867726387744355(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5737867726387744355(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5737867726387744355(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5737867726387744355(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5737867726387744355(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5737867726387744355(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5737867726387744355(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5737867726387744355);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5737867726387744355);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5737867726387744355);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__6484932448295229263 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      67108864 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6484932448295229263(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 67108864} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6484932448295229263(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6484932448295229263(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6484932448295229263(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6484932448295229263(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6484932448295229263(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6484932448295229263(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6484932448295229263);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6484932448295229263);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6484932448295229263);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__5535888665400834048 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5535888665400834048(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__5535888665400834048(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5535888665400834048(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5535888665400834048(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5535888665400834048(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__5535888665400834048(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5535888665400834048(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__5535888665400834048);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5535888665400834048);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__5535888665400834048);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__17456371258936887239 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      1024 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17456371258936887239(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17456371258936887239(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17456371258936887239(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17456371258936887239(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17456371258936887239(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17456371258936887239(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17456371258936887239(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17456371258936887239);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17456371258936887239);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17456371258936887239);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__13452148842808220188 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      112 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13452148842808220188(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13452148842808220188(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13452148842808220188(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13452148842808220188(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13452148842808220188(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13452148842808220188(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13452148842808220188(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13452148842808220188);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13452148842808220188);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13452148842808220188);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6523802494011570693 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      96 /* FilterCount */, \
      7 /* FilterHeight */, \
      7 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6523802494011570693(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 7} /* FilterHeight */, 
      {"filter_width", 7} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6523802494011570693(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6523802494011570693(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6523802494011570693(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6523802494011570693(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6523802494011570693(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6523802494011570693(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6523802494011570693);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6523802494011570693);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6523802494011570693);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__393687152679589448 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__393687152679589448(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__393687152679589448(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__393687152679589448(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__393687152679589448(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__393687152679589448(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__393687152679589448(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__393687152679589448(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__393687152679589448);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__393687152679589448);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__393687152679589448);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__17603081626822053981 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17603081626822053981(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17603081626822053981(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17603081626822053981(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17603081626822053981(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17603081626822053981(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17603081626822053981(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17603081626822053981(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17603081626822053981);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17603081626822053981);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17603081626822053981);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__707544879834492571 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      16777216 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__707544879834492571(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16777216} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__707544879834492571(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__707544879834492571(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__707544879834492571(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__707544879834492571(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__707544879834492571(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__707544879834492571(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__707544879834492571);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__707544879834492571);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__707544879834492571);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__2666692819096641857 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      64 /* Input0 */, \
      64 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2666692819096641857(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2666692819096641857(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2666692819096641857(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2666692819096641857(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2666692819096641857(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2666692819096641857(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2666692819096641857(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2666692819096641857);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2666692819096641857);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2666692819096641857);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__14362049737759509255 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      200 /* Input2 */, \
      200 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14362049737759509255(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 200} /* Input2 */, 
      {"input[3]", 200} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14362049737759509255(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14362049737759509255(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14362049737759509255(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14362049737759509255(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14362049737759509255(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14362049737759509255(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14362049737759509255);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14362049737759509255);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14362049737759509255);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__4187768857334924766 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      8 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__4187768857334924766(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 8} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__4187768857334924766(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__4187768857334924766(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__4187768857334924766(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__4187768857334924766(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__4187768857334924766(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__4187768857334924766(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__4187768857334924766);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__4187768857334924766);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__4187768857334924766);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2314341683910303295 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      96 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      208 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2314341683910303295(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 208} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2314341683910303295(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2314341683910303295(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2314341683910303295(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2314341683910303295(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2314341683910303295(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2314341683910303295(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2314341683910303295);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2314341683910303295);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2314341683910303295);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__10925653362608750184 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      64 /* Input0 */, \
      64 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10925653362608750184(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10925653362608750184(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10925653362608750184(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10925653362608750184(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10925653362608750184(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10925653362608750184(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10925653362608750184(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10925653362608750184);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10925653362608750184);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10925653362608750184);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__2998289451866793895 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      64 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2998289451866793895(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2998289451866793895(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2998289451866793895(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2998289451866793895(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2998289451866793895(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2998289451866793895(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2998289451866793895(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2998289451866793895);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2998289451866793895);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2998289451866793895);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4413404171703714165 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      16384 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4413404171703714165(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4413404171703714165(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4413404171703714165(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4413404171703714165(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4413404171703714165(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4413404171703714165(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4413404171703714165(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4413404171703714165);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4413404171703714165);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4413404171703714165);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14763982619828130876 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2097152 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14763982619828130876(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2097152} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14763982619828130876(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14763982619828130876(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14763982619828130876(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14763982619828130876(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__14763982619828130876(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14763982619828130876(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14763982619828130876);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14763982619828130876);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__14763982619828130876);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__2220236799532123520 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      544 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      544 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2220236799532123520(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 544} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 544} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2220236799532123520(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2220236799532123520(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2220236799532123520(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2220236799532123520(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2220236799532123520(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2220236799532123520(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2220236799532123520);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2220236799532123520);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2220236799532123520);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__4111816220364081621 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      65536 /* Input0 */, \
      384 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4111816220364081621(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4111816220364081621(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4111816220364081621(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4111816220364081621(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4111816220364081621(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4111816220364081621(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4111816220364081621(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4111816220364081621);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4111816220364081621);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__4111816220364081621);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14381851246169843422 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1280 /* Input1 */, \
      1 /* Input2 */, \
      1 /* Input3 */, \
      1000 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14381851246169843422(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1280} /* Input1 */, 
      {"input[2]", 1} /* Input2 */, 
      {"input[3]", 1} /* Input3 */, 
      {"filter_count", 1000} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14381851246169843422(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14381851246169843422(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14381851246169843422(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14381851246169843422(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14381851246169843422(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14381851246169843422(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14381851246169843422);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14381851246169843422);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14381851246169843422);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__13539315748817715422 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      96 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13539315748817715422(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13539315748817715422(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13539315748817715422(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13539315748817715422(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13539315748817715422(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13539315748817715422(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13539315748817715422(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13539315748817715422);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13539315748817715422);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13539315748817715422);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__476901516806820995 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      48 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__476901516806820995(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 48} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__476901516806820995(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__476901516806820995(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__476901516806820995(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__476901516806820995(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__476901516806820995(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__476901516806820995(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__476901516806820995);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__476901516806820995);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__476901516806820995);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__13592278580854453526 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      34359738368 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13592278580854453526(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 34359738368} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13592278580854453526(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13592278580854453526(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13592278580854453526(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13592278580854453526(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13592278580854453526(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13592278580854453526(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13592278580854453526);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13592278580854453526);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13592278580854453526);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__17994478305195328476 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17994478305195328476(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17994478305195328476(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17994478305195328476(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17994478305195328476(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17994478305195328476(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17994478305195328476(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17994478305195328476(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17994478305195328476);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17994478305195328476);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17994478305195328476);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__6295988394403473285 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      24 /* Input1 */, \
      113 /* Input2 */, \
      113 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6295988394403473285(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 113} /* Input2 */, 
      {"input[3]", 113} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6295988394403473285(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6295988394403473285(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6295988394403473285(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6295988394403473285(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6295988394403473285(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6295988394403473285(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6295988394403473285);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6295988394403473285);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__6295988394403473285);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__17814682457714153090 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17814682457714153090(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__17814682457714153090(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17814682457714153090(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17814682457714153090(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17814682457714153090(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__17814682457714153090(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17814682457714153090(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__17814682457714153090);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17814682457714153090);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__17814682457714153090);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__2302444874456274904 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      8 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2302444874456274904(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 8} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2302444874456274904(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2302444874456274904(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2302444874456274904(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2302444874456274904(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2302444874456274904(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2302444874456274904(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2302444874456274904);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2302444874456274904);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2302444874456274904);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__9977007714953950306 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      113 /* Input2 */, \
      113 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9977007714953950306(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 113} /* Input2 */, 
      {"input[3]", 113} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__9977007714953950306(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9977007714953950306(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9977007714953950306(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9977007714953950306(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__9977007714953950306(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9977007714953950306(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__9977007714953950306);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9977007714953950306);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__9977007714953950306);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_DROPOUT_FWD__11337979880219331917 {

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__11337979880219331917(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


static void LAYER_CUDNN_DROPOUT_FWD_FLOAT16__11337979880219331917(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<__half, batchnorm_mode, is_training>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__11337979880219331917(state);
}

static void LAYER_CUDNN_DROPOUT_FWD_FLOAT32__11337979880219331917(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<float, batchnorm_mode, is_training>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__11337979880219331917(state);
}

static void LAYER_CUDNN_DROPOUT_FWD_TENSORCOREHALF__11337979880219331917(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<__half, batchnorm_mode, is_training>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__11337979880219331917(state);
}




BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT16__11337979880219331917);

BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT32__11337979880219331917);

BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_TENSORCOREHALF__11337979880219331917);

#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES
}

namespace CUDNN_POOLING_FWD__3831749396814048086 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__3831749396814048086(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__3831749396814048086(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__3831749396814048086(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__3831749396814048086(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__3831749396814048086(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__3831749396814048086(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__3831749396814048086(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__3831749396814048086);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__3831749396814048086);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__3831749396814048086);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6138121043116353470 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6138121043116353470(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6138121043116353470(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6138121043116353470(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6138121043116353470(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6138121043116353470(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6138121043116353470(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6138121043116353470(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6138121043116353470);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6138121043116353470);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6138121043116353470);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__16109729242378910049 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      960 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16109729242378910049(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 960} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16109729242378910049(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16109729242378910049(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16109729242378910049(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16109729242378910049(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16109729242378910049(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16109729242378910049(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16109729242378910049);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16109729242378910049);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16109729242378910049);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__5205995873739938076 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      36864 /* Input0 */, \
      224 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5205995873739938076(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 36864} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__5205995873739938076(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5205995873739938076(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5205995873739938076(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5205995873739938076(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__5205995873739938076(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5205995873739938076(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__5205995873739938076);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5205995873739938076);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__5205995873739938076);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__18165625167647248486 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18165625167647248486(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__18165625167647248486(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18165625167647248486(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18165625167647248486(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18165625167647248486(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18165625167647248486(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18165625167647248486(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__18165625167647248486);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18165625167647248486);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18165625167647248486);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__12188593279200330109 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      256 /* Input0 */, \
      24 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12188593279200330109(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12188593279200330109(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12188593279200330109(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12188593279200330109(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12188593279200330109(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12188593279200330109(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12188593279200330109(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12188593279200330109);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12188593279200330109);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12188593279200330109);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__9247238374669043677 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      112 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9247238374669043677(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9247238374669043677(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9247238374669043677(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9247238374669043677(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9247238374669043677(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9247238374669043677(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9247238374669043677(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9247238374669043677);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9247238374669043677);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9247238374669043677);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__10224052846270232782 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      2048 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10224052846270232782(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__10224052846270232782(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10224052846270232782(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10224052846270232782(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10224052846270232782(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__10224052846270232782(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10224052846270232782(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__10224052846270232782);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10224052846270232782);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__10224052846270232782);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__3523507296020240283 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3523507296020240283(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3523507296020240283(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3523507296020240283(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3523507296020240283(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3523507296020240283(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3523507296020240283(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3523507296020240283(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3523507296020240283);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3523507296020240283);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3523507296020240283);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__3743642883629099794 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1024 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      112 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3743642883629099794(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 112} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3743642883629099794(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3743642883629099794(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3743642883629099794(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3743642883629099794(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3743642883629099794(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3743642883629099794(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3743642883629099794);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3743642883629099794);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3743642883629099794);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8978159188323800738 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8978159188323800738(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8978159188323800738(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8978159188323800738(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8978159188323800738(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8978159188323800738(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8978159188323800738(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8978159188323800738(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8978159188323800738);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8978159188323800738);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8978159188323800738);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8958509875836500440 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      112 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      112 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8958509875836500440(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 112} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8958509875836500440(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8958509875836500440(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8958509875836500440(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8958509875836500440(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8958509875836500440(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8958509875836500440(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8958509875836500440);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8958509875836500440);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8958509875836500440);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__17004383688461773993 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      13 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17004383688461773993(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17004383688461773993(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17004383688461773993(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17004383688461773993(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17004383688461773993(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17004383688461773993(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17004383688461773993(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17004383688461773993);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17004383688461773993);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17004383688461773993);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__15867081059835961730 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      36864 /* Input0 */, \
      160 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15867081059835961730(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 36864} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15867081059835961730(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15867081059835961730(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15867081059835961730(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15867081059835961730(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15867081059835961730(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15867081059835961730(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15867081059835961730);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15867081059835961730);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__15867081059835961730);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__16287111272625902613 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      536870912 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16287111272625902613(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 536870912} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__16287111272625902613(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16287111272625902613(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16287111272625902613(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16287111272625902613(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__16287111272625902613(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16287111272625902613(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__16287111272625902613);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16287111272625902613);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__16287111272625902613);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__10577399006807342433 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      64 /* Input0 */, \
      224 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10577399006807342433(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10577399006807342433(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10577399006807342433(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10577399006807342433(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10577399006807342433(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10577399006807342433(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10577399006807342433(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10577399006807342433);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10577399006807342433);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__10577399006807342433);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__5803933271538031119 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      36028797018963968 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5803933271538031119(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 36028797018963968} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5803933271538031119(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5803933271538031119(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5803933271538031119(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5803933271538031119(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5803933271538031119(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5803933271538031119(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5803933271538031119);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5803933271538031119);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5803933271538031119);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__10584501488166603860 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      64 /* Input0 */, \
      192 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__10584501488166603860(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__10584501488166603860(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__10584501488166603860(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__10584501488166603860(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__10584501488166603860(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__10584501488166603860(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__10584501488166603860(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__10584501488166603860);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__10584501488166603860);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__10584501488166603860);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__6842313322737127003 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      144115188075855872 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6842313322737127003(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 144115188075855872} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6842313322737127003(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6842313322737127003(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6842313322737127003(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6842313322737127003(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6842313322737127003(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6842313322737127003(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6842313322737127003);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6842313322737127003);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6842313322737127003);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__429985896337117372 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__429985896337117372(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__429985896337117372(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__429985896337117372(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__429985896337117372(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__429985896337117372(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__429985896337117372(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__429985896337117372(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__429985896337117372);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__429985896337117372);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__429985896337117372);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10266435012932268711 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      64 /* Input1 */, \
      52 /* Input2 */, \
      52 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10266435012932268711(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 52} /* Input2 */, 
      {"input[3]", 52} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10266435012932268711(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10266435012932268711(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10266435012932268711(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10266435012932268711(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10266435012932268711(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10266435012932268711(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10266435012932268711);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10266435012932268711);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10266435012932268711);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12447805174032346431 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12447805174032346431(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12447805174032346431(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12447805174032346431(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12447805174032346431(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12447805174032346431(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__12447805174032346431(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12447805174032346431(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12447805174032346431);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12447805174032346431);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__12447805174032346431);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__13861555791611592547 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      256 /* Input0 */, \
      160 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13861555791611592547(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13861555791611592547(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13861555791611592547(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13861555791611592547(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13861555791611592547(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13861555791611592547(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13861555791611592547(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13861555791611592547);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13861555791611592547);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13861555791611592547);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__7154078083833838117 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      272 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__7154078083833838117(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 272} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__7154078083833838117(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__7154078083833838117(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__7154078083833838117(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__7154078083833838117(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__7154078083833838117(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__7154078083833838117(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__7154078083833838117);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__7154078083833838117);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__7154078083833838117);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9277284538799615785 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      48 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9277284538799615785(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 48} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9277284538799615785(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9277284538799615785(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9277284538799615785(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9277284538799615785(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9277284538799615785(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9277284538799615785(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9277284538799615785);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9277284538799615785);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9277284538799615785);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__16632830441332629440 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      512 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16632830441332629440(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 512} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__16632830441332629440(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16632830441332629440(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16632830441332629440(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16632830441332629440(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__16632830441332629440(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16632830441332629440(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__16632830441332629440);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16632830441332629440);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__16632830441332629440);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__3902960370994517963 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3902960370994517963(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3902960370994517963(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3902960370994517963(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3902960370994517963(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3902960370994517963(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3902960370994517963(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3902960370994517963(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3902960370994517963);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3902960370994517963);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3902960370994517963);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__9391482289399311053 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      134217728 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9391482289399311053(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 134217728} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9391482289399311053(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9391482289399311053(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9391482289399311053(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9391482289399311053(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9391482289399311053(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9391482289399311053(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9391482289399311053);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9391482289399311053);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9391482289399311053);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__7490754067026137401 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      256 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7490754067026137401(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7490754067026137401(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7490754067026137401(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7490754067026137401(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7490754067026137401(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7490754067026137401(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7490754067026137401(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7490754067026137401);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7490754067026137401);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7490754067026137401);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__11973909879502332189 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      64 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11973909879502332189(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11973909879502332189(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11973909879502332189(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11973909879502332189(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11973909879502332189(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11973909879502332189(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11973909879502332189(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11973909879502332189);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11973909879502332189);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11973909879502332189);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__17022950768377567731 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1024 /* Input0 */, \
      288 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17022950768377567731(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 288} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17022950768377567731(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17022950768377567731(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17022950768377567731(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17022950768377567731(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17022950768377567731(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17022950768377567731(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17022950768377567731);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17022950768377567731);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__17022950768377567731);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__7787717258985555903 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      113 /* Input2 */, \
      113 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7787717258985555903(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 113} /* Input2 */, 
      {"input[3]", 113} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7787717258985555903(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7787717258985555903(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7787717258985555903(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7787717258985555903(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7787717258985555903(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7787717258985555903(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7787717258985555903);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7787717258985555903);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7787717258985555903);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12931993179955788384 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      64 /* Input2 */, \
      64 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12931993179955788384(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 64} /* Input2 */, 
      {"input[3]", 64} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12931993179955788384(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12931993179955788384(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12931993179955788384(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12931993179955788384(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12931993179955788384(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12931993179955788384(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12931993179955788384);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12931993179955788384);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12931993179955788384);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11835755202448434304 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1 /* Input1 */, \
      64 /* Input2 */, \
      64 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11835755202448434304(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1} /* Input1 */, 
      {"input[2]", 64} /* Input2 */, 
      {"input[3]", 64} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11835755202448434304(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11835755202448434304(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11835755202448434304(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11835755202448434304(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11835755202448434304(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11835755202448434304(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11835755202448434304);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11835755202448434304);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11835755202448434304);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__8089631561747715914 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8089631561747715914(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8089631561747715914(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8089631561747715914(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8089631561747715914(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8089631561747715914(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8089631561747715914(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8089631561747715914(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8089631561747715914);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8089631561747715914);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8089631561747715914);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__5726219828479838718 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      147456 /* Input0 */, \
      224 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5726219828479838718(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 147456} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5726219828479838718(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5726219828479838718(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5726219828479838718(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5726219828479838718(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5726219828479838718(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5726219828479838718(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5726219828479838718);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5726219828479838718);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5726219828479838718);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__13011425189416604842 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      160 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      320 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13011425189416604842(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 320} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13011425189416604842(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13011425189416604842(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13011425189416604842(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13011425189416604842(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13011425189416604842(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13011425189416604842(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13011425189416604842);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13011425189416604842);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13011425189416604842);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9371617550614312154 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      3 /* Input0 */, \
      136 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9371617550614312154(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 3} /* Input0 */, 
      {"input[1]", 136} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9371617550614312154(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9371617550614312154(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9371617550614312154(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9371617550614312154(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9371617550614312154(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9371617550614312154(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9371617550614312154);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9371617550614312154);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9371617550614312154);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__5815829623744744861 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1048576 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5815829623744744861(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1048576} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5815829623744744861(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5815829623744744861(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5815829623744744861(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5815829623744744861(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5815829623744744861(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5815829623744744861(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5815829623744744861);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5815829623744744861);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5815829623744744861);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__4238116798237246387 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      65536 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4238116798237246387(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4238116798237246387(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4238116798237246387(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4238116798237246387(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4238116798237246387(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4238116798237246387(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4238116798237246387(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4238116798237246387);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4238116798237246387);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4238116798237246387);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__15087646552068289456 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__15087646552068289456(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__15087646552068289456(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__15087646552068289456(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__15087646552068289456(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__15087646552068289456(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__15087646552068289456(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__15087646552068289456(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__15087646552068289456);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__15087646552068289456);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__15087646552068289456);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__12557368046752088273 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12557368046752088273(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12557368046752088273(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12557368046752088273(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12557368046752088273(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12557368046752088273(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12557368046752088273(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12557368046752088273(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12557368046752088273);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12557368046752088273);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__12557368046752088273);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace CUDNN_POOLING_FWD__8579728266776225750 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      256 /* Input0 */, \
      160 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__8579728266776225750(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__8579728266776225750(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__8579728266776225750(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__8579728266776225750(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__8579728266776225750(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__8579728266776225750(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__8579728266776225750(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__8579728266776225750);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__8579728266776225750);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__8579728266776225750);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__15121535866080494159 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      8 /* Input0 */, \
      32 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15121535866080494159(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 8} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15121535866080494159(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15121535866080494159(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15121535866080494159(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15121535866080494159(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15121535866080494159(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15121535866080494159(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15121535866080494159);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15121535866080494159);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15121535866080494159);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__3758081355151541313 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      48 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3758081355151541313(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3758081355151541313(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3758081355151541313(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3758081355151541313(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3758081355151541313(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3758081355151541313(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3758081355151541313(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3758081355151541313);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3758081355151541313);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3758081355151541313);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5732829196694291667 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      274877906944 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5732829196694291667(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 274877906944} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5732829196694291667(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5732829196694291667(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5732829196694291667(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5732829196694291667(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5732829196694291667(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5732829196694291667(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5732829196694291667);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5732829196694291667);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5732829196694291667);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__3043449720934595897 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      256 /* Input0 */, \
      160 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__3043449720934595897(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__3043449720934595897(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__3043449720934595897(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__3043449720934595897(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__3043449720934595897(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__3043449720934595897(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__3043449720934595897(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__3043449720934595897);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__3043449720934595897);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__3043449720934595897);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace CUDNN_POOLING_FWD__257463218135549220 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      3 /* Input0 */, \
      136 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__257463218135549220(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 3} /* Input0 */, 
      {"input[1]", 136} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__257463218135549220(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__257463218135549220(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__257463218135549220(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__257463218135549220(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__257463218135549220(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__257463218135549220(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__257463218135549220);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__257463218135549220);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__257463218135549220);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12909764585544611250 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      960 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12909764585544611250(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 960} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12909764585544611250(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12909764585544611250(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12909764585544611250(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12909764585544611250(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12909764585544611250(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12909764585544611250(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12909764585544611250);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12909764585544611250);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12909764585544611250);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__7662947898293869751 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      192 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7662947898293869751(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 192} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7662947898293869751(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7662947898293869751(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7662947898293869751(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7662947898293869751(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7662947898293869751(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7662947898293869751(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7662947898293869751);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7662947898293869751);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7662947898293869751);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__5088430916573185592 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1024 /* Input0 */, \
      32 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      64 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5088430916573185592(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5088430916573185592(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5088430916573185592(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5088430916573185592(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5088430916573185592(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5088430916573185592(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5088430916573185592(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5088430916573185592);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5088430916573185592);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5088430916573185592);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__949962475537020553 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__949962475537020553(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__949962475537020553(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__949962475537020553(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__949962475537020553(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__949962475537020553(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__949962475537020553(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__949962475537020553(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__949962475537020553);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__949962475537020553);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__949962475537020553);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__3971418395174548088 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      136 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      136 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3971418395174548088(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 136} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 136} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3971418395174548088(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3971418395174548088(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3971418395174548088(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3971418395174548088(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3971418395174548088(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3971418395174548088(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3971418395174548088);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3971418395174548088);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3971418395174548088);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__13523329562865626151 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13523329562865626151(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13523329562865626151(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13523329562865626151(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13523329562865626151(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13523329562865626151(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__13523329562865626151(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13523329562865626151(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13523329562865626151);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13523329562865626151);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__13523329562865626151);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__17880801084229826574 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2251799813685248 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17880801084229826574(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2251799813685248} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17880801084229826574(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17880801084229826574(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17880801084229826574(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17880801084229826574(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17880801084229826574(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17880801084229826574(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17880801084229826574);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17880801084229826574);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17880801084229826574);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14482067834342257391 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14482067834342257391(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14482067834342257391(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14482067834342257391(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14482067834342257391(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14482067834342257391(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__14482067834342257391(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14482067834342257391(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14482067834342257391);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14482067834342257391);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__14482067834342257391);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__13680423709873302542 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      112 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      136 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13680423709873302542(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 136} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13680423709873302542(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13680423709873302542(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13680423709873302542(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13680423709873302542(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13680423709873302542(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13680423709873302542(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13680423709873302542);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13680423709873302542);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13680423709873302542);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__16591758095447963871 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      16384 /* Input0 */, \
      320 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16591758095447963871(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16591758095447963871(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16591758095447963871(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16591758095447963871(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16591758095447963871(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16591758095447963871(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16591758095447963871(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16591758095447963871);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16591758095447963871);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__16591758095447963871);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__11298588391799646030 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      112 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11298588391799646030(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11298588391799646030(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11298588391799646030(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11298588391799646030(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11298588391799646030(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11298588391799646030(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11298588391799646030(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11298588391799646030);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11298588391799646030);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11298588391799646030);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14196299773104808517 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      16384 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14196299773104808517(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14196299773104808517(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14196299773104808517(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14196299773104808517(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14196299773104808517(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14196299773104808517(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14196299773104808517(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14196299773104808517);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14196299773104808517);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14196299773104808517);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__15702179921545340223 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      256 /* Input0 */, \
      160 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      24 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15702179921545340223(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 24} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15702179921545340223(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15702179921545340223(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15702179921545340223(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15702179921545340223(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15702179921545340223(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15702179921545340223(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15702179921545340223);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15702179921545340223);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15702179921545340223);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__7065942637133279829 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      68719476736 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7065942637133279829(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 68719476736} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7065942637133279829(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7065942637133279829(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7065942637133279829(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7065942637133279829(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7065942637133279829(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7065942637133279829(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7065942637133279829);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7065942637133279829);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7065942637133279829);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12857410744948560099 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      128 /* Input1 */, \
      52 /* Input2 */, \
      52 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12857410744948560099(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 52} /* Input2 */, 
      {"input[3]", 52} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12857410744948560099(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12857410744948560099(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12857410744948560099(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12857410744948560099(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__12857410744948560099(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12857410744948560099(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12857410744948560099);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12857410744948560099);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__12857410744948560099);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__14198419017787858620 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14198419017787858620(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14198419017787858620(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14198419017787858620(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14198419017787858620(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14198419017787858620(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14198419017787858620(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14198419017787858620(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14198419017787858620);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14198419017787858620);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14198419017787858620);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2425157335714570726 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      16384 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2425157335714570726(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2425157335714570726(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2425157335714570726(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2425157335714570726(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2425157335714570726(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2425157335714570726(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2425157335714570726(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2425157335714570726);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2425157335714570726);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2425157335714570726);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__4306529683339385503 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      27 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__4306529683339385503(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__4306529683339385503(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__4306529683339385503(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__4306529683339385503(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__4306529683339385503(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__4306529683339385503(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__4306529683339385503(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__4306529683339385503);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__4306529683339385503);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__4306529683339385503);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__831836460854888125 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__831836460854888125(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__831836460854888125(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__831836460854888125(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__831836460854888125(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__831836460854888125(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__831836460854888125(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__831836460854888125(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__831836460854888125);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__831836460854888125);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__831836460854888125);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12133108940302907008 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      36864 /* Input0 */, \
      192 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12133108940302907008(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 36864} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12133108940302907008(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12133108940302907008(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12133108940302907008(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12133108940302907008(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__12133108940302907008(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12133108940302907008(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12133108940302907008);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12133108940302907008);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__12133108940302907008);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__7158895492642518126 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7158895492642518126(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7158895492642518126(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7158895492642518126(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7158895492642518126(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7158895492642518126(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7158895492642518126(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7158895492642518126(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7158895492642518126);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7158895492642518126);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7158895492642518126);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__16294589502476362750 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16294589502476362750(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16294589502476362750(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16294589502476362750(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16294589502476362750(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16294589502476362750(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16294589502476362750(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16294589502476362750(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16294589502476362750);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16294589502476362750);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16294589502476362750);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__10124637858072950724 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10124637858072950724(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10124637858072950724(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10124637858072950724(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10124637858072950724(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10124637858072950724(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10124637858072950724(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10124637858072950724(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10124637858072950724);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10124637858072950724);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10124637858072950724);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14350163372222648467 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14350163372222648467(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14350163372222648467(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14350163372222648467(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14350163372222648467(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14350163372222648467(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__14350163372222648467(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14350163372222648467(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14350163372222648467);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14350163372222648467);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__14350163372222648467);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__5680056284274657766 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      64 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5680056284274657766(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5680056284274657766(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5680056284274657766(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5680056284274657766(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5680056284274657766(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5680056284274657766(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5680056284274657766(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5680056284274657766);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5680056284274657766);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5680056284274657766);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__3692590918538225237 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3692590918538225237(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3692590918538225237(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3692590918538225237(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3692590918538225237(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3692590918538225237(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3692590918538225237(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3692590918538225237(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3692590918538225237);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3692590918538225237);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__3692590918538225237);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__8376098661082297114 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      3072 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8376098661082297114(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 3072} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8376098661082297114(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8376098661082297114(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8376098661082297114(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8376098661082297114(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8376098661082297114(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8376098661082297114(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8376098661082297114);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8376098661082297114);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__8376098661082297114);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12144473031666270479 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      16384 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12144473031666270479(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12144473031666270479(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12144473031666270479(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12144473031666270479(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12144473031666270479(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12144473031666270479(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12144473031666270479(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12144473031666270479);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12144473031666270479);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12144473031666270479);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__13620459069082123845 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13620459069082123845(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13620459069082123845(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13620459069082123845(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13620459069082123845(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13620459069082123845(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13620459069082123845(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13620459069082123845(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13620459069082123845);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13620459069082123845);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13620459069082123845);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8030653797742509741 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      256 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8030653797742509741(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8030653797742509741(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8030653797742509741(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8030653797742509741(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8030653797742509741(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8030653797742509741(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8030653797742509741(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8030653797742509741);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8030653797742509741);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8030653797742509741);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__6449400684366778803 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6449400684366778803(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__6449400684366778803(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6449400684366778803(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6449400684366778803(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6449400684366778803(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__6449400684366778803(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6449400684366778803(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__6449400684366778803);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6449400684366778803);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__6449400684366778803);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_CONV_FWD__3012102407231625782 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      112 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3012102407231625782(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3012102407231625782(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3012102407231625782(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3012102407231625782(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3012102407231625782(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3012102407231625782(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3012102407231625782(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3012102407231625782);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3012102407231625782);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3012102407231625782);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__7451835566427819488 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      64 /* Input0 */, \
      192 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      112 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7451835566427819488(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 64} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 112} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7451835566427819488(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7451835566427819488(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7451835566427819488(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7451835566427819488(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7451835566427819488(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7451835566427819488(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7451835566427819488);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7451835566427819488);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7451835566427819488);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace CUDNN_POOLING_FWD__1359900246361476637 {

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      256 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__1359900246361476637(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}

  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT16__1359900246361476637(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__1359900246361476637(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_FLOAT32__1359900246361476637(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<float, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__1359900246361476637(state);
  }
  
  template <cudnnActivationMode_t activation_mode>
  static void CUDNN_POOLING_FWD_TENSORCOREHALF__1359900246361476637(benchmark::State& state) {
    CUDNN_POOLING_FWD_Impl<__half, activation_mode>(state);
    BENCHMARK_CUDNN_POOLING_FWD_ADD_COUNTERS__1359900246361476637(state);
  }
  

#define BENCHMARK_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)->BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT16__1359900246361476637);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_FLOAT32__1359900246361476637);
BENCHMARK_CUDNN_POOLING_FWD(CUDNN_POOLING_FWD_TENSORCOREHALF__1359900246361476637);

#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_CUDNN_POOLING_FWD
}

namespace LAYER_CUDNN_CONV_FWD__7853913981983946364 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      272 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      272 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7853913981983946364(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 272} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 272} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7853913981983946364(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7853913981983946364(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7853913981983946364(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7853913981983946364(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7853913981983946364(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7853913981983946364(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7853913981983946364);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7853913981983946364);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7853913981983946364);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__15794554034599751004 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      1024 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      125 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15794554034599751004(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"filter_count", 125} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15794554034599751004(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15794554034599751004(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15794554034599751004(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15794554034599751004(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15794554034599751004(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15794554034599751004(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15794554034599751004);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15794554034599751004);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15794554034599751004);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__2128039374656748663 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      16384 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2128039374656748663(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2128039374656748663(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2128039374656748663(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2128039374656748663(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2128039374656748663(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2128039374656748663(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2128039374656748663(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2128039374656748663);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2128039374656748663);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2128039374656748663);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__5079635435952189035 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5079635435952189035(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5079635435952189035(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5079635435952189035(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5079635435952189035(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5079635435952189035(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5079635435952189035(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5079635435952189035(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5079635435952189035);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5079635435952189035);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5079635435952189035);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__7019084016266375959 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1024 /* Input0 */, \
      32 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7019084016266375959(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1024} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7019084016266375959(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7019084016266375959(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7019084016266375959(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7019084016266375959(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7019084016266375959(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7019084016266375959(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7019084016266375959);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7019084016266375959);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__7019084016266375959);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__13507187380669362047 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13507187380669362047(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13507187380669362047(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13507187380669362047(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13507187380669362047(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13507187380669362047(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13507187380669362047(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13507187380669362047(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13507187380669362047);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13507187380669362047);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__13507187380669362047);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__2962000106640027459 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      200 /* Input2 */, \
      200 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2962000106640027459(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 200} /* Input2 */, 
      {"input[3]", 200} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2962000106640027459(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2962000106640027459(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2962000106640027459(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2962000106640027459(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2962000106640027459(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2962000106640027459(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2962000106640027459);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2962000106640027459);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__2962000106640027459);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__11006824204393138845 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      128 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11006824204393138845(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11006824204393138845(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11006824204393138845(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11006824204393138845(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11006824204393138845(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11006824204393138845(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11006824204393138845(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11006824204393138845);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11006824204393138845);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__11006824204393138845);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__14509906446812585052 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      4 /* Input0 */, \
      32 /* Input1 */, \
      27 /* Input2 */, \
      29 /* Input3 */, \
      96 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14509906446812585052(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14509906446812585052(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14509906446812585052(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14509906446812585052(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14509906446812585052(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14509906446812585052(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14509906446812585052(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14509906446812585052);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14509906446812585052);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14509906446812585052);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__442335219579300662 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      16384 /* Input0 */, \
      320 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__442335219579300662(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16384} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__442335219579300662(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__442335219579300662(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__442335219579300662(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__442335219579300662(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__442335219579300662(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__442335219579300662(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__442335219579300662);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__442335219579300662);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__442335219579300662);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8903221941647550432 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      5 /* PadHeight */, \
      5 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      5 /* DilationWidth */, \
      5 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8903221941647550432(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 5} /* PadHeight */, 
      {"pad_width", 5} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 5} /* DilationWidth */, 
      {"dilation_width", 5} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8903221941647550432(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8903221941647550432(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8903221941647550432(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8903221941647550432(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8903221941647550432(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8903221941647550432(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8903221941647550432);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8903221941647550432);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8903221941647550432);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__12305101108046529101 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      16 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12305101108046529101(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12305101108046529101(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12305101108046529101(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12305101108046529101(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12305101108046529101(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12305101108046529101(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12305101108046529101(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12305101108046529101);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12305101108046529101);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12305101108046529101);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_ACTIVATION_FWD__9932391046932113341 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      65536 /* Input0 */, \
      48 /* Input1 */, \
      6 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9932391046932113341(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 65536} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9932391046932113341(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9932391046932113341(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9932391046932113341(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9932391046932113341(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9932391046932113341(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9932391046932113341(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9932391046932113341);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9932391046932113341);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__9932391046932113341);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_CONV_FWD__1891125580653987228 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      29 /* Input2 */, \
      29 /* Input3 */, \
      384 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1891125580653987228(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 29} /* Input2 */, 
      {"input[3]", 29} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1891125580653987228(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1891125580653987228(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1891125580653987228(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1891125580653987228(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1891125580653987228(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1891125580653987228(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1891125580653987228);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1891125580653987228);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1891125580653987228);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__1819044840243697232 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      16 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1819044840243697232(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 16} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1819044840243697232(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1819044840243697232(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1819044840243697232(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1819044840243697232(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1819044840243697232(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1819044840243697232(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1819044840243697232);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1819044840243697232);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1819044840243697232);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_CONV_FWD__8283406541106240680 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      101 /* Input2 */, \
      101 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      9 /* PadHeight */, \
      9 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      9 /* DilationWidth */, \
      9 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8283406541106240680(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 101} /* Input2 */, 
      {"input[3]", 101} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 9} /* PadHeight */, 
      {"pad_width", 9} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 9} /* DilationWidth */, 
      {"dilation_width", 9} /* DilationHeight */, 
  });
}


template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8283406541106240680(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8283406541106240680(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8283406541106240680(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8283406541106240680(state);
}

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8283406541106240680(benchmark::State& state) {
  LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8283406541106240680(state);
}


#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_​WINOGRAD_NONFUSED)->BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8283406541106240680);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8283406541106240680);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8283406541106240680);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__99896546937457228 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      3 /* Input0 */, \
      136 /* Input1 */, \
      15 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__99896546937457228(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 3} /* Input0 */, 
      {"input[1]", 136} /* Input1 */, 
      {"input[2]", 15} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__99896546937457228(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__99896546937457228(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__99896546937457228(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__99896546937457228(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__99896546937457228(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__99896546937457228(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__99896546937457228);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__99896546937457228);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__99896546937457228);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__5301436226329136698 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      256 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      15 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5301436226329136698(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 256} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 15} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5301436226329136698(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5301436226329136698(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5301436226329136698(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5301436226329136698(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5301436226329136698(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5301436226329136698(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5301436226329136698);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5301436226329136698);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__5301436226329136698);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12937232957488818445 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12937232957488818445(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12937232957488818445(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12937232957488818445(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12937232957488818445(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12937232957488818445(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__12937232957488818445(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12937232957488818445(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12937232957488818445);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12937232957488818445);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__12937232957488818445);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_DROPOUT_FWD__17612952208313031193 {

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__17612952208313031193(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


static void LAYER_CUDNN_DROPOUT_FWD_FLOAT16__17612952208313031193(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<__half, batchnorm_mode, is_training>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__17612952208313031193(state);
}

static void LAYER_CUDNN_DROPOUT_FWD_FLOAT32__17612952208313031193(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<float, batchnorm_mode, is_training>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__17612952208313031193(state);
}

static void LAYER_CUDNN_DROPOUT_FWD_TENSORCOREHALF__17612952208313031193(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<__half, batchnorm_mode, is_training>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__17612952208313031193(state);
}




BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT16__17612952208313031193);

BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT32__17612952208313031193);

BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_TENSORCOREHALF__17612952208313031193);

#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__7227564274308548869 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      24 /* Input1 */, \
      57 /* Input2 */, \
      57 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7227564274308548869(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 57} /* Input2 */, 
      {"input[3]", 57} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__7227564274308548869(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7227564274308548869(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7227564274308548869(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7227564274308548869(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__7227564274308548869(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7227564274308548869(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__7227564274308548869);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7227564274308548869);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__7227564274308548869);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__11047781365771010104 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      1024 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11047781365771010104(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__11047781365771010104(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11047781365771010104(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11047781365771010104(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11047781365771010104(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__11047781365771010104(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11047781365771010104(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)->BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__11047781365771010104);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11047781365771010104);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_TENSORCOREHALF__11047781365771010104);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}

namespace LAYER_CUDNN_ACTIVATION_FWD__14565144235766138226 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14565144235766138226(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14565144235766138226(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14565144235766138226(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14565144235766138226(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14565144235766138226(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14565144235766138226(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14565144235766138226(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)->BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14565144235766138226);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14565144235766138226);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_TENSORCOREHALF__14565144235766138226);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
