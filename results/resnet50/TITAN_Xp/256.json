{
  "context": {
    "date": "2019-09-25 21:21:55",
    "executable": "./scope",
    "num_cpus": 10,
    "mhz_per_cpu": 4500,
    "cpu_scaling_enabled": true,
    "caches": [
      {
        "type": "Data",
        "level": 1,
        "size": 32000000,
        "num_sharing": 1
      },
      {
        "type": "Instruction",
        "level": 1,
        "size": 32000000,
        "num_sharing": 1
      },
      {
        "type": "Unified",
        "level": 2,
        "size": 1024000000,
        "num_sharing": 1
      },
      {
        "type": "Unified",
        "level": 3,
        "size": 14080000000,
        "num_sharing": 10
      }
    ],
    "library_build_type": "release"
  },
  "benchmarks": [
    {
      "name": "LAYER_CUBLAS_GEMM_FWD_FLOAT32__BatchSize_256__1908072166209867446/input[0]:256/input[1]:1000/input[2]:2048/input[3]:0/input[4]:1/input[5]:1/input[6]:1/input[7]:-1/batch_size:256/manual_time",
      "iterations": 4211,
      "real_time": 1.6617356830630300e+05,
      "cpu_time": 1.7170336000950163e+05,
      "time_unit": "ns",
      "items_per_second": 3.1550625369829868e+12,
      "K": 2.0480000000000000e+03,
      "M": 2.5600000000000000e+02,
      "N": 1.0000000000000000e+03,
      "alpha": 1.0000000000000000e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cublas_gemm_fwd.cpp": 7.2770906096155412e+18,
      "benchmark_func:void iLAYER_CUBLAS_GEMM_FWD_Impl(benchmark::State&) [with T = float]": 7.4787548781531884e+18,
      "beta": 1.0000000000000000e+00,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUBLAS_GEMM_FWD_Impl": 1.7610984762236383e+19,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0000000000000000e+03,
      "input[2]": 2.0480000000000000e+03,
      "input[3]": 0.0000000000000000e+00,
      "input[4]": 1.0000000000000000e+00,
      "input[5]": 1.0000000000000000e+00,
      "input[6]": 1.0000000000000000e+00,
      "input[7]": -1.0000000000000000e+00,
      "lda": 2.5600000000000000e+02,
      "ldb": 1.0000000000000000e+03,
      "num_iterations": 4.2110000000000000e+03,
      "predicted_flops": 6.3101250739659736e+12,
      "predicted_flops_count": 1.0485760000000000e+09,
      "transA": 0.0000000000000000e+00,
      "transB": 1.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__BatchSize_256__470712234344992899<CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/input[4]:-1/input[5]:-1/input[6]:-1/input[7]:-1/batch_size:256/manual_time",
      "iterations": 142,
      "real_time": 4.9439770149880312e+06,
      "cpu_time": 5.3076698732394362e+06,
      "time_unit": "ns",
      "items_per_second": 4.1569953779507515e+10,
      "activation_mode": 1.0000000000000000e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_activation_fwd.cpp": 1.4930345010129449e+19,
      "benchmark_func:void iLAYER_CUDNN_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1]": 1.0148130340695654e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_ACTIVATION_FWD_Impl": 1.7018446903751907e+19,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input[4]": -1.0000000000000000e+00,
      "input[5]": -1.0000000000000000e+00,
      "input[6]": -1.0000000000000000e+00,
      "input[7]": -1.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "num_iterations": 1.4200000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 5.6000000000000000e+01,
      "output_size": 2.0552089600000000e+08,
      "output_width": 5.6000000000000000e+01,
      "predicted_flops": 4.1569953779507515e+10,
      "predicted_flops_count": 2.0552089600000000e+08
    },
    {
      "name": "LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__BatchSize_256__470712234344992899<CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/input[4]:-1/input[5]:-1/input[6]:-1/input[7]:-1/batch_size:256/manual_time",
      "iterations": 142,
      "real_time": 4.9310931935906410e+06,
      "cpu_time": 5.2958668309858795e+06,
      "time_unit": "ns",
      "items_per_second": 4.1678566583801926e+10,
      "activation_mode": 3.0000000000000000e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_activation_fwd.cpp": 1.4930345010129449e+19,
      "benchmark_func:void iLAYER_CUDNN_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3]": 1.0150096267486517e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_ACTIVATION_FWD_Impl": 1.7018446903751907e+19,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input[4]": -1.0000000000000000e+00,
      "input[5]": -1.0000000000000000e+00,
      "input[6]": -1.0000000000000000e+00,
      "input[7]": -1.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "num_iterations": 1.4200000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 5.6000000000000000e+01,
      "output_size": 2.0552089600000000e+08,
      "output_width": 5.6000000000000000e+01,
      "predicted_flops": 4.1678566583801926e+10,
      "predicted_flops_count": 2.0552089600000000e+08
    },
    {
      "name": "LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__BatchSize_256__5224402096529463936<CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/input[4]:-1/input[5]:-1/input[6]:-1/input[7]:-1/batch_size:256/manual_time",
      "iterations": 1387,
      "real_time": 5.0464358341745182e+05,
      "cpu_time": 5.1356787166547991e+05,
      "time_unit": "ns",
      "items_per_second": 5.0907438128958832e+10,
      "activation_mode": 1.0000000000000000e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_activation_fwd.cpp": 1.4930345010129449e+19,
      "benchmark_func:void iLAYER_CUDNN_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1]": 1.0148130340695654e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_ACTIVATION_FWD_Impl": 1.7018446903751907e+19,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input[4]": -1.0000000000000000e+00,
      "input[5]": -1.0000000000000000e+00,
      "input[6]": -1.0000000000000000e+00,
      "input[7]": -1.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "num_iterations": 1.3870000000000000e+03,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "predicted_flops": 5.0907438128958832e+10,
      "predicted_flops_count": 2.5690112000000000e+07
    },
    {
      "name": "LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__BatchSize_256__5224402096529463936<CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/input[4]:-1/input[5]:-1/input[6]:-1/input[7]:-1/batch_size:256/manual_time",
      "iterations": 1386,
      "real_time": 5.0521106559992867e+05,
      "cpu_time": 5.1409237085137074e+05,
      "time_unit": "ns",
      "items_per_second": 5.0850255960829903e+10,
      "activation_mode": 3.0000000000000000e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_activation_fwd.cpp": 1.4930345010129449e+19,
      "benchmark_func:void iLAYER_CUDNN_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3]": 1.0150096267486517e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_ACTIVATION_FWD_Impl": 1.7018446903751907e+19,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input[4]": -1.0000000000000000e+00,
      "input[5]": -1.0000000000000000e+00,
      "input[6]": -1.0000000000000000e+00,
      "input[7]": -1.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "num_iterations": 1.3860000000000000e+03,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "predicted_flops": 5.0850255960829903e+10,
      "predicted_flops_count": 2.5690112000000000e+07
    },
    {
      "name": "LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__BatchSize_256__2427895342727703615<CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:112/input[3]:112/input[4]:-1/input[5]:-1/input[6]:-1/input[7]:-1/batch_size:256/manual_time",
      "iterations": 142,
      "real_time": 4.9375986424602671e+06,
      "cpu_time": 5.3022131197183290e+06,
      "time_unit": "ns",
      "items_per_second": 4.1623653699320259e+10,
      "activation_mode": 1.0000000000000000e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_activation_fwd.cpp": 1.4930345010129449e+19,
      "benchmark_func:void iLAYER_CUDNN_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1]": 1.0148130340695654e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_ACTIVATION_FWD_Impl": 1.7018446903751907e+19,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 1.1200000000000000e+02,
      "input[3]": 1.1200000000000000e+02,
      "input[4]": -1.0000000000000000e+00,
      "input[5]": -1.0000000000000000e+00,
      "input[6]": -1.0000000000000000e+00,
      "input[7]": -1.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 1.1200000000000000e+02,
      "input_size": 2.0552089600000000e+08,
      "input_width": 1.1200000000000000e+02,
      "num_iterations": 1.4200000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 1.1200000000000000e+02,
      "output_size": 2.0552089600000000e+08,
      "output_width": 1.1200000000000000e+02,
      "predicted_flops": 4.1623653699320259e+10,
      "predicted_flops_count": 2.0552089600000000e+08
    },
    {
      "name": "LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__BatchSize_256__2427895342727703615<CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:64/input[2]:112/input[3]:112/input[4]:-1/input[5]:-1/input[6]:-1/input[7]:-1/batch_size:256/manual_time",
      "iterations": 142,
      "real_time": 4.9244581400351208e+06,
      "cpu_time": 5.2875470915492699e+06,
      "time_unit": "ns",
      "items_per_second": 4.1734722918882248e+10,
      "activation_mode": 3.0000000000000000e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_activation_fwd.cpp": 1.4930345010129449e+19,
      "benchmark_func:void iLAYER_CUDNN_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3]": 1.0150096267486517e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_ACTIVATION_FWD_Impl": 1.7018446903751907e+19,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 1.1200000000000000e+02,
      "input[3]": 1.1200000000000000e+02,
      "input[4]": -1.0000000000000000e+00,
      "input[5]": -1.0000000000000000e+00,
      "input[6]": -1.0000000000000000e+00,
      "input[7]": -1.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 1.1200000000000000e+02,
      "input_size": 2.0552089600000000e+08,
      "input_width": 1.1200000000000000e+02,
      "num_iterations": 1.4200000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 1.1200000000000000e+02,
      "output_size": 2.0552089600000000e+08,
      "output_width": 1.1200000000000000e+02,
      "predicted_flops": 4.1734722918882248e+10,
      "predicted_flops_count": 2.0552089600000000e+08
    },
    {
      "name": "LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__BatchSize_256__14433999953897827071<CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/input[4]:-1/input[5]:-1/input[6]:-1/input[7]:-1/batch_size:256/manual_time",
      "iterations": 306,
      "real_time": 2.2882501498850831e+06,
      "cpu_time": 2.3708514379085070e+06,
      "time_unit": "ns",
      "items_per_second": 4.4907873383144180e+10,
      "activation_mode": 1.0000000000000000e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_activation_fwd.cpp": 1.4930345010129449e+19,
      "benchmark_func:void iLAYER_CUDNN_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1]": 1.0148130340695654e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_ACTIVATION_FWD_Impl": 1.7018446903751907e+19,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input[4]": -1.0000000000000000e+00,
      "input[5]": -1.0000000000000000e+00,
      "input[6]": -1.0000000000000000e+00,
      "input[7]": -1.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "num_iterations": 3.0600000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "predicted_flops": 4.4907873383144180e+10,
      "predicted_flops_count": 1.0276044800000000e+08
    },
    {
      "name": "LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__BatchSize_256__14433999953897827071<CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/input[4]:-1/input[5]:-1/input[6]:-1/input[7]:-1/batch_size:256/manual_time",
      "iterations": 306,
      "real_time": 2.2848781199784935e+06,
      "cpu_time": 2.3671985588236046e+06,
      "time_unit": "ns",
      "items_per_second": 4.4974148555883247e+10,
      "activation_mode": 3.0000000000000000e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_activation_fwd.cpp": 1.4930345010129449e+19,
      "benchmark_func:void iLAYER_CUDNN_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3]": 1.0150096267486517e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_ACTIVATION_FWD_Impl": 1.7018446903751907e+19,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input[4]": -1.0000000000000000e+00,
      "input[5]": -1.0000000000000000e+00,
      "input[6]": -1.0000000000000000e+00,
      "input[7]": -1.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "num_iterations": 3.0600000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "predicted_flops": 4.4974148555883247e+10,
      "predicted_flops_count": 1.0276044800000000e+08
    },
    {
      "name": "LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__BatchSize_256__8929215504087982075<CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/input[4]:-1/input[5]:-1/input[6]:-1/input[7]:-1/batch_size:256/manual_time",
      "iterations": 2783,
      "real_time": 2.5155074748689486e+05,
      "cpu_time": 2.5758349371177136e+05,
      "time_unit": "ns",
      "items_per_second": 5.1063477760761543e+10,
      "activation_mode": 1.0000000000000000e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_activation_fwd.cpp": 1.4930345010129449e+19,
      "benchmark_func:void iLAYER_CUDNN_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1]": 1.0148130340695654e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_ACTIVATION_FWD_Impl": 1.7018446903751907e+19,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input[4]": -1.0000000000000000e+00,
      "input[5]": -1.0000000000000000e+00,
      "input[6]": -1.0000000000000000e+00,
      "input[7]": -1.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "num_iterations": 2.7830000000000000e+03,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "predicted_flops": 5.1063477760761543e+10,
      "predicted_flops_count": 1.2845056000000000e+07
    },
    {
      "name": "LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__BatchSize_256__8929215504087982075<CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/input[4]:-1/input[5]:-1/input[6]:-1/input[7]:-1/batch_size:256/manual_time",
      "iterations": 2780,
      "real_time": 2.5179356626794412e+05,
      "cpu_time": 2.5784932266184440e+05,
      "time_unit": "ns",
      "items_per_second": 5.1014234360265724e+10,
      "activation_mode": 3.0000000000000000e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_activation_fwd.cpp": 1.4930345010129449e+19,
      "benchmark_func:void iLAYER_CUDNN_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3]": 1.0150096267486517e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_ACTIVATION_FWD_Impl": 1.7018446903751907e+19,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input[4]": -1.0000000000000000e+00,
      "input[5]": -1.0000000000000000e+00,
      "input[6]": -1.0000000000000000e+00,
      "input[7]": -1.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "num_iterations": 2.7800000000000000e+03,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "predicted_flops": 5.1014234360265724e+10,
      "predicted_flops_count": 1.2845056000000000e+07
    },
    {
      "name": "LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__BatchSize_256__12765051768404204215<CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/input[4]:-1/input[5]:-1/input[6]:-1/input[7]:-1/batch_size:256/manual_time",
      "iterations": 661,
      "real_time": 1.0577082433547606e+06,
      "cpu_time": 1.0794686248108321e+06,
      "time_unit": "ns",
      "items_per_second": 4.8576934445585884e+10,
      "activation_mode": 1.0000000000000000e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_activation_fwd.cpp": 1.4930345010129449e+19,
      "benchmark_func:void iLAYER_CUDNN_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1]": 1.0148130340695654e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_ACTIVATION_FWD_Impl": 1.7018446903751907e+19,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input[4]": -1.0000000000000000e+00,
      "input[5]": -1.0000000000000000e+00,
      "input[6]": -1.0000000000000000e+00,
      "input[7]": -1.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "num_iterations": 6.6100000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "predicted_flops": 4.8576934445585884e+10,
      "predicted_flops_count": 5.1380224000000000e+07
    },
    {
      "name": "LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__BatchSize_256__12765051768404204215<CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/input[4]:-1/input[5]:-1/input[6]:-1/input[7]:-1/batch_size:256/manual_time",
      "iterations": 662,
      "real_time": 1.0577397907414443e+06,
      "cpu_time": 1.0793961057403414e+06,
      "time_unit": "ns",
      "items_per_second": 4.8575485624856735e+10,
      "activation_mode": 3.0000000000000000e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_activation_fwd.cpp": 1.4930345010129449e+19,
      "benchmark_func:void iLAYER_CUDNN_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3]": 1.0150096267486517e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_ACTIVATION_FWD_Impl": 1.7018446903751907e+19,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input[4]": -1.0000000000000000e+00,
      "input[5]": -1.0000000000000000e+00,
      "input[6]": -1.0000000000000000e+00,
      "input[7]": -1.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "num_iterations": 6.6200000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "predicted_flops": 4.8575485624856735e+10,
      "predicted_flops_count": 5.1380224000000000e+07
    },
    {
      "name": "LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__BatchSize_256__10334936282238873727<CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/input[4]:-1/input[5]:-1/input[6]:-1/input[7]:-1/batch_size:256/manual_time",
      "iterations": 661,
      "real_time": 1.0577162312774858e+06,
      "cpu_time": 1.0793852118002740e+06,
      "time_unit": "ns",
      "items_per_second": 4.8576567590292267e+10,
      "activation_mode": 1.0000000000000000e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_activation_fwd.cpp": 1.4930345010129449e+19,
      "benchmark_func:void iLAYER_CUDNN_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1]": 1.0148130340695654e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_ACTIVATION_FWD_Impl": 1.7018446903751907e+19,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input[4]": -1.0000000000000000e+00,
      "input[5]": -1.0000000000000000e+00,
      "input[6]": -1.0000000000000000e+00,
      "input[7]": -1.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "num_iterations": 6.6100000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "predicted_flops": 4.8576567590292267e+10,
      "predicted_flops_count": 5.1380224000000000e+07
    },
    {
      "name": "LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__BatchSize_256__10334936282238873727<CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/input[4]:-1/input[5]:-1/input[6]:-1/input[7]:-1/batch_size:256/manual_time",
      "iterations": 662,
      "real_time": 1.0581312360301097e+06,
      "cpu_time": 1.0798146540785702e+06,
      "time_unit": "ns",
      "items_per_second": 4.8557515599641510e+10,
      "activation_mode": 3.0000000000000000e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_activation_fwd.cpp": 1.4930345010129449e+19,
      "benchmark_func:void iLAYER_CUDNN_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3]": 1.0150096267486517e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_ACTIVATION_FWD_Impl": 1.7018446903751907e+19,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input[4]": -1.0000000000000000e+00,
      "input[5]": -1.0000000000000000e+00,
      "input[6]": -1.0000000000000000e+00,
      "input[7]": -1.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "num_iterations": 6.6200000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "predicted_flops": 4.8557515599641510e+10,
      "predicted_flops_count": 5.1380224000000000e+07
    },
    {
      "name": "LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__BatchSize_256__13874940523211836359<CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/input[4]:-1/input[5]:-1/input[6]:-1/input[7]:-1/batch_size:256/manual_time",
      "iterations": 5458,
      "real_time": 1.2825808764889494e+05,
      "cpu_time": 1.3362437944297891e+05,
      "time_unit": "ns",
      "items_per_second": 5.0075033221932930e+10,
      "activation_mode": 1.0000000000000000e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_activation_fwd.cpp": 1.4930345010129449e+19,
      "benchmark_func:void iLAYER_CUDNN_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1]": 1.0148130340695654e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_ACTIVATION_FWD_Impl": 1.7018446903751907e+19,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input[4]": -1.0000000000000000e+00,
      "input[5]": -1.0000000000000000e+00,
      "input[6]": -1.0000000000000000e+00,
      "input[7]": -1.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "num_iterations": 5.4580000000000000e+03,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "predicted_flops": 5.0075033221932930e+10,
      "predicted_flops_count": 6.4225280000000000e+06
    },
    {
      "name": "LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__BatchSize_256__13874940523211836359<CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/input[4]:-1/input[5]:-1/input[6]:-1/input[7]:-1/batch_size:256/manual_time",
      "iterations": 5445,
      "real_time": 1.2853287600936099e+05,
      "cpu_time": 1.3390128613409869e+05,
      "time_unit": "ns",
      "items_per_second": 4.9967978616865692e+10,
      "activation_mode": 3.0000000000000000e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_activation_fwd.cpp": 1.4930345010129449e+19,
      "benchmark_func:void iLAYER_CUDNN_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3]": 1.0150096267486517e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_ACTIVATION_FWD_Impl": 1.7018446903751907e+19,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input[4]": -1.0000000000000000e+00,
      "input[5]": -1.0000000000000000e+00,
      "input[6]": -1.0000000000000000e+00,
      "input[7]": -1.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "num_iterations": 5.4450000000000000e+03,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "predicted_flops": 4.9967978616865692e+10,
      "predicted_flops_count": 6.4225280000000000e+06
    },
    {
      "name": "LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__BatchSize_256__12912851417108060376<CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/input[4]:-1/input[5]:-1/input[6]:-1/input[7]:-1/batch_size:256/manual_time",
      "iterations": 1388,
      "real_time": 5.0478163166058861e+05,
      "cpu_time": 5.1363302089336264e+05,
      "time_unit": "ns",
      "items_per_second": 5.0893515906049919e+10,
      "activation_mode": 1.0000000000000000e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_activation_fwd.cpp": 1.4930345010129449e+19,
      "benchmark_func:void iLAYER_CUDNN_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1]": 1.0148130340695654e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_ACTIVATION_FWD_Impl": 1.7018446903751907e+19,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.0480000000000000e+03,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input[4]": -1.0000000000000000e+00,
      "input[5]": -1.0000000000000000e+00,
      "input[6]": -1.0000000000000000e+00,
      "input[7]": -1.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.0480000000000000e+03,
      "input_height": 7.0000000000000000e+00,
      "input_size": 2.5690112000000000e+07,
      "input_width": 7.0000000000000000e+00,
      "num_iterations": 1.3880000000000000e+03,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "predicted_flops": 5.0893515906049919e+10,
      "predicted_flops_count": 2.5690112000000000e+07
    },
    {
      "name": "LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__BatchSize_256__12912851417108060376<CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/input[4]:-1/input[5]:-1/input[6]:-1/input[7]:-1/batch_size:256/manual_time",
      "iterations": 1387,
      "real_time": 5.0513996298994974e+05,
      "cpu_time": 5.1402406344627886e+05,
      "time_unit": "ns",
      "items_per_second": 5.0857413553144539e+10,
      "activation_mode": 3.0000000000000000e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_activation_fwd.cpp": 1.4930345010129449e+19,
      "benchmark_func:void iLAYER_CUDNN_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3]": 1.0150096267486517e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_ACTIVATION_FWD_Impl": 1.7018446903751907e+19,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.0480000000000000e+03,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input[4]": -1.0000000000000000e+00,
      "input[5]": -1.0000000000000000e+00,
      "input[6]": -1.0000000000000000e+00,
      "input[7]": -1.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.0480000000000000e+03,
      "input_height": 7.0000000000000000e+00,
      "input_size": 2.5690112000000000e+07,
      "input_width": 7.0000000000000000e+00,
      "num_iterations": 1.3870000000000000e+03,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "predicted_flops": 5.0857413553144539e+10,
      "predicted_flops_count": 2.5690112000000000e+07
    },
    {
      "name": "LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__BatchSize_256__14901140743263269465<CUDNN_BATCHNORM_PER_ACTIVATION>/input[0]:256/input[1]:64/input[2]:112/input[3]:112/input[4]:-1/input[5]:-1/input[6]:-1/input[7]:-1/batch_size:256/manual_time",
      "iterations": 91,
      "real_time": 7.6403288416318838e+06,
      "cpu_time": 8.4875220329672527e+06,
      "time_unit": "ns",
      "items_per_second": 2.6899483027500576e+10,
      "batch_size": 2.5600000000000000e+02,
      "batchnorm_mode": 0.0000000000000000e+00,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_batchnorm_fwd.cpp": 1.2850267679440775e+19,
      "benchmark_func:void iLAYER_CUDNN_BATCHNORM_FWD_Impl(benchmark::State&) [with T = float; cudnnBatchNormMode_t batchnorm_mode = (cudnnBatchNormMode_t)0; bool is_training = false]": 1.4627515035010154e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_BATCHNORM_FWD_Impl": 2.4394916016563661e+18,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 1.1200000000000000e+02,
      "input[3]": 1.1200000000000000e+02,
      "input[4]": -1.0000000000000000e+00,
      "input[5]": -1.0000000000000000e+00,
      "input[6]": -1.0000000000000000e+00,
      "input[7]": -1.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 1.1200000000000000e+02,
      "input_size": 2.0552089600000000e+08,
      "input_width": 1.1200000000000000e+02,
      "is_training": 0.0000000000000000e+00,
      "num_iterations": 9.1000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 1.1200000000000000e+02,
      "output_size": 2.0552089600000000e+08,
      "output_width": 1.1200000000000000e+02,
      "predicted_flops": 2.6899483027500576e+10,
      "predicted_flops_count": 2.0552089600000000e+08
    },
    {
      "name": "LAYER_CUDNN_BATCHNORM_FWD_TRAINING_FLOAT32__BatchSize_256__3170781988685782094<CUDNN_BATCHNORM_PER_ACTIVATION>/input[0]:256/input[1]:64/input[2]:112/input[3]:112/input[4]:-1/input[5]:-1/input[6]:-1/input[7]:-1/batch_size:256/manual_time",
      "iterations": 59,
      "real_time": 1.1876231485630495e+07,
      "cpu_time": 1.3892385355931839e+07,
      "time_unit": "ns",
      "items_per_second": 1.7305228198748699e+10,
      "batch_size": 2.5600000000000000e+02,
      "batchnorm_mode": 0.0000000000000000e+00,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_batchnorm_fwd.cpp": 1.2850267679440775e+19,
      "benchmark_func:void iLAYER_CUDNN_BATCHNORM_FWD_Impl(benchmark::State&) [with T = float; cudnnBatchNormMode_t batchnorm_mode = (cudnnBatchNormMode_t)0; bool is_training = true]": 9.0033003156525926e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_BATCHNORM_FWD_Impl": 2.4394916016563661e+18,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 1.1200000000000000e+02,
      "input[3]": 1.1200000000000000e+02,
      "input[4]": -1.0000000000000000e+00,
      "input[5]": -1.0000000000000000e+00,
      "input[6]": -1.0000000000000000e+00,
      "input[7]": -1.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 1.1200000000000000e+02,
      "input_size": 2.0552089600000000e+08,
      "input_width": 1.1200000000000000e+02,
      "is_training": 1.0000000000000000e+00,
      "num_iterations": 5.9000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 1.1200000000000000e+02,
      "output_size": 2.0552089600000000e+08,
      "output_width": 1.1200000000000000e+02,
      "predicted_flops": 1.7305228198748699e+10,
      "predicted_flops_count": 2.0552089600000000e+08
    },
    {
      "name": "LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__BatchSize_256__6799436594138236113<CUDNN_BATCHNORM_PER_ACTIVATION>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/input[4]:-1/input[5]:-1/input[6]:-1/input[7]:-1/batch_size:256/manual_time",
      "iterations": 364,
      "real_time": 1.9134590034449860e+06,
      "cpu_time": 1.9712386153846832e+06,
      "time_unit": "ns",
      "items_per_second": 2.6852011936234428e+10,
      "batch_size": 2.5600000000000000e+02,
      "batchnorm_mode": 0.0000000000000000e+00,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_batchnorm_fwd.cpp": 1.2850267679440775e+19,
      "benchmark_func:void iLAYER_CUDNN_BATCHNORM_FWD_Impl(benchmark::State&) [with T = float; cudnnBatchNormMode_t batchnorm_mode = (cudnnBatchNormMode_t)0; bool is_training = false]": 1.4627515035010154e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_BATCHNORM_FWD_Impl": 2.4394916016563661e+18,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input[4]": -1.0000000000000000e+00,
      "input[5]": -1.0000000000000000e+00,
      "input[6]": -1.0000000000000000e+00,
      "input[7]": -1.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "is_training": 0.0000000000000000e+00,
      "num_iterations": 3.6400000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "predicted_flops": 2.6852011936234428e+10,
      "predicted_flops_count": 5.1380224000000000e+07
    },
    {
      "name": "LAYER_CUDNN_BATCHNORM_FWD_TRAINING_FLOAT32__BatchSize_256__13589592546425367238<CUDNN_BATCHNORM_PER_ACTIVATION>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/input[4]:-1/input[5]:-1/input[6]:-1/input[7]:-1/batch_size:256/manual_time",
      "iterations": 242,
      "real_time": 2.9009008810909325e+06,
      "cpu_time": 3.0261657355371956e+06,
      "time_unit": "ns",
      "items_per_second": 1.7711816468778347e+10,
      "batch_size": 2.5600000000000000e+02,
      "batchnorm_mode": 0.0000000000000000e+00,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_batchnorm_fwd.cpp": 1.2850267679440775e+19,
      "benchmark_func:void iLAYER_CUDNN_BATCHNORM_FWD_Impl(benchmark::State&) [with T = float; cudnnBatchNormMode_t batchnorm_mode = (cudnnBatchNormMode_t)0; bool is_training = true]": 9.0033003156525926e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_BATCHNORM_FWD_Impl": 2.4394916016563661e+18,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input[4]": -1.0000000000000000e+00,
      "input[5]": -1.0000000000000000e+00,
      "input[6]": -1.0000000000000000e+00,
      "input[7]": -1.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "is_training": 1.0000000000000000e+00,
      "num_iterations": 2.4200000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "predicted_flops": 1.7711816468778347e+10,
      "predicted_flops_count": 5.1380224000000000e+07
    },
    {
      "name": "LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__BatchSize_256__2825233640362292377<CUDNN_BATCHNORM_PER_ACTIVATION>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/input[4]:-1/input[5]:-1/input[6]:-1/input[7]:-1/batch_size:256/manual_time",
      "iterations": 183,
      "real_time": 3.8450853733722465e+06,
      "cpu_time": 4.0609980928962063e+06,
      "time_unit": "ns",
      "items_per_second": 2.6725140802238216e+10,
      "batch_size": 2.5600000000000000e+02,
      "batchnorm_mode": 0.0000000000000000e+00,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_batchnorm_fwd.cpp": 1.2850267679440775e+19,
      "benchmark_func:void iLAYER_CUDNN_BATCHNORM_FWD_Impl(benchmark::State&) [with T = float; cudnnBatchNormMode_t batchnorm_mode = (cudnnBatchNormMode_t)0; bool is_training = false]": 1.4627515035010154e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_BATCHNORM_FWD_Impl": 2.4394916016563661e+18,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input[4]": -1.0000000000000000e+00,
      "input[5]": -1.0000000000000000e+00,
      "input[6]": -1.0000000000000000e+00,
      "input[7]": -1.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "is_training": 0.0000000000000000e+00,
      "num_iterations": 1.8300000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "predicted_flops": 2.6725140802238216e+10,
      "predicted_flops_count": 1.0276044800000000e+08
    },
    {
      "name": "LAYER_CUDNN_BATCHNORM_FWD_TRAINING_FLOAT32__BatchSize_256__14267019768968714894<CUDNN_BATCHNORM_PER_ACTIVATION>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/input[4]:-1/input[5]:-1/input[6]:-1/input[7]:-1/batch_size:256/manual_time",
      "iterations": 120,
      "real_time": 5.8629466531177359e+06,
      "cpu_time": 6.3574409250004869e+06,
      "time_unit": "ns",
      "items_per_second": 1.7527099269333302e+10,
      "batch_size": 2.5600000000000000e+02,
      "batchnorm_mode": 0.0000000000000000e+00,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_batchnorm_fwd.cpp": 1.2850267679440775e+19,
      "benchmark_func:void iLAYER_CUDNN_BATCHNORM_FWD_Impl(benchmark::State&) [with T = float; cudnnBatchNormMode_t batchnorm_mode = (cudnnBatchNormMode_t)0; bool is_training = true]": 9.0033003156525926e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_BATCHNORM_FWD_Impl": 2.4394916016563661e+18,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input[4]": -1.0000000000000000e+00,
      "input[5]": -1.0000000000000000e+00,
      "input[6]": -1.0000000000000000e+00,
      "input[7]": -1.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "is_training": 1.0000000000000000e+00,
      "num_iterations": 1.2000000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "predicted_flops": 1.7527099269333302e+10,
      "predicted_flops_count": 1.0276044800000000e+08
    },
    {
      "name": "LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__BatchSize_256__3456320959234200993<CUDNN_BATCHNORM_PER_ACTIVATION>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/input[4]:-1/input[5]:-1/input[6]:-1/input[7]:-1/batch_size:256/manual_time",
      "iterations": 4998,
      "real_time": 1.4010099315748070e+05,
      "cpu_time": 1.4546291896758656e+05,
      "time_unit": "ns",
      "items_per_second": 4.5842130417881828e+10,
      "batch_size": 2.5600000000000000e+02,
      "batchnorm_mode": 0.0000000000000000e+00,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_batchnorm_fwd.cpp": 1.2850267679440775e+19,
      "benchmark_func:void iLAYER_CUDNN_BATCHNORM_FWD_Impl(benchmark::State&) [with T = float; cudnnBatchNormMode_t batchnorm_mode = (cudnnBatchNormMode_t)0; bool is_training = false]": 1.4627515035010154e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_BATCHNORM_FWD_Impl": 2.4394916016563661e+18,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input[4]": -1.0000000000000000e+00,
      "input[5]": -1.0000000000000000e+00,
      "input[6]": -1.0000000000000000e+00,
      "input[7]": -1.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "is_training": 0.0000000000000000e+00,
      "num_iterations": 4.9980000000000000e+03,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "predicted_flops": 4.5842130417881828e+10,
      "predicted_flops_count": 6.4225280000000000e+06
    },
    {
      "name": "LAYER_CUDNN_BATCHNORM_FWD_TRAINING_FLOAT32__BatchSize_256__14788920112504667062<CUDNN_BATCHNORM_PER_ACTIVATION>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/input[4]:-1/input[5]:-1/input[6]:-1/input[7]:-1/batch_size:256/manual_time",
      "iterations": 2749,
      "real_time": 2.5415321896814837e+05,
      "cpu_time": 2.6022439578024251e+05,
      "time_unit": "ns",
      "items_per_second": 2.5270299648673347e+10,
      "batch_size": 2.5600000000000000e+02,
      "batchnorm_mode": 0.0000000000000000e+00,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_batchnorm_fwd.cpp": 1.2850267679440775e+19,
      "benchmark_func:void iLAYER_CUDNN_BATCHNORM_FWD_Impl(benchmark::State&) [with T = float; cudnnBatchNormMode_t batchnorm_mode = (cudnnBatchNormMode_t)0; bool is_training = true]": 9.0033003156525926e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_BATCHNORM_FWD_Impl": 2.4394916016563661e+18,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input[4]": -1.0000000000000000e+00,
      "input[5]": -1.0000000000000000e+00,
      "input[6]": -1.0000000000000000e+00,
      "input[7]": -1.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "is_training": 1.0000000000000000e+00,
      "num_iterations": 2.7490000000000000e+03,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "predicted_flops": 2.5270299648673347e+10,
      "predicted_flops_count": 6.4225280000000000e+06
    },
    {
      "name": "LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__BatchSize_256__16857728998690705125<CUDNN_BATCHNORM_PER_ACTIVATION>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/input[4]:-1/input[5]:-1/input[6]:-1/input[7]:-1/batch_size:256/manual_time",
      "iterations": 92,
      "real_time": 7.6237102950234776e+06,
      "cpu_time": 8.4562739891304262e+06,
      "time_unit": "ns",
      "items_per_second": 2.6958119871653267e+10,
      "batch_size": 2.5600000000000000e+02,
      "batchnorm_mode": 0.0000000000000000e+00,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_batchnorm_fwd.cpp": 1.2850267679440775e+19,
      "benchmark_func:void iLAYER_CUDNN_BATCHNORM_FWD_Impl(benchmark::State&) [with T = float; cudnnBatchNormMode_t batchnorm_mode = (cudnnBatchNormMode_t)0; bool is_training = false]": 1.4627515035010154e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_BATCHNORM_FWD_Impl": 2.4394916016563661e+18,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input[4]": -1.0000000000000000e+00,
      "input[5]": -1.0000000000000000e+00,
      "input[6]": -1.0000000000000000e+00,
      "input[7]": -1.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "is_training": 0.0000000000000000e+00,
      "num_iterations": 9.2000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 5.6000000000000000e+01,
      "output_size": 2.0552089600000000e+08,
      "output_width": 5.6000000000000000e+01,
      "predicted_flops": 2.6958119871653267e+10,
      "predicted_flops_count": 2.0552089600000000e+08
    },
    {
      "name": "LAYER_CUDNN_BATCHNORM_FWD_TRAINING_FLOAT32__BatchSize_256__808733478892790002<CUDNN_BATCHNORM_PER_ACTIVATION>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/input[4]:-1/input[5]:-1/input[6]:-1/input[7]:-1/batch_size:256/manual_time",
      "iterations": 59,
      "real_time": 1.1854936873887556e+07,
      "cpu_time": 1.3864688322033372e+07,
      "time_unit": "ns",
      "items_per_second": 1.7336312979674614e+10,
      "batch_size": 2.5600000000000000e+02,
      "batchnorm_mode": 0.0000000000000000e+00,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_batchnorm_fwd.cpp": 1.2850267679440775e+19,
      "benchmark_func:void iLAYER_CUDNN_BATCHNORM_FWD_Impl(benchmark::State&) [with T = float; cudnnBatchNormMode_t batchnorm_mode = (cudnnBatchNormMode_t)0; bool is_training = true]": 9.0033003156525926e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_BATCHNORM_FWD_Impl": 2.4394916016563661e+18,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input[4]": -1.0000000000000000e+00,
      "input[5]": -1.0000000000000000e+00,
      "input[6]": -1.0000000000000000e+00,
      "input[7]": -1.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "is_training": 1.0000000000000000e+00,
      "num_iterations": 5.9000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 5.6000000000000000e+01,
      "output_size": 2.0552089600000000e+08,
      "output_width": 5.6000000000000000e+01,
      "predicted_flops": 1.7336312979674614e+10,
      "predicted_flops_count": 2.0552089600000000e+08
    },
    {
      "name": "LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__BatchSize_256__12104039157700183270<CUDNN_BATCHNORM_PER_ACTIVATION>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/input[4]:-1/input[5]:-1/input[6]:-1/input[7]:-1/batch_size:256/manual_time",
      "iterations": 730,
      "real_time": 9.6027194370821828e+05,
      "cpu_time": 9.7854251095878053e+05,
      "time_unit": "ns",
      "items_per_second": 2.6752954898165829e+10,
      "batch_size": 2.5600000000000000e+02,
      "batchnorm_mode": 0.0000000000000000e+00,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_batchnorm_fwd.cpp": 1.2850267679440775e+19,
      "benchmark_func:void iLAYER_CUDNN_BATCHNORM_FWD_Impl(benchmark::State&) [with T = float; cudnnBatchNormMode_t batchnorm_mode = (cudnnBatchNormMode_t)0; bool is_training = false]": 1.4627515035010154e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_BATCHNORM_FWD_Impl": 2.4394916016563661e+18,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input[4]": -1.0000000000000000e+00,
      "input[5]": -1.0000000000000000e+00,
      "input[6]": -1.0000000000000000e+00,
      "input[7]": -1.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "is_training": 0.0000000000000000e+00,
      "num_iterations": 7.3000000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "predicted_flops": 2.6752954898165829e+10,
      "predicted_flops_count": 2.5690112000000000e+07
    },
    {
      "name": "LAYER_CUDNN_BATCHNORM_FWD_TRAINING_FLOAT32__BatchSize_256__4985962657501637361<CUDNN_BATCHNORM_PER_ACTIVATION>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/input[4]:-1/input[5]:-1/input[6]:-1/input[7]:-1/batch_size:256/manual_time",
      "iterations": 491,
      "real_time": 1.4236268787521513e+06,
      "cpu_time": 1.4576458105908039e+06,
      "time_unit": "ns",
      "items_per_second": 1.8045537340878323e+10,
      "batch_size": 2.5600000000000000e+02,
      "batchnorm_mode": 0.0000000000000000e+00,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_batchnorm_fwd.cpp": 1.2850267679440775e+19,
      "benchmark_func:void iLAYER_CUDNN_BATCHNORM_FWD_Impl(benchmark::State&) [with T = float; cudnnBatchNormMode_t batchnorm_mode = (cudnnBatchNormMode_t)0; bool is_training = true]": 9.0033003156525926e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_BATCHNORM_FWD_Impl": 2.4394916016563661e+18,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input[4]": -1.0000000000000000e+00,
      "input[5]": -1.0000000000000000e+00,
      "input[6]": -1.0000000000000000e+00,
      "input[7]": -1.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "is_training": 1.0000000000000000e+00,
      "num_iterations": 4.9100000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "predicted_flops": 1.8045537340878323e+10,
      "predicted_flops_count": 2.5690112000000000e+07
    },
    {
      "name": "LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__BatchSize_256__10705077531754725789<CUDNN_BATCHNORM_PER_ACTIVATION>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/input[4]:-1/input[5]:-1/input[6]:-1/input[7]:-1/batch_size:256/manual_time",
      "iterations": 2400,
      "real_time": 2.9174138576005743e+05,
      "cpu_time": 2.9807642083316209e+05,
      "time_unit": "ns",
      "items_per_second": 4.4028912684210022e+10,
      "batch_size": 2.5600000000000000e+02,
      "batchnorm_mode": 0.0000000000000000e+00,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_batchnorm_fwd.cpp": 1.2850267679440775e+19,
      "benchmark_func:void iLAYER_CUDNN_BATCHNORM_FWD_Impl(benchmark::State&) [with T = float; cudnnBatchNormMode_t batchnorm_mode = (cudnnBatchNormMode_t)0; bool is_training = false]": 1.4627515035010154e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_BATCHNORM_FWD_Impl": 2.4394916016563661e+18,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input[4]": -1.0000000000000000e+00,
      "input[5]": -1.0000000000000000e+00,
      "input[6]": -1.0000000000000000e+00,
      "input[7]": -1.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "is_training": 0.0000000000000000e+00,
      "num_iterations": 2.4000000000000000e+03,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "predicted_flops": 4.4028912684210022e+10,
      "predicted_flops_count": 1.2845056000000000e+07
    },
    {
      "name": "LAYER_CUDNN_BATCHNORM_FWD_TRAINING_FLOAT32__BatchSize_256__8528567176743726986<CUDNN_BATCHNORM_PER_ACTIVATION>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/input[4]:-1/input[5]:-1/input[6]:-1/input[7]:-1/batch_size:256/manual_time",
      "iterations": 1345,
      "real_time": 5.1997897230767302e+05,
      "cpu_time": 5.2898003866167681e+05,
      "time_unit": "ns",
      "items_per_second": 2.4703029707131207e+10,
      "batch_size": 2.5600000000000000e+02,
      "batchnorm_mode": 0.0000000000000000e+00,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_batchnorm_fwd.cpp": 1.2850267679440775e+19,
      "benchmark_func:void iLAYER_CUDNN_BATCHNORM_FWD_Impl(benchmark::State&) [with T = float; cudnnBatchNormMode_t batchnorm_mode = (cudnnBatchNormMode_t)0; bool is_training = true]": 9.0033003156525926e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_BATCHNORM_FWD_Impl": 2.4394916016563661e+18,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input[4]": -1.0000000000000000e+00,
      "input[5]": -1.0000000000000000e+00,
      "input[6]": -1.0000000000000000e+00,
      "input[7]": -1.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "is_training": 1.0000000000000000e+00,
      "num_iterations": 1.3450000000000000e+03,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "predicted_flops": 2.4703029707131207e+10,
      "predicted_flops_count": 1.2845056000000000e+07
    },
    {
      "name": "LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__BatchSize_256__6924267623131016729<CUDNN_BATCHNORM_PER_ACTIVATION>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/input[4]:-1/input[5]:-1/input[6]:-1/input[7]:-1/batch_size:256/manual_time",
      "iterations": 366,
      "real_time": 1.9202850958780269e+06,
      "cpu_time": 1.9776370655737356e+06,
      "time_unit": "ns",
      "items_per_second": 2.6756560320282558e+10,
      "batch_size": 2.5600000000000000e+02,
      "batchnorm_mode": 0.0000000000000000e+00,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_batchnorm_fwd.cpp": 1.2850267679440775e+19,
      "benchmark_func:void iLAYER_CUDNN_BATCHNORM_FWD_Impl(benchmark::State&) [with T = float; cudnnBatchNormMode_t batchnorm_mode = (cudnnBatchNormMode_t)0; bool is_training = false]": 1.4627515035010154e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_BATCHNORM_FWD_Impl": 2.4394916016563661e+18,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input[4]": -1.0000000000000000e+00,
      "input[5]": -1.0000000000000000e+00,
      "input[6]": -1.0000000000000000e+00,
      "input[7]": -1.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "is_training": 0.0000000000000000e+00,
      "num_iterations": 3.6600000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "predicted_flops": 2.6756560320282558e+10,
      "predicted_flops_count": 5.1380224000000000e+07
    },
    {
      "name": "LAYER_CUDNN_BATCHNORM_FWD_TRAINING_FLOAT32__BatchSize_256__9429531921578868750<CUDNN_BATCHNORM_PER_ACTIVATION>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/input[4]:-1/input[5]:-1/input[6]:-1/input[7]:-1/batch_size:256/manual_time",
      "iterations": 242,
      "real_time": 2.8979151263402021e+06,
      "cpu_time": 3.0225872355373497e+06,
      "time_unit": "ns",
      "items_per_second": 1.7730065153732937e+10,
      "batch_size": 2.5600000000000000e+02,
      "batchnorm_mode": 0.0000000000000000e+00,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_batchnorm_fwd.cpp": 1.2850267679440775e+19,
      "benchmark_func:void iLAYER_CUDNN_BATCHNORM_FWD_Impl(benchmark::State&) [with T = float; cudnnBatchNormMode_t batchnorm_mode = (cudnnBatchNormMode_t)0; bool is_training = true]": 9.0033003156525926e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_BATCHNORM_FWD_Impl": 2.4394916016563661e+18,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input[4]": -1.0000000000000000e+00,
      "input[5]": -1.0000000000000000e+00,
      "input[6]": -1.0000000000000000e+00,
      "input[7]": -1.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "is_training": 1.0000000000000000e+00,
      "num_iterations": 2.4200000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "predicted_flops": 1.7730065153732937e+10,
      "predicted_flops_count": 5.1380224000000000e+07
    },
    {
      "name": "LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__BatchSize_256__6649926107756922558<CUDNN_BATCHNORM_PER_ACTIVATION>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/input[4]:-1/input[5]:-1/input[6]:-1/input[7]:-1/batch_size:256/manual_time",
      "iterations": 729,
      "real_time": 9.5684020050235395e+05,
      "cpu_time": 9.7509778189288033e+05,
      "time_unit": "ns",
      "items_per_second": 2.6848905372613262e+10,
      "batch_size": 2.5600000000000000e+02,
      "batchnorm_mode": 0.0000000000000000e+00,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_batchnorm_fwd.cpp": 1.2850267679440775e+19,
      "benchmark_func:void iLAYER_CUDNN_BATCHNORM_FWD_Impl(benchmark::State&) [with T = float; cudnnBatchNormMode_t batchnorm_mode = (cudnnBatchNormMode_t)0; bool is_training = false]": 1.4627515035010154e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_BATCHNORM_FWD_Impl": 2.4394916016563661e+18,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.0480000000000000e+03,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input[4]": -1.0000000000000000e+00,
      "input[5]": -1.0000000000000000e+00,
      "input[6]": -1.0000000000000000e+00,
      "input[7]": -1.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.0480000000000000e+03,
      "input_height": 7.0000000000000000e+00,
      "input_size": 2.5690112000000000e+07,
      "input_width": 7.0000000000000000e+00,
      "is_training": 0.0000000000000000e+00,
      "num_iterations": 7.2900000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "predicted_flops": 2.6848905372613262e+10,
      "predicted_flops_count": 2.5690112000000000e+07
    },
    {
      "name": "LAYER_CUDNN_BATCHNORM_FWD_TRAINING_FLOAT32__BatchSize_256__13727769268826581161<CUDNN_BATCHNORM_PER_ACTIVATION>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/input[4]:-1/input[5]:-1/input[6]:-1/input[7]:-1/batch_size:256/manual_time",
      "iterations": 490,
      "real_time": 1.4240278829155224e+06,
      "cpu_time": 1.4583283448979096e+06,
      "time_unit": "ns",
      "items_per_second": 1.8040455744028442e+10,
      "batch_size": 2.5600000000000000e+02,
      "batchnorm_mode": 0.0000000000000000e+00,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_batchnorm_fwd.cpp": 1.2850267679440775e+19,
      "benchmark_func:void iLAYER_CUDNN_BATCHNORM_FWD_Impl(benchmark::State&) [with T = float; cudnnBatchNormMode_t batchnorm_mode = (cudnnBatchNormMode_t)0; bool is_training = true]": 9.0033003156525926e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_BATCHNORM_FWD_Impl": 2.4394916016563661e+18,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.0480000000000000e+03,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input[4]": -1.0000000000000000e+00,
      "input[5]": -1.0000000000000000e+00,
      "input[6]": -1.0000000000000000e+00,
      "input[7]": -1.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.0480000000000000e+03,
      "input_height": 7.0000000000000000e+00,
      "input_size": 2.5690112000000000e+07,
      "input_width": 7.0000000000000000e+00,
      "is_training": 1.0000000000000000e+00,
      "num_iterations": 4.9000000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "predicted_flops": 1.8040455744028442e+10,
      "predicted_flops_count": 2.5690112000000000e+07
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__11288739203476491984<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 107,
      "real_time": 6.5474362138693575e+06,
      "cpu_time": 7.1802516635514209e+06,
      "time_unit": "ns",
      "items_per_second": 2.0089294365537214e+12,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 6.5170559883117676e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4730153338016043e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 1.0700000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 5.6000000000000000e+01,
      "output_size": 2.0552089600000000e+08,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 2.0089294365537214e+12,
      "predicted_advised_flops_count": 1.3153337344000000e+10,
      "predicted_flops": 2.0089294365537214e+12,
      "predicted_flops_count": 1.3153337344000000e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__11288739203476491984<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 150,
      "real_time": 4.6634410787373781e+06,
      "cpu_time": 4.9913752933333432e+06,
      "time_unit": "ns",
      "items_per_second": 2.8205218253902017e+12,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.8824000000000000e+04,
      "advised_time": 4.8486399650573730e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.2419525699096977e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 1.5000000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 5.6000000000000000e+01,
      "output_size": 2.0552089600000000e+08,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 2.8205218253902017e+12,
      "predicted_advised_flops_count": 1.3153337344000000e+10,
      "predicted_flops": 2.8205218253902017e+12,
      "predicted_flops_count": 1.3153337344000000e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.8824000000000000e+04,
      "workspace_megabytes": 1.7951965332031250e-02
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__11288739203476491984<CUDNN_CONVOLUTION_FWD_ALGO_GEMM>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 85,
      "real_time": 8.2613741759868227e+06,
      "cpu_time": 9.2571374235292822e+06,
      "time_unit": "ns",
      "items_per_second": 1.5921488439819797e+12,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.0552089600000000e+08,
      "advised_time": 8.2800636291503906e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4280899851402570e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 8.5000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 5.6000000000000000e+01,
      "output_size": 2.0552089600000000e+08,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 1.5921488439819797e+12,
      "predicted_advised_flops_count": 1.3153337344000000e+10,
      "predicted_flops": 1.5921488439819797e+12,
      "predicted_flops_count": 1.3153337344000000e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 2.0552089600000000e+08,
      "workspace_megabytes": 1.9600000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__11288739203476491984<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__11288739203476491984<CUDNN_CONVOLUTION_FWD_ALGO_FFT>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 30,
      "real_time": 2.3489710999031860e+07,
      "cpu_time": 3.1380865166666660e+07,
      "time_unit": "ns",
      "items_per_second": 5.5996165063683081e+11,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.2146908160000000e+09,
      "advised_time": 2.3474176406860352e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnMathType_t math_type = (cudnnMathType_t)0]": 2.3237108083472476e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 3.0000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 5.6000000000000000e+01,
      "output_size": 2.0552089600000000e+08,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 5.5996165063683081e+11,
      "predicted_advised_flops_count": 1.3153337344000000e+10,
      "predicted_flops": 7.1239427788632910e+11,
      "predicted_flops_count": 1.6733935704913866e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 2.2146908160000000e+09,
      "workspace_megabytes": 2.1120937500000000e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__11288739203476491984<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 44,
      "real_time": 1.6066435893828219e+07,
      "cpu_time": 1.9764901954545166e+07,
      "time_unit": "ns",
      "items_per_second": 8.1868420793019434e+11,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.2500344960000000e+09,
      "advised_time": 1.6408575057983398e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.6667819325789325e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 4.4000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 5.6000000000000000e+01,
      "output_size": 2.0552089600000000e+08,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 8.1868420793019434e+11,
      "predicted_advised_flops_count": 1.3153337344000000e+10,
      "predicted_flops": 1.0415462281427371e+12,
      "predicted_flops_count": 1.6733935704913866e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.2500344960000000e+09,
      "workspace_megabytes": 1.1921257934570312e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__11288739203476491984<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__11288739203476491984<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__558036841815313172<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 111,
      "real_time": 6.2888003256473988e+06,
      "cpu_time": 6.8635612792792860e+06,
      "time_unit": "ns",
      "items_per_second": 2.0915495266016309e+12,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 6.3170561790466309e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4730153338016043e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 1.1100000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 2.0915495266016309e+12,
      "predicted_advised_flops_count": 1.3153337344000000e+10,
      "predicted_flops": 2.0915495266016309e+12,
      "predicted_flops_count": 1.3153337344000000e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__558036841815313172<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 179,
      "real_time": 3.9062249960700395e+06,
      "cpu_time": 4.1315246871505291e+06,
      "time_unit": "ns",
      "items_per_second": 3.3672759140175649e+12,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.8824000000000000e+04,
      "advised_time": 3.9403519630432129e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.2419525699096977e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 1.7900000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 3.3672759140175649e+12,
      "predicted_advised_flops_count": 1.3153337344000000e+10,
      "predicted_flops": 3.3672759140175649e+12,
      "predicted_flops_count": 1.3153337344000000e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.8824000000000000e+04,
      "workspace_megabytes": 1.7951965332031250e-02
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__558036841815313172<CUDNN_CONVOLUTION_FWD_ALGO_GEMM>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 74,
      "real_time": 9.5005923680759761e+06,
      "cpu_time": 1.0794447283783644e+07,
      "time_unit": "ns",
      "items_per_second": 1.3844754973593044e+12,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 8.2208358400000000e+08,
      "advised_time": 9.4873600006103516e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4280899851402570e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 7.4000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 1.3844754973593044e+12,
      "predicted_advised_flops_count": 1.3153337344000000e+10,
      "predicted_flops": 1.3844754973593044e+12,
      "predicted_flops_count": 1.3153337344000000e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 8.2208358400000000e+08,
      "workspace_megabytes": 7.8400000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__558036841815313172<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__558036841815313172<CUDNN_CONVOLUTION_FWD_ALGO_FFT>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 31,
      "real_time": 2.2229813640156101e+07,
      "cpu_time": 2.9415419903226014e+07,
      "time_unit": "ns",
      "items_per_second": 5.9169804825712598e+11,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.7683389440000000e+09,
      "advised_time": 2.3066368103027344e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnMathType_t math_type = (cudnnMathType_t)0]": 2.3237108083472476e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 3.1000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 5.9169804825712598e+11,
      "predicted_advised_flops_count": 1.3153337344000000e+10,
      "predicted_flops": 7.5276995011265234e+11,
      "predicted_flops_count": 1.6733935704913866e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 2.7683389440000000e+09,
      "workspace_megabytes": 2.6400937500000000e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__558036841815313172<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 46,
      "real_time": 1.5204966250483109e+07,
      "cpu_time": 1.8522838869564541e+07,
      "time_unit": "ns",
      "items_per_second": 8.6506849981216357e+11,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.2500344960000000e+09,
      "advised_time": 1.5618047714233398e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.6667819325789325e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 4.6000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 8.6506849981216357e+11,
      "predicted_advised_flops_count": 1.3153337344000000e+10,
      "predicted_flops": 1.1005572409200303e+12,
      "predicted_flops_count": 1.6733935704913866e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.2500344960000000e+09,
      "workspace_megabytes": 1.1921257934570312e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__558036841815313172<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__558036841815313172<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__1085515221638657438<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 154,
      "real_time": 4.5340100013009915e+06,
      "cpu_time": 4.8328314090909977e+06,
      "time_unit": "ns",
      "items_per_second": 2.9010384494577148e+12,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 4.5537281036376953e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4730153338016043e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 1.5400000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 2.9010384494577148e+12,
      "predicted_advised_flops_count": 1.3153337344000000e+10,
      "predicted_flops": 2.9010384494577148e+12,
      "predicted_flops_count": 1.3153337344000000e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__1085515221638657438<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 231,
      "real_time": 3.0375218857482914e+06,
      "cpu_time": 3.1741658354975809e+06,
      "time_unit": "ns",
      "items_per_second": 4.3302856205625938e+12,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.1840000000000000e+03,
      "advised_time": 3.0801920890808105e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.2419525699096977e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 2.3100000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 4.3302856205625938e+12,
      "predicted_advised_flops_count": 1.3153337344000000e+10,
      "predicted_flops": 4.3302856205625938e+12,
      "predicted_flops_count": 1.3153337344000000e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.1840000000000000e+03,
      "workspace_megabytes": 1.1291503906250000e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__1085515221638657438<CUDNN_CONVOLUTION_FWD_ALGO_GEMM>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 145,
      "real_time": 4.8391521009134836e+06,
      "cpu_time": 5.1804602344829170e+06,
      "time_unit": "ns",
      "items_per_second": 2.7181078564397788e+12,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.0552089600000000e+08,
      "advised_time": 4.8650240898132324e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4280899851402570e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 1.4500000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 2.7181078564397788e+12,
      "predicted_advised_flops_count": 1.3153337344000000e+10,
      "predicted_flops": 2.7181078564397788e+12,
      "predicted_flops_count": 1.3153337344000000e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 2.0552089600000000e+08,
      "workspace_megabytes": 1.9600000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__1085515221638657438<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__1085515221638657438<CUDNN_CONVOLUTION_FWD_ALGO_FFT>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 65,
      "real_time": 1.0715597337828232e+07,
      "cpu_time": 1.2371014661538137e+07,
      "time_unit": "ns",
      "items_per_second": 1.2274945511031897e+12,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.2090081280000000e+09,
      "advised_time": 1.0748928070068359e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnMathType_t math_type = (cudnnMathType_t)0]": 2.3237108083472476e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 6.5000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 1.2274945511031897e+12,
      "predicted_advised_flops_count": 1.3153337344000000e+10,
      "predicted_flops": 1.3096460487369290e+12,
      "predicted_flops_count": 1.4033639713342701e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.2090081280000000e+09,
      "workspace_megabytes": 1.1530000000000000e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__1085515221638657438<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 42,
      "real_time": 1.6690855491019432e+07,
      "cpu_time": 2.0664730761904508e+07,
      "time_unit": "ns",
      "items_per_second": 7.8805651100850977e+11,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 6.5955472000000000e+08,
      "advised_time": 1.6891584396362305e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.6667819325789325e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 4.2000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 7.8805651100850977e+11,
      "predicted_advised_flops_count": 1.3153337344000000e+10,
      "predicted_flops": 8.4079810773591248e+11,
      "predicted_flops_count": 1.4033639713342701e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 6.5955472000000000e+08,
      "workspace_megabytes": 6.2900039672851562e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__1085515221638657438<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__1085515221638657438<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__1211487690380309299<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 135,
      "real_time": 5.1744969596189484e+06,
      "cpu_time": 5.5691542740742378e+06,
      "time_unit": "ns",
      "items_per_second": 2.5419547922526206e+12,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 5.1806077957153320e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4730153338016043e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 1.3500000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 2.5419547922526206e+12,
      "predicted_advised_flops_count": 1.3153337344000000e+10,
      "predicted_flops": 2.5419547922526206e+12,
      "predicted_flops_count": 1.3153337344000000e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__1211487690380309299<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 189,
      "real_time": 3.7029745111429188e+06,
      "cpu_time": 3.9080178888895032e+06,
      "time_unit": "ns",
      "items_per_second": 3.5521004274858589e+12,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.7120000000000000e+03,
      "advised_time": 3.7386240959167480e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.2419525699096977e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 1.8900000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 3.5521004274858589e+12,
      "predicted_advised_flops_count": 1.3153337344000000e+10,
      "predicted_flops": 3.5521004274858589e+12,
      "predicted_flops_count": 1.3153337344000000e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.7120000000000000e+03,
      "workspace_megabytes": 4.4937133789062500e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__1211487690380309299<CUDNN_CONVOLUTION_FWD_ALGO_GEMM>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 121,
      "real_time": 5.7849438691681083e+06,
      "cpu_time": 6.2750665123968143e+06,
      "time_unit": "ns",
      "items_per_second": 2.2737190958935767e+12,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.0276044800000000e+08,
      "advised_time": 5.7926077842712402e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4280899851402570e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 1.2100000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 2.2737190958935767e+12,
      "predicted_advised_flops_count": 1.3153337344000000e+10,
      "predicted_flops": 2.2737190958935767e+12,
      "predicted_flops_count": 1.3153337344000000e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.0276044800000000e+08,
      "workspace_megabytes": 9.8000000000000000e+01
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__1211487690380309299<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__1211487690380309299<CUDNN_CONVOLUTION_FWD_ALGO_FFT>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 59,
      "real_time": 1.1953143860702798e+07,
      "cpu_time": 1.3998120474576242e+07,
      "time_unit": "ns",
      "items_per_second": 1.1004081852677239e+12,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.1411128320000000e+09,
      "advised_time": 1.1944959640502930e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnMathType_t math_type = (cudnnMathType_t)0]": 2.3237108083472476e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 5.9000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 1.1004081852677239e+12,
      "predicted_advised_flops_count": 1.3153337344000000e+10,
      "predicted_flops": 1.2450580639397351e+12,
      "predicted_flops_count": 1.4882358153199755e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.1411128320000000e+09,
      "workspace_megabytes": 1.0882500000000000e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__1211487690380309299<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 68,
      "real_time": 1.0216136887559995e+07,
      "cpu_time": 1.1732890573529050e+07,
      "time_unit": "ns",
      "items_per_second": 1.2875059808582419e+12,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 6.3281603200000000e+08,
      "advised_time": 1.0462176322937012e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.6667819325789325e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 6.8000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 1.2875059808582419e+12,
      "predicted_advised_flops_count": 1.3153337344000000e+10,
      "predicted_flops": 1.4567500726543447e+12,
      "predicted_flops_count": 1.4882358153199755e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 6.3281603200000000e+08,
      "workspace_megabytes": 6.0350039672851562e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__1211487690380309299<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__1211487690380309299<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__258854267416158047<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 154,
      "real_time": 4.5330709681011643e+06,
      "cpu_time": 4.8359316168825384e+06,
      "time_unit": "ns",
      "items_per_second": 2.9016394044035312e+12,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 4.4669442176818848e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4730153338016043e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.0480000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.0480000000000000e+03,
      "num_iterations": 1.5400000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 2.9016394044035312e+12,
      "predicted_advised_flops_count": 1.3153337344000000e+10,
      "predicted_flops": 2.9016394044035312e+12,
      "predicted_flops_count": 1.3153337344000000e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__258854267416158047<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 224,
      "real_time": 3.1312414139392786e+06,
      "cpu_time": 3.2775367812501853e+06,
      "time_unit": "ns",
      "items_per_second": 4.2006781353381367e+12,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.0000000000000000e+02,
      "advised_time": 3.1764481067657471e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.2419525699096977e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.0480000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.0480000000000000e+03,
      "num_iterations": 2.2400000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 4.2006781353381367e+12,
      "predicted_advised_flops_count": 1.3153337344000000e+10,
      "predicted_flops": 4.2006781353381367e+12,
      "predicted_flops_count": 1.3153337344000000e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 3.0000000000000000e+02,
      "workspace_megabytes": 2.8610229492187500e-04
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__258854267416158047<CUDNN_CONVOLUTION_FWD_ALGO_GEMM>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 174,
      "real_time": 4.0166280473229187e+06,
      "cpu_time": 4.2539240919539342e+06,
      "time_unit": "ns",
      "items_per_second": 3.2747212808929858e+12,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.5690112000000000e+07,
      "advised_time": 4.0386562347412109e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4280899851402570e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.0480000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.0480000000000000e+03,
      "num_iterations": 1.7400000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 3.2747212808929858e+12,
      "predicted_advised_flops_count": 1.3153337344000000e+10,
      "predicted_flops": 3.2747212808929858e+12,
      "predicted_flops_count": 1.3153337344000000e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 2.5690112000000000e+07,
      "workspace_megabytes": 2.4500000000000000e+01
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__258854267416158047<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__258854267416158047<CUDNN_CONVOLUTION_FWD_ALGO_FFT>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 18,
      "real_time": 3.8781527015897959e+07,
      "cpu_time": 6.0377182611112013e+07,
      "time_unit": "ns",
      "items_per_second": 3.3916501891758844e+11,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.7221032960000000e+09,
      "advised_time": 3.8762496948242188e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnMathType_t math_type = (cudnnMathType_t)0]": 2.3237108083472476e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.0480000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.0480000000000000e+03,
      "num_iterations": 1.8000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 3.3916501891758844e+11,
      "predicted_advised_flops_count": 1.3153337344000000e+10,
      "predicted_flops": 3.5125294431624536e+11,
      "predicted_flops_count": 1.3622125549414173e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 2.7221032960000000e+09,
      "workspace_megabytes": 2.5960000000000000e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__258854267416158047<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 25,
      "real_time": 2.8504272624850273e+07,
      "cpu_time": 3.9912207239998452e+07,
      "time_unit": "ns",
      "items_per_second": 4.6145142930371802e+11,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 7.6650947200000000e+08,
      "advised_time": 2.8728319168090820e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.6667819325789325e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.0480000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.0480000000000000e+03,
      "num_iterations": 2.5000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 4.6145142930371802e+11,
      "predicted_advised_flops_count": 1.3153337344000000e+10,
      "predicted_flops": 4.7789767269971606e+11,
      "predicted_flops_count": 1.3622125549414173e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 7.6650947200000000e+08,
      "workspace_megabytes": 7.3100039672851562e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__258854267416158047<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__258854267416158047<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__13581997518006265142<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 142,
      "real_time": 4.9326941043033563e+06,
      "cpu_time": 5.2875797112673139e+06,
      "time_unit": "ns",
      "items_per_second": 2.6665625449031655e+12,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 4.9408001899719238e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4730153338016043e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.0240000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.0240000000000000e+03,
      "num_iterations": 1.4200000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 2.6665625449031655e+12,
      "predicted_advised_flops_count": 1.3153337344000000e+10,
      "predicted_flops": 2.6665625449031655e+12,
      "predicted_flops_count": 1.3153337344000000e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__13581997518006265142<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 212,
      "real_time": 3.3008392454335853e+06,
      "cpu_time": 3.4629016839619162e+06,
      "time_unit": "ns",
      "items_per_second": 3.9848463878380205e+12,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.1840000000000000e+03,
      "advised_time": 3.3597440719604492e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.2419525699096977e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.0240000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.0240000000000000e+03,
      "num_iterations": 2.1200000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 3.9848463878380205e+12,
      "predicted_advised_flops_count": 1.3153337344000000e+10,
      "predicted_flops": 3.9848463878380205e+12,
      "predicted_flops_count": 1.3153337344000000e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.1840000000000000e+03,
      "workspace_megabytes": 1.1291503906250000e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__13581997518006265142<CUDNN_CONVOLUTION_FWD_ALGO_GEMM>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 127,
      "real_time": 5.4929295268760426e+06,
      "cpu_time": 5.9342427480320511e+06,
      "time_unit": "ns",
      "items_per_second": 2.3945942287522139e+12,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 5.1380224000000000e+07,
      "advised_time": 5.5334720611572266e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4280899851402570e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.0240000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.0240000000000000e+03,
      "num_iterations": 1.2700000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 2.3945942287522139e+12,
      "predicted_advised_flops_count": 1.3153337344000000e+10,
      "predicted_flops": 2.3945942287522139e+12,
      "predicted_flops_count": 1.3153337344000000e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 5.1380224000000000e+07,
      "workspace_megabytes": 4.9000000000000000e+01
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__13581997518006265142<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__13581997518006265142<CUDNN_CONVOLUTION_FWD_ALGO_FFT>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 63,
      "real_time": 1.1069432314899232e+07,
      "cpu_time": 1.2837665666666901e+07,
      "time_unit": "ns",
      "items_per_second": 1.1882576242230483e+12,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 7.5602329600000000e+08,
      "advised_time": 1.1103232383728027e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnMathType_t math_type = (cudnnMathType_t)0]": 2.3237108083472476e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.0240000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.0240000000000000e+03,
      "num_iterations": 6.3000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 1.1882576242230483e+12,
      "predicted_advised_flops_count": 1.3153337344000000e+10,
      "predicted_flops": 1.2677831449814915e+12,
      "predicted_flops_count": 1.4033639713342701e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 7.5602329600000000e+08,
      "workspace_megabytes": 7.2100000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__13581997518006265142<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 44,
      "real_time": 1.5933902637863701e+07,
      "cpu_time": 1.9565570613636717e+07,
      "time_unit": "ns",
      "items_per_second": 8.2549376903708142e+11,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 6.5955472000000000e+08,
      "advised_time": 1.6134080886840820e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.6667819325789325e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.0240000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.0240000000000000e+03,
      "num_iterations": 4.4000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 8.2549376903708142e+11,
      "predicted_advised_flops_count": 1.3153337344000000e+10,
      "predicted_flops": 8.8074089771294263e+11,
      "predicted_flops_count": 1.4033639713342701e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 6.5955472000000000e+08,
      "workspace_megabytes": 6.2900039672851562e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__13581997518006265142<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__13581997518006265142<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__4650042837402288837<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 70,
      "real_time": 9.9969129876366686e+06,
      "cpu_time": 1.1432681057142295e+07,
      "time_unit": "ns",
      "items_per_second": 3.2893497623383661e+11,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 1.0035200119018555e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4730153338016043e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 7.0000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "predicted_advised_flops": -1.0003087965622137e+02,
      "predicted_advised_flops_count": -1.0000000000000000e+00,
      "predicted_flops": 2.9604147861045293e+12,
      "predicted_flops_count": 2.9595009024000000e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__4650042837402288837<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 106,
      "real_time": 6.5828316783497352e+06,
      "cpu_time": 7.2099253962266119e+06,
      "time_unit": "ns",
      "items_per_second": 4.9953188789787799e+11,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.7120000000000000e+03,
      "advised_time": 6.6139841079711914e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.2419525699096977e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 1.0600000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "predicted_advised_flops": -1.5191030985782280e+02,
      "predicted_advised_flops_count": -1.0000000000000000e+00,
      "predicted_flops": 4.4957869910809014e+12,
      "predicted_flops_count": 2.9595009024000000e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.7120000000000000e+03,
      "workspace_megabytes": 4.4937133789062500e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__4650042837402288837<CUDNN_CONVOLUTION_FWD_ALGO_GEMM>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 59,
      "real_time": 1.1965129192981679e+07,
      "cpu_time": 1.4000948661017470e+07,
      "time_unit": "ns",
      "items_per_second": 2.7482647976160760e+11,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 9.2484403200000000e+08,
      "advised_time": 1.1985823631286621e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4280899851402570e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 5.9000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "predicted_advised_flops": -8.3576197454396436e+01,
      "predicted_advised_flops_count": -1.0000000000000000e+00,
      "predicted_flops": 2.4734383178544688e+12,
      "predicted_flops_count": 2.9595009024000000e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 9.2484403200000000e+08,
      "workspace_megabytes": 8.8200000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__4650042837402288837<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__4650042837402288837<CUDNN_CONVOLUTION_FWD_ALGO_FFT>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 196,
      "real_time": 3.5650453777337559e+06,
      "cpu_time": 3.7526184489799598e+06,
      "time_unit": "ns",
      "items_per_second": 9.2238218243671924e+11,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.2840883200000000e+08,
      "advised_time": 3.6935679912567139e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnMathType_t math_type = (cudnnMathType_t)0]": 2.3237108083472476e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 1.9600000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "predicted_advised_flops": -2.8050133842495001e+02,
      "predicted_advised_flops_count": -1.0000000000000000e+00,
      "predicted_flops": 1.0955938435599769e+12,
      "predicted_flops_count": 3.9058417678570557e+09,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.2840883200000000e+08,
      "workspace_megabytes": 4.0856250000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__4650042837402288837<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 197,
      "real_time": 3.5518314145732350e+06,
      "cpu_time": 3.7369376497458648e+06,
      "time_unit": "ns",
      "items_per_second": 9.2581374287864526e+11,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.5654041600000000e+08,
      "advised_time": 3.6628479957580566e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.6667819325789325e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 1.9700000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "predicted_advised_flops": -2.8154489424722698e+02,
      "predicted_advised_flops_count": -1.0000000000000000e+00,
      "predicted_flops": 1.0996698074777168e+12,
      "predicted_flops_count": 3.9058417678570557e+09,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 3.5654041600000000e+08,
      "workspace_megabytes": 3.4002343750000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__4650042837402288837<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 171,
      "real_time": 4.0842219152384335e+06,
      "cpu_time": 4.3284286081875283e+06,
      "time_unit": "ns",
      "items_per_second": 8.0513116188203735e+11,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.6389120000000000e+06,
      "advised_time": 4.1687040328979492e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)6; cudnnMathType_t math_type = (cudnnMathType_t)0]": 5.7500739770702787e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 6.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 1.7100000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "predicted_advised_flops": -2.4484467806926716e+02,
      "predicted_advised_flops_count": -1.0000000000000000e+00,
      "predicted_flops": -2.4484467806926716e+02,
      "predicted_flops_count": -1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.6389120000000000e+06,
      "workspace_megabytes": 1.5629882812500000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_0_FLOAT32__BatchSize_256__4650042837402288837<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 199,
      "real_time": 3.5161080819334667e+06,
      "cpu_time": 3.6983533266327437e+06,
      "time_unit": "ns",
      "items_per_second": 9.3521992480725549e+11,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.6478131200000000e+08,
      "advised_time": 3.6311039924621582e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)7; cudnnMathType_t math_type = (cudnnMathType_t)0]": 8.2538945752816425e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 7.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 1.9900000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "predicted_advised_flops": -2.8440536431124485e+02,
      "predicted_advised_flops_count": -1.0000000000000000e+00,
      "predicted_flops": -2.8440536431124485e+02,
      "predicted_flops_count": -1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.6478131200000000e+08,
      "workspace_megabytes": 4.4325000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_1_FLOAT32__BatchSize_256__8323995546528292013<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 75,
      "real_time": 9.2876872792840004e+06,
      "cpu_time": 1.0542082053331872e+07,
      "time_unit": "ns",
      "items_per_second": 1.1329698727766818e+13,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 9.3020162582397461e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4730153338016043e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 7.5000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 2.8324246819417046e+12,
      "predicted_advised_flops_count": 2.6306674688000000e+10,
      "predicted_flops": 2.8324246819417046e+12,
      "predicted_flops_count": 2.6306674688000000e+10,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_1_FLOAT32__BatchSize_256__8323995546528292013<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 102,
      "real_time": 6.8444426755840871e+06,
      "cpu_time": 7.5284896568629788e+06,
      "time_unit": "ns",
      "items_per_second": 1.5374034635043564e+13,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.7120000000000000e+03,
      "advised_time": 6.8872637748718262e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.2419525699096977e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 1.0200000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 3.8435086587608911e+12,
      "predicted_advised_flops_count": 2.6306674688000000e+10,
      "predicted_flops": 3.8435086587608911e+12,
      "predicted_flops_count": 2.6306674688000000e+10,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 4.7120000000000000e+03,
      "workspace_megabytes": 4.4937133789062500e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_1_FLOAT32__BatchSize_256__8323995546528292013<CUDNN_CONVOLUTION_FWD_ALGO_GEMM>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 63,
      "real_time": 1.1190001764112994e+07,
      "cpu_time": 1.2983151063490964e+07,
      "time_unit": "ns",
      "items_per_second": 9.4036355820307656e+12,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.0552089600000000e+08,
      "advised_time": 1.1204607963562012e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4280899851402570e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 6.3000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 2.3509088955076914e+12,
      "predicted_advised_flops_count": 2.6306674688000000e+10,
      "predicted_flops": 2.3509088955076914e+12,
      "predicted_flops_count": 2.6306674688000000e+10,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 2.0552089600000000e+08,
      "workspace_megabytes": 1.9600000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_1_FLOAT32__BatchSize_256__8323995546528292013<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_1_FLOAT32__BatchSize_256__8323995546528292013<CUDNN_CONVOLUTION_FWD_ALGO_FFT>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_1_FLOAT32__BatchSize_256__8323995546528292013<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_1_FLOAT32__BatchSize_256__8323995546528292013<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_1_FLOAT32__BatchSize_256__8323995546528292013<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_1_FLOAT32__BatchSize_256__15950758122554176294<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 73,
      "real_time": 9.5438391531575210e+06,
      "cpu_time": 1.0860832863015698e+07,
      "time_unit": "ns",
      "items_per_second": 1.1025615275293736e+13,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 9.5865602493286133e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4730153338016043e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.0240000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.0240000000000000e+03,
      "num_iterations": 7.3000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 2.7564038188234341e+12,
      "predicted_advised_flops_count": 2.6306674688000000e+10,
      "predicted_flops": 2.7564038188234341e+12,
      "predicted_flops_count": 2.6306674688000000e+10,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_1_FLOAT32__BatchSize_256__15950758122554176294<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 107,
      "real_time": 6.5368505621276731e+06,
      "cpu_time": 7.1556215981323160e+06,
      "time_unit": "ns",
      "items_per_second": 1.6097461270056918e+13,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.1840000000000000e+03,
      "advised_time": 6.5740160942077637e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.2419525699096977e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.0240000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.0240000000000000e+03,
      "num_iterations": 1.0700000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 4.0243653175142295e+12,
      "predicted_advised_flops_count": 2.6306674688000000e+10,
      "predicted_flops": 4.0243653175142295e+12,
      "predicted_flops_count": 2.6306674688000000e+10,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 1.1840000000000000e+03,
      "workspace_megabytes": 1.1291503906250000e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_1_FLOAT32__BatchSize_256__15950758122554176294<CUDNN_CONVOLUTION_FWD_ALGO_GEMM>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 84,
      "real_time": 8.3392610263434192e+06,
      "cpu_time": 9.3412287857148610e+06,
      "time_unit": "ns",
      "items_per_second": 1.2618228212259182e+13,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.0276044800000000e+08,
      "advised_time": 8.4234237670898438e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4280899851402570e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.0240000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.0240000000000000e+03,
      "num_iterations": 8.4000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 3.1545570530647954e+12,
      "predicted_advised_flops_count": 2.6306674688000000e+10,
      "predicted_flops": 3.1545570530647954e+12,
      "predicted_flops_count": 2.6306674688000000e+10,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 1.0276044800000000e+08,
      "workspace_megabytes": 9.8000000000000000e+01
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_1_FLOAT32__BatchSize_256__15950758122554176294<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_1_FLOAT32__BatchSize_256__15950758122554176294<CUDNN_CONVOLUTION_FWD_ALGO_FFT>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_1_FLOAT32__BatchSize_256__15950758122554176294<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_1_FLOAT32__BatchSize_256__15950758122554176294<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_1_FLOAT32__BatchSize_256__15950758122554176294<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_2_FLOAT32__BatchSize_256__3186413901075968236<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 77,
      "real_time": 9.1351754358643070e+06,
      "cpu_time": 1.0331215480519334e+07,
      "time_unit": "ns",
      "items_per_second": 3.5996400497029767e+11,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 9.1258878707885742e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4730153338016043e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 7.7000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "predicted_advised_flops": 3.2396760447326787e+12,
      "predicted_advised_flops_count": 2.9595009024000000e+10,
      "predicted_flops": 3.2396760447326787e+12,
      "predicted_flops_count": 2.9595009024000000e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_2_FLOAT32__BatchSize_256__3186413901075968236<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 105,
      "real_time": 6.6284303065566793e+06,
      "cpu_time": 7.2642137047623806e+06,
      "time_unit": "ns",
      "items_per_second": 4.9609548323186884e+11,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.1840000000000000e+03,
      "advised_time": 6.6571202278137207e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.2419525699096977e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 1.0500000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "predicted_advised_flops": 4.4648593490868193e+12,
      "predicted_advised_flops_count": 2.9595009024000000e+10,
      "predicted_flops": 4.4648593490868193e+12,
      "predicted_flops_count": 2.9595009024000000e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.1840000000000000e+03,
      "workspace_megabytes": 1.1291503906250000e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_2_FLOAT32__BatchSize_256__3186413901075968236<CUDNN_CONVOLUTION_FWD_ALGO_GEMM>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 76,
      "real_time": 9.2144160114816930e+06,
      "cpu_time": 1.0429868578948518e+07,
      "time_unit": "ns",
      "items_per_second": 3.5686844743091107e+11,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.6242201600000000e+08,
      "advised_time": 9.3399038314819336e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4280899851402570e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 7.6000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "predicted_advised_flops": 3.2118160268781997e+12,
      "predicted_advised_flops_count": 2.9595009024000000e+10,
      "predicted_flops": 3.2118160268781997e+12,
      "predicted_flops_count": 2.9595009024000000e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.6242201600000000e+08,
      "workspace_megabytes": 4.4100000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_2_FLOAT32__BatchSize_256__3186413901075968236<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_2_FLOAT32__BatchSize_256__3186413901075968236<CUDNN_CONVOLUTION_FWD_ALGO_FFT>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 226,
      "real_time": 3.0943396484759529e+06,
      "cpu_time": 3.2366024867264940e+06,
      "time_unit": "ns",
      "items_per_second": 1.0626934045910554e+12,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.0434918400000000e+08,
      "advised_time": 3.1559679508209229e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnMathType_t math_type = (cudnnMathType_t)0]": 2.3237108083472476e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 2.2600000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "predicted_advised_flops": 9.5642406413194980e+12,
      "predicted_advised_flops_count": 2.9595009024000000e+10,
      "predicted_flops": 1.1575227240740532e+12,
      "predicted_flops_count": 3.5817684591142335e+09,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 3.0434918400000000e+08,
      "workspace_megabytes": 2.9025000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_2_FLOAT32__BatchSize_256__3186413901075968236<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 68,
      "real_time": 1.0367232881595984e+07,
      "cpu_time": 1.1892997102943154e+07,
      "time_unit": "ns",
      "items_per_second": 3.1718534478351349e+11,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 8.5566259200000000e+08,
      "advised_time": 1.0491871833801270e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.6667819325789325e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 6.8000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "predicted_advised_flops": 2.8546681030516216e+12,
      "predicted_advised_flops_count": 2.9595009024000000e+10,
      "predicted_flops": 3.4548934127568652e+11,
      "predicted_flops_count": 3.5817684591142335e+09,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 8.5566259200000000e+08,
      "workspace_megabytes": 8.1602343750000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_2_FLOAT32__BatchSize_256__3186413901075968236<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 183,
      "real_time": 3.8289802894505160e+06,
      "cpu_time": 4.0426551584708760e+06,
      "time_unit": "ns",
      "items_per_second": 8.5880158355996606e+11,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 6.5546240000000000e+06,
      "advised_time": 3.9116799831390381e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)6; cudnnMathType_t math_type = (cudnnMathType_t)0]": 5.7500739770702787e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 6.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 1.8300000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "predicted_advised_flops": 7.7292142520396943e+12,
      "predicted_advised_flops_count": 2.9595009024000000e+10,
      "predicted_flops": -2.6116613939099346e+02,
      "predicted_flops_count": -1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 6.5546240000000000e+06,
      "workspace_megabytes": 6.2509765625000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_2_FLOAT32__BatchSize_256__3186413901075968236<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 219,
      "real_time": 3.1969049364991793e+06,
      "cpu_time": 3.3479069041086105e+06,
      "time_unit": "ns",
      "items_per_second": 1.0285993488442424e+12,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.1142707200000000e+08,
      "advised_time": 3.3167359828948975e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)7; cudnnMathType_t math_type = (cudnnMathType_t)0]": 8.2538945752816425e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 7.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 2.1900000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "predicted_advised_flops": 9.2573941395981816e+12,
      "predicted_advised_flops_count": 2.9595009024000000e+10,
      "predicted_flops": -3.1280254491866924e+02,
      "predicted_flops_count": -1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 3.1142707200000000e+08,
      "workspace_megabytes": 2.9700000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_2_FLOAT32__BatchSize_256__17000990389697758201<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 290,
      "real_time": 2.4138643839878254e+06,
      "cpu_time": 2.5020269586202274e+06,
      "time_unit": "ns",
      "items_per_second": 1.0898157685453750e+13,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 2.4400320053100586e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4730153338016043e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 2.9000000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 2.7245394213634375e+12,
      "predicted_advised_flops_count": 6.5766686720000000e+09,
      "predicted_flops": 2.7245394213634375e+12,
      "predicted_flops_count": 6.5766686720000000e+09,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_2_FLOAT32__BatchSize_256__17000990389697758201<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 387,
      "real_time": 1.8078054425376407e+06,
      "cpu_time": 1.8594257002581910e+06,
      "time_unit": "ns",
      "items_per_second": 1.4551717828149123e+13,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.0000000000000000e+02,
      "advised_time": 1.8470400571823120e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.2419525699096977e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 3.8700000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 3.6379294570372808e+12,
      "predicted_advised_flops_count": 6.5766686720000000e+09,
      "predicted_flops": 3.6379294570372808e+12,
      "predicted_flops_count": 6.5766686720000000e+09,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 3.0000000000000000e+02,
      "workspace_megabytes": 2.8610229492187500e-04
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_2_FLOAT32__BatchSize_256__17000990389697758201<CUDNN_CONVOLUTION_FWD_ALGO_GEMM>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 285,
      "real_time": 2.4532668643810768e+06,
      "cpu_time": 2.5444101894748383e+06,
      "time_unit": "ns",
      "items_per_second": 1.0723119881471512e+13,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 5.1380224000000000e+07,
      "advised_time": 2.4627199172973633e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4280899851402570e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 2.8500000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 2.6807799703678779e+12,
      "predicted_advised_flops_count": 6.5766686720000000e+09,
      "predicted_flops": 2.6807799703678779e+12,
      "predicted_flops_count": 6.5766686720000000e+09,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 5.1380224000000000e+07,
      "workspace_megabytes": 4.9000000000000000e+01
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_2_FLOAT32__BatchSize_256__17000990389697758201<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_2_FLOAT32__BatchSize_256__17000990389697758201<CUDNN_CONVOLUTION_FWD_ALGO_FFT>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_2_FLOAT32__BatchSize_256__17000990389697758201<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_2_FLOAT32__BatchSize_256__17000990389697758201<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_2_FLOAT32__BatchSize_256__17000990389697758201<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__3290992122269695807<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 163,
      "real_time": 4.2911729306851067e+06,
      "cpu_time": 4.5601838343551848e+06,
      "time_unit": "ns",
      "items_per_second": 3.0652079411537505e+12,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 4.3649921417236328e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4730153338016043e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.0480000000000000e+03,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.0480000000000000e+03,
      "input_height": 7.0000000000000000e+00,
      "input_size": 2.5690112000000000e+07,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 1.6300000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 3.0652079411537505e+12,
      "predicted_advised_flops_count": 1.3153337344000000e+10,
      "predicted_flops": 3.0652079411537505e+12,
      "predicted_flops_count": 1.3153337344000000e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__3290992122269695807<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 229,
      "real_time": 3.0522772668165411e+06,
      "cpu_time": 3.1902542794759329e+06,
      "time_unit": "ns",
      "items_per_second": 4.3093520654231538e+12,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.0000000000000000e+02,
      "advised_time": 3.0689599514007568e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.2419525699096977e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.0480000000000000e+03,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.0480000000000000e+03,
      "input_height": 7.0000000000000000e+00,
      "input_size": 2.5690112000000000e+07,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 2.2900000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 4.3093520654231538e+12,
      "predicted_advised_flops_count": 1.3153337344000000e+10,
      "predicted_flops": 4.3093520654231538e+12,
      "predicted_flops_count": 1.3153337344000000e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 3.0000000000000000e+02,
      "workspace_megabytes": 2.8610229492187500e-04
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__3290992122269695807<CUDNN_CONVOLUTION_FWD_ALGO_GEMM>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 160,
      "real_time": 4.3827405897900462e+06,
      "cpu_time": 4.6616284062510617e+06,
      "time_unit": "ns",
      "items_per_second": 3.0011672090841465e+12,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.0276044800000000e+08,
      "advised_time": 4.4769282341003418e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4280899851402570e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.0480000000000000e+03,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.0480000000000000e+03,
      "input_height": 7.0000000000000000e+00,
      "input_size": 2.5690112000000000e+07,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 1.6000000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 3.0011672090841465e+12,
      "predicted_advised_flops_count": 1.3153337344000000e+10,
      "predicted_flops": 3.0011672090841465e+12,
      "predicted_flops_count": 1.3153337344000000e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.0276044800000000e+08,
      "workspace_megabytes": 9.8000000000000000e+01
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__3290992122269695807<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__3290992122269695807<CUDNN_CONVOLUTION_FWD_ALGO_FFT>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 19,
      "real_time": 3.7189946754982598e+07,
      "cpu_time": 5.6772350999997467e+07,
      "time_unit": "ns",
      "items_per_second": 3.5367991867957581e+11,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.6280729600000000e+09,
      "advised_time": 3.7162815093994141e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnMathType_t math_type = (cudnnMathType_t)0]": 2.3237108083472476e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.0480000000000000e+03,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.0480000000000000e+03,
      "input_height": 7.0000000000000000e+00,
      "input_size": 2.5690112000000000e+07,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 1.9000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 3.5367991867957581e+11,
      "predicted_advised_flops_count": 1.3153337344000000e+10,
      "predicted_flops": 3.6628515870593768e+11,
      "predicted_flops_count": 1.3622125549414173e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 3.6280729600000000e+09,
      "workspace_megabytes": 3.4600000000000000e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__3290992122269695807<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 24,
      "real_time": 2.8918279800564051e+07,
      "cpu_time": 4.0959286541670062e+07,
      "time_unit": "ns",
      "items_per_second": 4.5484508188981018e+11,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 7.6650947200000000e+08,
      "advised_time": 2.9119487762451172e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.6667819325789325e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.0480000000000000e+03,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.0480000000000000e+03,
      "input_height": 7.0000000000000000e+00,
      "input_size": 2.5690112000000000e+07,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 2.4000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 4.5484508188981018e+11,
      "predicted_advised_flops_count": 1.3153337344000000e+10,
      "predicted_flops": 4.7105587342537836e+11,
      "predicted_flops_count": 1.3622125549414173e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 7.6650947200000000e+08,
      "workspace_megabytes": 7.3100039672851562e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__3290992122269695807<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__3290992122269695807<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__11588042906753169461<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 349,
      "real_time": 2.0014690802323187e+06,
      "cpu_time": 2.0649611260737851e+06,
      "time_unit": "ns",
      "items_per_second": 1.6429603477153440e+12,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 2.0367360115051270e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4730153338016043e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 3.4900000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 1.6429603477153440e+12,
      "predicted_advised_flops_count": 3.2883343360000000e+09,
      "predicted_flops": 1.6429603477153440e+12,
      "predicted_flops_count": 3.2883343360000000e+09,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__11588042906753169461<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 499,
      "real_time": 1.4019654706803586e+06,
      "cpu_time": 1.4359094809631237e+06,
      "time_unit": "ns",
      "items_per_second": 2.3455173502983687e+12,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.8824000000000000e+04,
      "advised_time": 1.4317760467529297e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.2419525699096977e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 4.9900000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 2.3455173502983687e+12,
      "predicted_advised_flops_count": 3.2883343360000000e+09,
      "predicted_flops": 2.3455173502983687e+12,
      "predicted_flops_count": 3.2883343360000000e+09,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.8824000000000000e+04,
      "workspace_megabytes": 1.7951965332031250e-02
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__11588042906753169461<CUDNN_CONVOLUTION_FWD_ALGO_GEMM>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 237,
      "real_time": 2.9587652377061201e+06,
      "cpu_time": 3.0902073924057530e+06,
      "time_unit": "ns",
      "items_per_second": 1.1113873767657854e+12,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.0552089600000000e+08,
      "advised_time": 2.9890239238739014e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4280899851402570e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 2.3700000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 1.1113873767657854e+12,
      "predicted_advised_flops_count": 3.2883343360000000e+09,
      "predicted_flops": 1.1113873767657854e+12,
      "predicted_flops_count": 3.2883343360000000e+09,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 2.0552089600000000e+08,
      "workspace_megabytes": 1.9600000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__11588042906753169461<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__11588042906753169461<CUDNN_CONVOLUTION_FWD_ALGO_FFT>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 89,
      "real_time": 7.8806000123365540e+06,
      "cpu_time": 8.7744526629229430e+06,
      "time_unit": "ns",
      "items_per_second": 4.1726953922954236e+11,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 6.9215846400000000e+08,
      "advised_time": 7.8991360664367676e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnMathType_t math_type = (cudnnMathType_t)0]": 2.3237108083472476e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 8.9000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 4.1726953922954236e+11,
      "predicted_advised_flops_count": 3.2883343360000000e+09,
      "predicted_flops": 5.8765306120004651e+11,
      "predicted_flops_count": 4.6310587213427000e+09,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 6.9215846400000000e+08,
      "workspace_megabytes": 6.6009375000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__11588042906753169461<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 132,
      "real_time": 5.2836625484015904e+06,
      "cpu_time": 5.6893703560609249e+06,
      "time_unit": "ns",
      "items_per_second": 6.2235888569280115e+11,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.9968006400000000e+08,
      "advised_time": 5.4722881317138672e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.6667819325789325e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 1.3200000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 6.2235888569280115e+11,
      "predicted_advised_flops_count": 3.2883343360000000e+09,
      "predicted_flops": 8.7648646727896814e+11,
      "predicted_flops_count": 4.6310587213427000e+09,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.9968006400000000e+08,
      "workspace_megabytes": 4.7653204345703125e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__11588042906753169461<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__11588042906753169461<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__5889100136049897371<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 151,
      "real_time": 4.6369727078011101e+06,
      "cpu_time": 4.9499991192064295e+06,
      "time_unit": "ns",
      "items_per_second": 2.8366216867895732e+12,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 4.6551041603088379e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4730153338016043e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 1.5100000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 2.8366216867895732e+12,
      "predicted_advised_flops_count": 1.3153337344000000e+10,
      "predicted_flops": 2.8366216867895732e+12,
      "predicted_flops_count": 1.3153337344000000e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__5889100136049897371<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 227,
      "real_time": 3.0833174810962267e+06,
      "cpu_time": 3.2251444713644693e+06,
      "time_unit": "ns",
      "items_per_second": 4.2659691791853789e+12,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.7120000000000000e+03,
      "advised_time": 3.0914559364318848e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.2419525699096977e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 2.2700000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 4.2659691791853789e+12,
      "predicted_advised_flops_count": 1.3153337344000000e+10,
      "predicted_flops": 4.2659691791853789e+12,
      "predicted_flops_count": 1.3153337344000000e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.7120000000000000e+03,
      "workspace_megabytes": 4.4937133789062500e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__5889100136049897371<CUDNN_CONVOLUTION_FWD_ALGO_GEMM>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 108,
      "real_time": 6.4773875314535368e+06,
      "cpu_time": 7.0831612500004712e+06,
      "time_unit": "ns",
      "items_per_second": 2.0306546860333318e+12,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.1104179200000000e+08,
      "advised_time": 6.5136637687683105e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4280899851402570e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 1.0800000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 2.0306546860333318e+12,
      "predicted_advised_flops_count": 1.3153337344000000e+10,
      "predicted_flops": 2.0306546860333318e+12,
      "predicted_flops_count": 1.3153337344000000e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.1104179200000000e+08,
      "workspace_megabytes": 3.9200000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__5889100136049897371<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__5889100136049897371<CUDNN_CONVOLUTION_FWD_ALGO_FFT>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 61,
      "real_time": 1.1478037405453745e+07,
      "cpu_time": 1.3367229639344878e+07,
      "time_unit": "ns",
      "items_per_second": 1.1459570028714355e+12,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.7115381760000000e+09,
      "advised_time": 1.1676768302917480e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnMathType_t math_type = (cudnnMathType_t)0]": 2.3237108083472476e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 6.1000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 1.1459570028714355e+12,
      "predicted_advised_flops_count": 1.3153337344000000e+10,
      "predicted_flops": 1.2965943242290237e+12,
      "predicted_flops_count": 1.4882358153199755e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.7115381760000000e+09,
      "workspace_megabytes": 1.6322500000000000e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__5889100136049897371<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 66,
      "real_time": 1.0636957573958419e+07,
      "cpu_time": 1.2254441939394711e+07,
      "time_unit": "ns",
      "items_per_second": 1.2365695033138259e+12,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 6.3281603200000000e+08,
      "advised_time": 1.0820608139038086e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.6667819325789325e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 6.6000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 1.2365695033138259e+12,
      "predicted_advised_flops_count": 1.3153337344000000e+10,
      "predicted_flops": 1.3991179385386475e+12,
      "predicted_flops_count": 1.4882358153199755e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 6.3281603200000000e+08,
      "workspace_megabytes": 6.0350039672851562e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__5889100136049897371<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__5889100136049897371<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__13868055979005703433<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 44,
      "real_time": 1.5895936595783992e+07,
      "cpu_time": 1.9550359522728331e+07,
      "time_unit": "ns",
      "items_per_second": 1.5514976026351996e+11,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 1.5920288085937500e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4730153338016043e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 7.0000000000000000e+00,
      "filter_width": 7.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 3.0000000000000000e+00,
      "input[2]": 2.2400000000000000e+02,
      "input[3]": 2.2400000000000000e+02,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 3.0000000000000000e+00,
      "input_height": 2.2400000000000000e+02,
      "input_size": 3.8535168000000000e+07,
      "input_width": 2.2400000000000000e+02,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 4.4000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 1.1200000000000000e+02,
      "output_size": 2.0552089600000000e+08,
      "output_width": 1.1200000000000000e+02,
      "pad_height": 3.0000000000000000e+00,
      "pad_width": 3.0000000000000000e+00,
      "predicted_advised_flops": 1.9005845632281196e+12,
      "predicted_advised_flops_count": 3.0211571712000000e+10,
      "predicted_flops": 1.9005845632281196e+12,
      "predicted_flops_count": 3.0211571712000000e+10,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__13868055979005703433<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 74,
      "real_time": 9.4980142028951962e+06,
      "cpu_time": 1.0807552797297072e+07,
      "time_unit": "ns",
      "items_per_second": 2.5965961929686673e+11,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 7.5272000000000000e+04,
      "advised_time": 9.5610876083374023e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.2419525699096977e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 7.0000000000000000e+00,
      "filter_width": 7.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 3.0000000000000000e+00,
      "input[2]": 2.2400000000000000e+02,
      "input[3]": 2.2400000000000000e+02,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 3.0000000000000000e+00,
      "input_height": 2.2400000000000000e+02,
      "input_size": 3.8535168000000000e+07,
      "input_width": 2.2400000000000000e+02,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 7.4000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 1.1200000000000000e+02,
      "output_size": 2.0552089600000000e+08,
      "output_width": 1.1200000000000000e+02,
      "pad_height": 3.0000000000000000e+00,
      "pad_width": 3.0000000000000000e+00,
      "predicted_advised_flops": 3.1808303363866177e+12,
      "predicted_advised_flops_count": 3.0211571712000000e+10,
      "predicted_flops": 3.1808303363866177e+12,
      "predicted_flops_count": 3.0211571712000000e+10,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 7.5272000000000000e+04,
      "workspace_megabytes": 7.1784973144531250e-02
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__13868055979005703433<CUDNN_CONVOLUTION_FWD_ALGO_GEMM>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 37,
      "real_time": 1.8963838096808743e+07,
      "cpu_time": 2.4138713297296099e+07,
      "time_unit": "ns",
      "items_per_second": 1.3005019023100728e+11,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.8882232320000000e+09,
      "advised_time": 1.8986015319824219e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4280899851402570e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 7.0000000000000000e+00,
      "filter_width": 7.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 3.0000000000000000e+00,
      "input[2]": 2.2400000000000000e+02,
      "input[3]": 2.2400000000000000e+02,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 3.0000000000000000e+00,
      "input_height": 2.2400000000000000e+02,
      "input_size": 3.8535168000000000e+07,
      "input_width": 2.2400000000000000e+02,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 3.7000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 1.1200000000000000e+02,
      "output_size": 2.0552089600000000e+08,
      "output_width": 1.1200000000000000e+02,
      "pad_height": 3.0000000000000000e+00,
      "pad_width": 3.0000000000000000e+00,
      "predicted_advised_flops": 1.5931148303298391e+12,
      "predicted_advised_flops_count": 3.0211571712000000e+10,
      "predicted_flops": 1.5931148303298391e+12,
      "predicted_flops_count": 3.0211571712000000e+10,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 1.8882232320000000e+09,
      "workspace_megabytes": 1.8007500000000000e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__13868055979005703433<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__13868055979005703433<CUDNN_CONVOLUTION_FWD_ALGO_FFT>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__13868055979005703433<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 15,
      "real_time": 4.7289809087912239e+07,
      "cpu_time": 7.3439728333333209e+07,
      "time_unit": "ns",
      "items_per_second": 5.2151844119632935e+10,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.5017574400000000e+08,
      "advised_time": 4.8380126953125000e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.6667819325789325e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 7.0000000000000000e+00,
      "filter_width": 7.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 3.0000000000000000e+00,
      "input[2]": 2.2400000000000000e+02,
      "input[3]": 2.2400000000000000e+02,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 3.0000000000000000e+00,
      "input_height": 2.2400000000000000e+02,
      "input_size": 3.8535168000000000e+07,
      "input_width": 2.2400000000000000e+02,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 1.5000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 1.1200000000000000e+02,
      "output_size": 2.0552089600000000e+08,
      "output_width": 1.1200000000000000e+02,
      "pad_height": 3.0000000000000000e+00,
      "pad_width": 3.0000000000000000e+00,
      "predicted_advised_flops": 6.3886009046550342e+11,
      "predicted_advised_flops_count": 3.0211571712000000e+10,
      "predicted_flops": 3.3950214702319281e+11,
      "predicted_flops_count": 1.6054991717663103e+10,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 1.5017574400000000e+08,
      "workspace_megabytes": 1.4321875000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__13868055979005703433<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__13868055979005703433<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__8617613095419670733<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 296,
      "real_time": 2.3663997613963345e+06,
      "cpu_time": 2.4520254459465099e+06,
      "time_unit": "ns",
      "items_per_second": 1.1116750059371752e+13,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 2.3736319541931152e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4730153338016043e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 2.9600000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 2.7791875148429380e+12,
      "predicted_advised_flops_count": 6.5766686720000000e+09,
      "predicted_flops": 2.7791875148429380e+12,
      "predicted_flops_count": 6.5766686720000000e+09,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__8617613095419670733<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 400,
      "real_time": 1.7504264833405614e+06,
      "cpu_time": 1.7996201049999883e+06,
      "time_unit": "ns",
      "items_per_second": 1.5028722964586109e+13,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.7120000000000000e+03,
      "advised_time": 1.7960959672927856e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.2419525699096977e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 4.0000000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 3.7571807411465273e+12,
      "predicted_advised_flops_count": 6.5766686720000000e+09,
      "predicted_flops": 3.7571807411465273e+12,
      "predicted_flops_count": 6.5766686720000000e+09,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 4.7120000000000000e+03,
      "workspace_megabytes": 4.4937133789062500e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__8617613095419670733<CUDNN_CONVOLUTION_FWD_ALGO_GEMM>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 170,
      "real_time": 4.1089946267140261e+06,
      "cpu_time": 4.3568043470590031e+06,
      "time_unit": "ns",
      "items_per_second": 6.4022168627262275e+12,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.0552089600000000e+08,
      "advised_time": 4.1338558197021484e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4280899851402570e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 1.7000000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 1.6005542156815569e+12,
      "predicted_advised_flops_count": 6.5766686720000000e+09,
      "predicted_flops": 1.6005542156815569e+12,
      "predicted_flops_count": 6.5766686720000000e+09,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 2.0552089600000000e+08,
      "workspace_megabytes": 1.9600000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__8617613095419670733<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__8617613095419670733<CUDNN_CONVOLUTION_FWD_ALGO_FFT>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__8617613095419670733<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__8617613095419670733<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__8617613095419670733<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__14832155211702735389<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 77,
      "real_time": 9.1279555329835266e+06,
      "cpu_time": 1.0319978285716379e+07,
      "time_unit": "ns",
      "items_per_second": 3.6024872427541162e+11,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 9.1381759643554688e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4730153338016043e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 7.7000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "predicted_advised_flops": 3.2422385184787046e+12,
      "predicted_advised_flops_count": 2.9595009024000000e+10,
      "predicted_flops": 3.2422385184787046e+12,
      "predicted_flops_count": 2.9595009024000000e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__14832155211702735389<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 103,
      "real_time": 6.7822049028303437e+06,
      "cpu_time": 7.4448806990296068e+06,
      "time_unit": "ns",
      "items_per_second": 4.8484738858711200e+11,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.0000000000000000e+02,
      "advised_time": 6.8136639595031738e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.2419525699096977e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 1.0300000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "predicted_advised_flops": 4.3636264972840083e+12,
      "predicted_advised_flops_count": 2.9595009024000000e+10,
      "predicted_flops": 4.3636264972840083e+12,
      "predicted_flops_count": 2.9595009024000000e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 3.0000000000000000e+02,
      "workspace_megabytes": 2.8610229492187500e-04
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__14832155211702735389<CUDNN_CONVOLUTION_FWD_ALGO_GEMM>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 83,
      "real_time": 8.4599567105971184e+06,
      "cpu_time": 9.4851095421668012e+06,
      "time_unit": "ns",
      "items_per_second": 3.8869399081923950e+11,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.3121100800000000e+08,
      "advised_time": 8.4869117736816406e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4280899851402570e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 8.3000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "predicted_advised_flops": 3.4982459173731553e+12,
      "predicted_advised_flops_count": 2.9595009024000000e+10,
      "predicted_flops": 3.4982459173731553e+12,
      "predicted_flops_count": 2.9595009024000000e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 2.3121100800000000e+08,
      "workspace_megabytes": 2.2050000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__14832155211702735389<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__14832155211702735389<CUDNN_CONVOLUTION_FWD_ALGO_FFT>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 70,
      "real_time": 1.0025817129228797e+07,
      "cpu_time": 1.1463987085714068e+07,
      "time_unit": "ns",
      "items_per_second": 3.2798666618537701e+11,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 9.1540684800000000e+08,
      "advised_time": 1.0058464050292969e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnMathType_t math_type = (cudnnMathType_t)0]": 2.3237108083472476e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 7.0000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "predicted_advised_flops": 2.9518799956683931e+12,
      "predicted_advised_flops_count": 2.9595009024000000e+10,
      "predicted_flops": 3.4237377527420172e+11,
      "predicted_flops_count": 3.4325768607428222e+09,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 9.1540684800000000e+08,
      "workspace_megabytes": 8.7300000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__14832155211702735389<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 20,
      "real_time": 3.5752712003886700e+07,
      "cpu_time": 5.3640427200002708e+07,
      "time_unit": "ns",
      "items_per_second": 9.1974402826910675e+10,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.2817259520000000e+09,
      "advised_time": 3.6033535003662109e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.6667819325789325e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 2.0000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "predicted_advised_flops": 8.2776962544219604e+11,
      "predicted_advised_flops_count": 2.9595009024000000e+10,
      "predicted_flops": 9.6008852709401871e+10,
      "predicted_flops_count": 3.4325768607428222e+09,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 2.2817259520000000e+09,
      "workspace_megabytes": 2.1760234375000000e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__14832155211702735389<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 81,
      "real_time": 8.6038735591702983e+06,
      "cpu_time": 9.6719897901251707e+06,
      "time_unit": "ns",
      "items_per_second": 3.8219231296061786e+11,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.6216448000000000e+07,
      "advised_time": 8.8657922744750977e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)6; cudnnMathType_t math_type = (cudnnMathType_t)0]": 5.7500739770702787e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 6.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 8.1000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "predicted_advised_flops": 3.4397308166455605e+12,
      "predicted_advised_flops_count": 2.9595009024000000e+10,
      "predicted_flops": -1.1622671964235995e+02,
      "predicted_flops_count": -1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 2.6216448000000000e+07,
      "workspace_megabytes": 2.5001953125000000e+01
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_3_FLOAT32__BatchSize_256__14832155211702735389<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 255,
      "real_time": 2.7484307841708264e+06,
      "cpu_time": 2.8611809999992657e+06,
      "time_unit": "ns",
      "items_per_second": 1.1964406580433704e+12,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.8874368000000000e+08,
      "advised_time": 2.8078079223632812e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)7; cudnnMathType_t math_type = (cudnnMathType_t)0]": 8.2538945752816425e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 7.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 2.5500000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "predicted_advised_flops": 1.0767965922390334e+13,
      "predicted_advised_flops_count": 2.9595009024000000e+10,
      "predicted_flops": -3.6384398172198826e+02,
      "predicted_flops_count": -1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.8874368000000000e+08,
      "workspace_megabytes": 1.8000000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_4_FLOAT32__BatchSize_256__8422760723114437459<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 274,
      "real_time": 2.5571799109669495e+06,
      "cpu_time": 2.6565068795621982e+06,
      "time_unit": "ns",
      "items_per_second": 1.0287377346888598e+13,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 2.5722880363464355e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4730153338016043e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 2.7400000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 2.5718443367221494e+12,
      "predicted_advised_flops_count": 6.5766686720000000e+09,
      "predicted_flops": 2.5718443367221494e+12,
      "predicted_flops_count": 6.5766686720000000e+09,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_4_FLOAT32__BatchSize_256__8422760723114437459<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 411,
      "real_time": 1.7069540560979694e+06,
      "cpu_time": 1.7533898150848963e+06,
      "time_unit": "ns",
      "items_per_second": 1.5411472027627992e+13,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.1840000000000000e+03,
      "advised_time": 1.7674239873886108e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.2419525699096977e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 4.1100000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 3.8528680069069980e+12,
      "predicted_advised_flops_count": 6.5766686720000000e+09,
      "predicted_flops": 3.8528680069069980e+12,
      "predicted_flops_count": 6.5766686720000000e+09,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 1.1840000000000000e+03,
      "workspace_megabytes": 1.1291503906250000e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_4_FLOAT32__BatchSize_256__8422760723114437459<CUDNN_CONVOLUTION_FWD_ALGO_GEMM>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 212,
      "real_time": 3.3026067847561724e+06,
      "cpu_time": 3.4633462735844250e+06,
      "time_unit": "ns",
      "items_per_second": 7.9654274342993545e+12,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.0276044800000000e+08,
      "advised_time": 3.3157119750976562e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4280899851402570e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 2.1200000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 1.9913568585748386e+12,
      "predicted_advised_flops_count": 6.5766686720000000e+09,
      "predicted_flops": 1.9913568585748386e+12,
      "predicted_flops_count": 6.5766686720000000e+09,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 1.0276044800000000e+08,
      "workspace_megabytes": 9.8000000000000000e+01
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_4_FLOAT32__BatchSize_256__8422760723114437459<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_4_FLOAT32__BatchSize_256__8422760723114437459<CUDNN_CONVOLUTION_FWD_ALGO_FFT>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_4_FLOAT32__BatchSize_256__8422760723114437459<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_4_FLOAT32__BatchSize_256__8422760723114437459<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_4_FLOAT32__BatchSize_256__8422760723114437459<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_4_FLOAT32__BatchSize_256__12993157299225267682<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 76,
      "real_time": 9.1464404801004808e+06,
      "cpu_time": 1.0356642671052372e+07,
      "time_unit": "ns",
      "items_per_second": 1.1504661182777850e+13,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 9.1637763977050781e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4730153338016043e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.0480000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.0480000000000000e+03,
      "num_iterations": 7.6000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 2.8761652956944624e+12,
      "predicted_advised_flops_count": 2.6306674688000000e+10,
      "predicted_flops": 2.8761652956944624e+12,
      "predicted_flops_count": 2.6306674688000000e+10,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_4_FLOAT32__BatchSize_256__12993157299225267682<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 100,
      "real_time": 7.0199052803218365e+06,
      "cpu_time": 7.7279704900004733e+06,
      "time_unit": "ns",
      "items_per_second": 1.4989760481095230e+13,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.0000000000000000e+02,
      "advised_time": 7.0389761924743652e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.2419525699096977e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.0480000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.0480000000000000e+03,
      "num_iterations": 1.0000000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 3.7474401202738076e+12,
      "predicted_advised_flops_count": 2.6306674688000000e+10,
      "predicted_flops": 3.7474401202738076e+12,
      "predicted_flops_count": 2.6306674688000000e+10,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 3.0000000000000000e+02,
      "workspace_megabytes": 2.8610229492187500e-04
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_4_FLOAT32__BatchSize_256__12993157299225267682<CUDNN_CONVOLUTION_FWD_ALGO_GEMM>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 93,
      "real_time": 7.5198988560386887e+06,
      "cpu_time": 8.3339275591399232e+06,
      "time_unit": "ns",
      "items_per_second": 1.3993100275211816e+13,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 5.1380224000000000e+07,
      "advised_time": 7.5803837776184082e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4280899851402570e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.0480000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.0480000000000000e+03,
      "num_iterations": 9.3000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "predicted_advised_flops": 3.4982750688029541e+12,
      "predicted_advised_flops_count": 2.6306674688000000e+10,
      "predicted_flops": 3.4982750688029541e+12,
      "predicted_flops_count": 2.6306674688000000e+10,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 5.1380224000000000e+07,
      "workspace_megabytes": 4.9000000000000000e+01
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_4_FLOAT32__BatchSize_256__12993157299225267682<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_4_FLOAT32__BatchSize_256__12993157299225267682<CUDNN_CONVOLUTION_FWD_ALGO_FFT>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_4_FLOAT32__BatchSize_256__12993157299225267682<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_4_FLOAT32__BatchSize_256__12993157299225267682<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_4_FLOAT32__BatchSize_256__12993157299225267682<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_4_FLOAT32__BatchSize_256__3861710050714821281<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 49,
      "real_time": 1.4189117283997487e+07,
      "cpu_time": 1.7096277816328753e+07,
      "time_unit": "ns",
      "items_per_second": 2.3175045143284491e+11,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 1.4215167999267578e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4730153338016043e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 4.9000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "predicted_advised_flops": -7.0476547623424182e+01,
      "predicted_advised_flops_count": -1.0000000000000000e+00,
      "predicted_flops": 2.0857540628956042e+12,
      "predicted_flops_count": 2.9595009024000000e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_4_FLOAT32__BatchSize_256__3861710050714821281<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 88,
      "real_time": 7.9205242324281819e+06,
      "cpu_time": 8.8293268409103118e+06,
      "time_unit": "ns",
      "items_per_second": 4.1516624903903625e+11,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.8824000000000000e+04,
      "advised_time": 7.9472641944885254e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.2419525699096977e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 8.8000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "predicted_advised_flops": -1.2625426937093425e+02,
      "predicted_advised_flops_count": -1.0000000000000000e+00,
      "predicted_flops": 3.7364962413513262e+12,
      "predicted_flops_count": 2.9595009024000000e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.8824000000000000e+04,
      "workspace_megabytes": 1.7951965332031250e-02
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_4_FLOAT32__BatchSize_256__3861710050714821281<CUDNN_CONVOLUTION_FWD_ALGO_GEMM>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 40,
      "real_time": 1.7493746429681778e+07,
      "cpu_time": 2.1871333624998357e+07,
      "time_unit": "ns",
      "items_per_second": 1.8797199040341965e+11,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.8496880640000000e+09,
      "advised_time": 1.7592319488525391e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4280899851402570e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 4.0000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "predicted_advised_flops": -5.7163284263872264e+01,
      "predicted_advised_flops_count": -1.0000000000000000e+00,
      "predicted_flops": 1.6917479136307769e+12,
      "predicted_flops_count": 2.9595009024000000e+10,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.8496880640000000e+09,
      "workspace_megabytes": 1.7640000000000000e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_4_FLOAT32__BatchSize_256__3861710050714821281<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_4_FLOAT32__BatchSize_256__3861710050714821281<CUDNN_CONVOLUTION_FWD_ALGO_FFT>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 89,
      "real_time": 7.8361170429192232e+06,
      "cpu_time": 8.7249506404517759e+06,
      "time_unit": "ns",
      "items_per_second": 4.1963823638537463e+11,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 6.9220761600000000e+08,
      "advised_time": 8.2749443054199219e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnMathType_t math_type = (cudnnMathType_t)0]": 2.3237108083472476e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 8.9000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "predicted_advised_flops": -1.2761422456082478e+02,
      "predicted_advised_flops_count": -1.0000000000000000e+00,
      "predicted_flops": 5.9098896761979346e+11,
      "predicted_flops_count": 4.6310587213427000e+09,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 6.9220761600000000e+08,
      "workspace_megabytes": 6.6014062500000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_4_FLOAT32__BatchSize_256__3861710050714821281<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 126,
      "real_time": 5.5405625644775610e+06,
      "cpu_time": 5.9850354523813538e+06,
      "time_unit": "ns",
      "items_per_second": 5.9350188680886572e+11,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.0308761600000000e+08,
      "advised_time": 5.7743358612060547e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.6667819325789325e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 1.2600000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "predicted_advised_flops": -1.8048708743248233e+02,
      "predicted_advised_flops_count": -1.0000000000000000e+00,
      "predicted_flops": 8.3584630034393958e+11,
      "predicted_flops_count": 4.6310587213427000e+09,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 3.0308761600000000e+08,
      "workspace_megabytes": 2.8904687500000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_4_FLOAT32__BatchSize_256__3861710050714821281<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 146,
      "real_time": 4.8024196860265648e+06,
      "cpu_time": 5.1380057260252144e+06,
      "time_unit": "ns",
      "items_per_second": 6.8472448286182751e+11,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.0985600000000000e+05,
      "advised_time": 4.8782401084899902e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)6; cudnnMathType_t math_type = (cudnnMathType_t)0]": 5.7500739770702787e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 6.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 1.4600000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "predicted_advised_flops": -2.0822836515302183e+02,
      "predicted_advised_flops_count": -1.0000000000000000e+00,
      "predicted_flops": -2.0822836515302183e+02,
      "predicted_flops_count": -1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.0985600000000000e+05,
      "workspace_megabytes": 3.9086914062500000e-01
    },
    {
      "name": "LAYER_CUDNN_CONV_FWD_4_FLOAT32__BatchSize_256__3861710050714821281<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:1/conv_bwd_type:0/manual_time",
      "iterations": 122,
      "real_time": 5.7009781031396054e+06,
      "cpu_time": 6.1755157950817300e+06,
      "time_unit": "ns",
      "items_per_second": 5.7680178322191235e+11,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 9.2543385600000000e+08,
      "advised_time": 6.1808638572692871e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_fwd.inc": 6.3416289576367145e+18,
      "benchmark_func:void iLAYER_CUDNN_CONV_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)7; cudnnMathType_t math_type = (cudnnMathType_t)0]": 8.2538945752816425e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 1.0000000000000000e+00,
      "convolution_algorithm": 7.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_FWD_Impl": 7.6772958670751939e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 1.2200000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "predicted_advised_flops": -1.7540849691200998e+02,
      "predicted_advised_flops_count": -1.0000000000000000e+00,
      "predicted_flops": -1.7540849691200998e+02,
      "predicted_flops_count": -1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 9.2543385600000000e+08,
      "workspace_megabytes": 8.8256250000000000e+02
    },
    {
      "name": "LAYER_CUDNN_OP_TENSOR_ADD_FWD_FLOAT32__BatchSize_256__620774724954516720/input[0]:256/input[1]:512/input[2]:28/input[3]:28/input[4]:-1/input[5]:-1/input[6]:-1/input[7]:-1/batch_size:256/manual_time",
      "iterations": 198,
      "real_time": 3.5424410492520439e+06,
      "cpu_time": 3.7269148030313863e+06,
      "time_unit": "ns",
      "items_per_second": 2.9008372072048168e+10,
      "alpha": 1.0000000000000000e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_op_tensor.cpp": 1.3194847084940028e+19,
      "benchmark_func:void iLAYER_CUDNN_OP_TENSOR_Impl(benchmark::State&) [with T = float; cudnnOpTensorOp_t op_type = (cudnnOpTensorOp_t)0]": 1.4844646694895837e+19,
      "beta": 0.0000000000000000e+00,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_OP_TENSOR_Impl": 3.0075642356307840e+18,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input[4]": -1.0000000000000000e+00,
      "input[5]": -1.0000000000000000e+00,
      "input[6]": -1.0000000000000000e+00,
      "input[7]": -1.0000000000000000e+00,
      "input_c": 5.1200000000000000e+02,
      "input_h": 2.8000000000000000e+01,
      "input_n": 2.5600000000000000e+02,
      "input_size": 1.0276044800000000e+08,
      "input_w": 2.8000000000000000e+01,
      "num_iterations": 1.9800000000000000e+02,
      "op_type": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_OP_TENSOR_ADD_FWD_FLOAT32__BatchSize_256__14293017161603585676/input[0]:256/input[1]:256/input[2]:56/input[3]:56/input[4]:-1/input[5]:-1/input[6]:-1/input[7]:-1/batch_size:256/manual_time",
      "iterations": 92,
      "real_time": 7.6100615906002726e+06,
      "cpu_time": 8.4422512934773527e+06,
      "time_unit": "ns",
      "items_per_second": 2.7006469468506462e+10,
      "alpha": 1.0000000000000000e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_op_tensor.cpp": 1.3194847084940028e+19,
      "benchmark_func:void iLAYER_CUDNN_OP_TENSOR_Impl(benchmark::State&) [with T = float; cudnnOpTensorOp_t op_type = (cudnnOpTensorOp_t)0]": 1.4844646694895837e+19,
      "beta": 0.0000000000000000e+00,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_OP_TENSOR_Impl": 3.0075642356307840e+18,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input[4]": -1.0000000000000000e+00,
      "input[5]": -1.0000000000000000e+00,
      "input[6]": -1.0000000000000000e+00,
      "input[7]": -1.0000000000000000e+00,
      "input_c": 2.5600000000000000e+02,
      "input_h": 5.6000000000000000e+01,
      "input_n": 2.5600000000000000e+02,
      "input_size": 2.0552089600000000e+08,
      "input_w": 5.6000000000000000e+01,
      "num_iterations": 9.2000000000000000e+01,
      "op_type": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_OP_TENSOR_ADD_FWD_FLOAT32__BatchSize_256__5746488549407274608/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/input[4]:-1/input[5]:-1/input[6]:-1/input[7]:-1/batch_size:256/manual_time",
      "iterations": 383,
      "real_time": 1.8265495473742678e+06,
      "cpu_time": 1.8794474490858242e+06,
      "time_unit": "ns",
      "items_per_second": 2.8129663426793407e+10,
      "alpha": 1.0000000000000000e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_op_tensor.cpp": 1.3194847084940028e+19,
      "benchmark_func:void iLAYER_CUDNN_OP_TENSOR_Impl(benchmark::State&) [with T = float; cudnnOpTensorOp_t op_type = (cudnnOpTensorOp_t)0]": 1.4844646694895837e+19,
      "beta": 0.0000000000000000e+00,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_OP_TENSOR_Impl": 3.0075642356307840e+18,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input[4]": -1.0000000000000000e+00,
      "input[5]": -1.0000000000000000e+00,
      "input[6]": -1.0000000000000000e+00,
      "input[7]": -1.0000000000000000e+00,
      "input_c": 1.0240000000000000e+03,
      "input_h": 1.4000000000000000e+01,
      "input_n": 2.5600000000000000e+02,
      "input_size": 5.1380224000000000e+07,
      "input_w": 1.4000000000000000e+01,
      "num_iterations": 3.8300000000000000e+02,
      "op_type": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_OP_TENSOR_ADD_FWD_FLOAT32__BatchSize_256__8349982516626710231/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/input[4]:-1/input[5]:-1/input[6]:-1/input[7]:-1/batch_size:256/manual_time",
      "iterations": 498,
      "real_time": 1.4061964924417317e+06,
      "cpu_time": 1.4395987028104039e+06,
      "time_unit": "ns",
      "items_per_second": 1.8269219229377731e+10,
      "alpha": 1.0000000000000000e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_op_tensor.cpp": 1.3194847084940028e+19,
      "benchmark_func:void iLAYER_CUDNN_OP_TENSOR_Impl(benchmark::State&) [with T = float; cudnnOpTensorOp_t op_type = (cudnnOpTensorOp_t)0]": 1.4844646694895837e+19,
      "beta": 0.0000000000000000e+00,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_OP_TENSOR_Impl": 3.0075642356307840e+18,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.0480000000000000e+03,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input[4]": -1.0000000000000000e+00,
      "input[5]": -1.0000000000000000e+00,
      "input[6]": -1.0000000000000000e+00,
      "input[7]": -1.0000000000000000e+00,
      "input_c": 2.0480000000000000e+03,
      "input_h": 7.0000000000000000e+00,
      "input_n": 2.5600000000000000e+02,
      "input_size": 2.5690112000000000e+07,
      "input_w": 7.0000000000000000e+00,
      "num_iterations": 4.9800000000000000e+02,
      "op_type": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_POOLING_FWD_FLOAT32__BatchSize_256__12631777757240854768<CUDNN_POOLING_MAX>/input[0]:256/input[1]:64/input[2]:112/input[3]:112/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:2/stride_width:2/batch_size:256/manual_time",
      "iterations": 177,
      "real_time": 3.9481835592390792e+06,
      "cpu_time": 4.1787357909612893e+06,
      "time_unit": "ns",
      "items_per_second": 5.2054544302800705e+10,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_pooling_fwd.cpp": 9.1239988607066061e+18,
      "benchmark_func:void iLAYER_CUDNN_POOLING_FWD_Impl(benchmark::State&) [with T = float; cudnnPoolingMode_t pooling_mode = (cudnnPoolingMode_t)0]": 1.5686689287215342e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_POOLING_FWD_Impl": 1.4645785230092718e+19,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "horizontal_padding": 1.0000000000000000e+00,
      "horizontal_stride": 2.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 1.1200000000000000e+02,
      "input[3]": 1.1200000000000000e+02,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 1.1200000000000000e+02,
      "input_size": 2.0552089600000000e+08,
      "input_width": 1.1200000000000000e+02,
      "num_iterations": 1.7700000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "pooling_mode": 0.0000000000000000e+00,
      "predicted_flops": 5.2054544302800705e+10,
      "predicted_flops_count": 2.0552089600000000e+08,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "vertical_padding": 1.0000000000000000e+00,
      "vertical_stride": 2.0000000000000000e+00,
      "window_height": 3.0000000000000000e+00,
      "window_width": 3.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_POOLING_FWD_FLOAT32__BatchSize_256__12631777757240854768<CUDNN_POOLING_MAX_DETERMINISTIC>/input[0]:256/input[1]:64/input[2]:112/input[3]:112/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:2/stride_width:2/batch_size:256/manual_time",
      "iterations": 177,
      "real_time": 3.9496562933012592e+06,
      "cpu_time": 4.1800770112995659e+06,
      "time_unit": "ns",
      "items_per_second": 5.2035134385888184e+10,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_pooling_fwd.cpp": 9.1239988607066061e+18,
      "benchmark_func:void iLAYER_CUDNN_POOLING_FWD_Impl(benchmark::State&) [with T = float; cudnnPoolingMode_t pooling_mode = (cudnnPoolingMode_t)3]": 1.5689715143215610e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_POOLING_FWD_Impl": 1.4645785230092718e+19,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "horizontal_padding": 1.0000000000000000e+00,
      "horizontal_stride": 2.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 1.1200000000000000e+02,
      "input[3]": 1.1200000000000000e+02,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 1.1200000000000000e+02,
      "input_size": 2.0552089600000000e+08,
      "input_width": 1.1200000000000000e+02,
      "num_iterations": 1.7700000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "pooling_mode": 3.0000000000000000e+00,
      "predicted_flops": 5.2035134385888184e+10,
      "predicted_flops_count": 2.0552089600000000e+08,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "vertical_padding": 1.0000000000000000e+00,
      "vertical_stride": 2.0000000000000000e+00,
      "window_height": 3.0000000000000000e+00,
      "window_width": 3.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 148,
      "real_time": 4.7289997691640984e+06,
      "cpu_time": 5.0532634121624772e+06,
      "time_unit": "ns",
      "items_per_second": 2.7814205933710576e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 4.6274561882019043e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 1.4800000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 148,
      "real_time": 4.7277517371332729e+06,
      "cpu_time": 5.0519749864884345e+06,
      "time_unit": "ns",
      "items_per_second": 2.7821548328541631e+12,
      "activation_mode": 3.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 4.6366720199584961e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3; cudnnMathType_t math_type = (cudnnMathType_t)0]": 3.7948729185673933e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 1.4800000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 148,
      "real_time": 4.7257731706407424e+06,
      "cpu_time": 5.0501012094578166e+06,
      "time_unit": "ns",
      "items_per_second": 2.7833196535365259e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 4.6558079719543457e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 1.4800000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 148,
      "real_time": 4.7270910204010634e+06,
      "cpu_time": 5.0510620810812861e+06,
      "time_unit": "ns",
      "items_per_second": 2.7825437012389116e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 4.6375679969787598e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 1.4800000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 148,
      "real_time": 4.7271548285828652e+06,
      "cpu_time": 5.0506453581072716e+06,
      "time_unit": "ns",
      "items_per_second": 2.7825061418483696e+12,
      "activation_mode": 5.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 4.6415362358093262e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.2315062669022114e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 1.4800000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 227,
      "real_time": 3.0805034427554859e+06,
      "cpu_time": 3.2212538193820496e+06,
      "time_unit": "ns",
      "items_per_second": 4.2698661398782412e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.7120000000000000e+03,
      "advised_time": 3.1581759452819824e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 2.2700000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.7120000000000000e+03,
      "workspace_megabytes": 4.4937133789062500e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 227,
      "real_time": 3.0799075634944937e+06,
      "cpu_time": 3.2203426343593318e+06,
      "time_unit": "ns",
      "items_per_second": 4.2706922441127070e+12,
      "activation_mode": 3.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.7120000000000000e+03,
      "advised_time": 3.1511359214782715e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.3805875302202324e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 2.2700000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.7120000000000000e+03,
      "workspace_megabytes": 4.4937133789062500e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 227,
      "real_time": 3.0796628348972434e+06,
      "cpu_time": 3.2203263656378821e+06,
      "time_unit": "ns",
      "items_per_second": 4.2710316190956909e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.7120000000000000e+03,
      "advised_time": 3.1150081157684326e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 2.2700000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.7120000000000000e+03,
      "workspace_megabytes": 4.4937133789062500e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 227,
      "real_time": 3.0841294630935276e+06,
      "cpu_time": 3.2246645462537706e+06,
      "time_unit": "ns",
      "items_per_second": 4.2648460453429155e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.7120000000000000e+03,
      "advised_time": 3.1597120761871338e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 2.2700000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.7120000000000000e+03,
      "workspace_megabytes": 4.4937133789062500e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 227,
      "real_time": 3.0842007738886820e+06,
      "cpu_time": 3.2247308590312228e+06,
      "time_unit": "ns",
      "items_per_second": 4.2647474364698228e+12,
      "activation_mode": 5.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.7120000000000000e+03,
      "advised_time": 3.1539199352264404e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 8.1554158113550264e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 2.2700000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.7120000000000000e+03,
      "workspace_megabytes": 4.4937133789062500e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 110,
      "real_time": 6.3560026232153177e+06,
      "cpu_time": 6.9378358909096448e+06,
      "time_unit": "ns",
      "items_per_second": 2.0694354807150957e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.1104179200000000e+08,
      "advised_time": 6.5136637687683105e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 1.1000000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.1104179200000000e+08,
      "workspace_megabytes": 3.9200000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 110,
      "real_time": 6.3555825628678910e+06,
      "cpu_time": 6.9374370454547517e+06,
      "time_unit": "ns",
      "items_per_second": 2.0695722561842849e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.1104179200000000e+08,
      "advised_time": 6.5368638038635254e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 1.1000000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.1104179200000000e+08,
      "workspace_megabytes": 3.9200000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 110,
      "real_time": 6.3556523654948585e+06,
      "cpu_time": 6.9373100363675756e+06,
      "time_unit": "ns",
      "items_per_second": 2.0695495265616006e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.1104179200000000e+08,
      "advised_time": 6.5167679786682129e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 1.1000000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.1104179200000000e+08,
      "workspace_megabytes": 3.9200000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 62,
      "real_time": 1.1362829936608192e+07,
      "cpu_time": 1.3200339483872877e+07,
      "time_unit": "ns",
      "items_per_second": 1.1575758343107153e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.7115381760000000e+09,
      "advised_time": 1.1554816246032715e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.5608866559154586e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 6.2000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.7115381760000000e+09,
      "workspace_megabytes": 1.6322500000000000e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 62,
      "real_time": 1.1361112803100578e+07,
      "cpu_time": 1.3197972387090322e+07,
      "time_unit": "ns",
      "items_per_second": 1.1577507918423540e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.7115381760000000e+09,
      "advised_time": 1.1558912277221680e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.5608866559154586e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 6.2000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.7115381760000000e+09,
      "workspace_megabytes": 1.6322500000000000e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 62,
      "real_time": 1.1360335524284070e+07,
      "cpu_time": 1.3197088193546476e+07,
      "time_unit": "ns",
      "items_per_second": 1.1578300056264336e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.7115381760000000e+09,
      "advised_time": 1.1561984062194824e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.5608866559154586e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 6.2000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.7115381760000000e+09,
      "workspace_megabytes": 1.6322500000000000e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 64,
      "real_time": 1.0994733005645685e+07,
      "cpu_time": 1.2713545765625333e+07,
      "time_unit": "ns",
      "items_per_second": 1.1963307646712197e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 6.3281603200000000e+08,
      "advised_time": 1.0798080444335938e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 6.4000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 6.3281603200000000e+08,
      "workspace_megabytes": 6.0350039672851562e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 64,
      "real_time": 1.0992156487191096e+07,
      "cpu_time": 1.2711257390627395e+07,
      "time_unit": "ns",
      "items_per_second": 1.1966111799197253e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 6.3281603200000000e+08,
      "advised_time": 1.0798111915588379e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 6.4000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 6.3281603200000000e+08,
      "workspace_megabytes": 6.0350039672851562e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 64,
      "real_time": 1.0994318974553607e+07,
      "cpu_time": 1.2713132531255944e+07,
      "time_unit": "ns",
      "items_per_second": 1.1963758168599119e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 6.3281603200000000e+08,
      "advised_time": 1.0801152229309082e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 6.4000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 6.3281603200000000e+08,
      "workspace_megabytes": 6.0350039672851562e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__15480260549307381539<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 288,
      "real_time": 2.4363452208086122e+06,
      "cpu_time": 2.5258480069434182e+06,
      "time_unit": "ns",
      "items_per_second": 1.0797597345120463e+13,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 2.3552000522613525e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 2.8800000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 287,
      "real_time": 2.4349473494685902e+06,
      "cpu_time": 2.5250021463407171e+06,
      "time_unit": "ns",
      "items_per_second": 1.0803796104150361e+13,
      "activation_mode": 3.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 2.3193280696868896e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3; cudnnMathType_t math_type = (cudnnMathType_t)0]": 3.7948729185673933e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 2.8700000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 287,
      "real_time": 2.4355401809064457e+06,
      "cpu_time": 2.5256199965147441e+06,
      "time_unit": "ns",
      "items_per_second": 1.0801166367211945e+13,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 2.3408639430999756e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 2.8700000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 287,
      "real_time": 2.4360159463946829e+06,
      "cpu_time": 2.5256739268286112e+06,
      "time_unit": "ns",
      "items_per_second": 1.0799056848102340e+13,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 2.3449599742889404e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 2.8700000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 287,
      "real_time": 2.4366373283859952e+06,
      "cpu_time": 2.5265897630657740e+06,
      "time_unit": "ns",
      "items_per_second": 1.0796302913665566e+13,
      "activation_mode": 5.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 2.3283839225769043e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.2315062669022114e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 2.8700000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 400,
      "real_time": 1.7485652057803238e+06,
      "cpu_time": 1.7973428950008953e+06,
      "time_unit": "ns",
      "items_per_second": 1.5044720437668922e+13,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.7120000000000000e+03,
      "advised_time": 1.7592320442199707e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 4.0000000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 4.7120000000000000e+03,
      "workspace_megabytes": 4.4937133789062500e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 400,
      "real_time": 1.7491553630679846e+06,
      "cpu_time": 1.7979280350002113e+06,
      "time_unit": "ns",
      "items_per_second": 1.5039644415495832e+13,
      "activation_mode": 3.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.7120000000000000e+03,
      "advised_time": 1.7797119617462158e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.3805875302202324e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 4.0000000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 4.7120000000000000e+03,
      "workspace_megabytes": 4.4937133789062500e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 400,
      "real_time": 1.7494066458311861e+06,
      "cpu_time": 1.7979758199997775e+06,
      "time_unit": "ns",
      "items_per_second": 1.5037484138229650e+13,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.7120000000000000e+03,
      "advised_time": 1.7797119617462158e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 4.0000000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 4.7120000000000000e+03,
      "workspace_megabytes": 4.4937133789062500e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 400,
      "real_time": 1.7489385671797211e+06,
      "cpu_time": 1.7975130999992928e+06,
      "time_unit": "ns",
      "items_per_second": 1.5041508708005252e+13,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.7120000000000000e+03,
      "advised_time": 1.7807040214538574e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 4.0000000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 4.7120000000000000e+03,
      "workspace_megabytes": 4.4937133789062500e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 400,
      "real_time": 1.7492380010662600e+06,
      "cpu_time": 1.7979462900002829e+06,
      "time_unit": "ns",
      "items_per_second": 1.5038933908344426e+13,
      "activation_mode": 5.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.7120000000000000e+03,
      "advised_time": 1.7805759906768799e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 8.1554158113550264e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 4.0000000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 4.7120000000000000e+03,
      "workspace_megabytes": 4.4937133789062500e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 168,
      "real_time": 4.1562533283251384e+06,
      "cpu_time": 4.4080046904783538e+06,
      "time_unit": "ns",
      "items_per_second": 6.3294204202420225e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.0552089600000000e+08,
      "advised_time": 4.1615362167358398e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 1.6800000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 2.0552089600000000e+08,
      "workspace_megabytes": 1.9600000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 168,
      "real_time": 4.1558201663151737e+06,
      "cpu_time": 4.4076144523828020e+06,
      "time_unit": "ns",
      "items_per_second": 6.3300801370635928e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.0552089600000000e+08,
      "advised_time": 4.1481919288635254e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 1.6800000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 2.0552089600000000e+08,
      "workspace_megabytes": 1.9600000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 168,
      "real_time": 4.1532699140675720e+06,
      "cpu_time": 4.4044335238098139e+06,
      "time_unit": "ns",
      "items_per_second": 6.3339670265340723e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.0552089600000000e+08,
      "advised_time": 4.1246719360351562e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 1.6800000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 2.0552089600000000e+08,
      "workspace_megabytes": 1.9600000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__17363996559804377205<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:128/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 146,
      "real_time": 4.7897385044202004e+06,
      "cpu_time": 5.1228544041105406e+06,
      "time_unit": "ns",
      "items_per_second": 2.7461493632400742e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 4.8463678359985352e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.0240000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.0240000000000000e+03,
      "num_iterations": 1.4600000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 146,
      "real_time": 4.7906080024852734e+06,
      "cpu_time": 5.1233995616429057e+06,
      "time_unit": "ns",
      "items_per_second": 2.7456509355756733e+12,
      "activation_mode": 3.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 4.8240637779235840e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3; cudnnMathType_t math_type = (cudnnMathType_t)0]": 3.7948729185673933e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.0240000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.0240000000000000e+03,
      "num_iterations": 1.4600000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 146,
      "real_time": 4.7907574893268822e+06,
      "cpu_time": 5.1245563219162999e+06,
      "time_unit": "ns",
      "items_per_second": 2.7455652625505972e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 4.8402562141418457e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.0240000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.0240000000000000e+03,
      "num_iterations": 1.4600000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 146,
      "real_time": 4.7879145314840423e+06,
      "cpu_time": 5.1197495205466980e+06,
      "time_unit": "ns",
      "items_per_second": 2.7471955185304961e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 4.8383998870849609e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.0240000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.0240000000000000e+03,
      "num_iterations": 1.4600000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 146,
      "real_time": 4.7899544085950069e+06,
      "cpu_time": 5.1231086301389458e+06,
      "time_unit": "ns",
      "items_per_second": 2.7460255822890278e+12,
      "activation_mode": 5.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 4.8414721488952637e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.2315062669022114e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.0240000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.0240000000000000e+03,
      "num_iterations": 1.4600000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 212,
      "real_time": 3.3075870003744536e+06,
      "cpu_time": 3.4684100377361644e+06,
      "time_unit": "ns",
      "items_per_second": 3.9767169669341738e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.1840000000000000e+03,
      "advised_time": 3.3433599472045898e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.0240000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.0240000000000000e+03,
      "num_iterations": 2.1200000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.1840000000000000e+03,
      "workspace_megabytes": 1.1291503906250000e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 212,
      "real_time": 3.3071444255633736e+06,
      "cpu_time": 3.4681175424511158e+06,
      "time_unit": "ns",
      "items_per_second": 3.9772491465229321e+12,
      "activation_mode": 3.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.1840000000000000e+03,
      "advised_time": 3.3187839984893799e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.3805875302202324e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.0240000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.0240000000000000e+03,
      "num_iterations": 2.1200000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.1840000000000000e+03,
      "workspace_megabytes": 1.1291503906250000e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 212,
      "real_time": 3.3075531981973294e+06,
      "cpu_time": 3.4684551415073476e+06,
      "time_unit": "ns",
      "items_per_second": 3.9767576077593506e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.1840000000000000e+03,
      "advised_time": 3.3434560298919678e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.0240000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.0240000000000000e+03,
      "num_iterations": 2.1200000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.1840000000000000e+03,
      "workspace_megabytes": 1.1291503906250000e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 212,
      "real_time": 3.3075273759610387e+06,
      "cpu_time": 3.4683040424530399e+06,
      "time_unit": "ns",
      "items_per_second": 3.9767886547509380e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.1840000000000000e+03,
      "advised_time": 3.3228800296783447e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.0240000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.0240000000000000e+03,
      "num_iterations": 2.1200000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.1840000000000000e+03,
      "workspace_megabytes": 1.1291503906250000e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 212,
      "real_time": 3.3075476069671083e+06,
      "cpu_time": 3.4683530943401870e+06,
      "time_unit": "ns",
      "items_per_second": 3.9767643302528594e+12,
      "activation_mode": 5.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.1840000000000000e+03,
      "advised_time": 3.3443839550018311e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 8.1554158113550264e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.0240000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.0240000000000000e+03,
      "num_iterations": 2.1200000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.1840000000000000e+03,
      "workspace_megabytes": 1.1291503906250000e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 129,
      "real_time": 5.4608022310417295e+06,
      "cpu_time": 5.8878471782974014e+06,
      "time_unit": "ns",
      "items_per_second": 2.4086822388897988e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 5.1380224000000000e+07,
      "advised_time": 5.5685119628906250e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.0240000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.0240000000000000e+03,
      "num_iterations": 1.2900000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 5.1380224000000000e+07,
      "workspace_megabytes": 4.9000000000000000e+01
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 128,
      "real_time": 5.4638742585666478e+06,
      "cpu_time": 5.8951136718756733e+06,
      "time_unit": "ns",
      "items_per_second": 2.4073279730728188e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 5.1380224000000000e+07,
      "advised_time": 5.5459837913513184e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.0240000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.0240000000000000e+03,
      "num_iterations": 1.2800000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 5.1380224000000000e+07,
      "workspace_megabytes": 4.9000000000000000e+01
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 128,
      "real_time": 5.4594847679254599e+06,
      "cpu_time": 5.8898498828150462e+06,
      "time_unit": "ns",
      "items_per_second": 2.4092634933750562e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 5.1380224000000000e+07,
      "advised_time": 5.5633921623229980e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.0240000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.0240000000000000e+03,
      "num_iterations": 1.2800000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 5.1380224000000000e+07,
      "workspace_megabytes": 4.9000000000000000e+01
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 65,
      "real_time": 1.0735919770713035e+07,
      "cpu_time": 1.2391641246153120e+07,
      "time_unit": "ns",
      "items_per_second": 1.2251709797498245e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 7.5602329600000000e+08,
      "advised_time": 1.0656736373901367e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.5608866559154586e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.0240000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.0240000000000000e+03,
      "num_iterations": 6.5000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 7.5602329600000000e+08,
      "workspace_megabytes": 7.2100000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 65,
      "real_time": 1.0737585276365280e+07,
      "cpu_time": 1.2393842215383828e+07,
      "time_unit": "ns",
      "items_per_second": 1.2249809436160737e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 7.5602329600000000e+08,
      "advised_time": 1.0653440475463867e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.5608866559154586e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.0240000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.0240000000000000e+03,
      "num_iterations": 6.5000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 7.5602329600000000e+08,
      "workspace_megabytes": 7.2100000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 65,
      "real_time": 1.0735749926131506e+07,
      "cpu_time": 1.2392336846161155e+07,
      "time_unit": "ns",
      "items_per_second": 1.2251903625273472e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 7.5602329600000000e+08,
      "advised_time": 1.0643615722656250e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.5608866559154586e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.0240000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.0240000000000000e+03,
      "num_iterations": 6.5000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 7.5602329600000000e+08,
      "workspace_megabytes": 7.2100000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 39,
      "real_time": 1.7802501049561378e+07,
      "cpu_time": 2.2363242230767354e+07,
      "time_unit": "ns",
      "items_per_second": 7.3884772186681458e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 6.5955472000000000e+08,
      "advised_time": 1.6199680328369141e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.0240000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.0240000000000000e+03,
      "num_iterations": 3.9000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 6.5955472000000000e+08,
      "workspace_megabytes": 6.2900039672851562e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 39,
      "real_time": 1.7800186211482074e+07,
      "cpu_time": 2.2360568589744955e+07,
      "time_unit": "ns",
      "items_per_second": 7.3894380585273840e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 6.5955472000000000e+08,
      "advised_time": 1.6200704574584961e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.0240000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.0240000000000000e+03,
      "num_iterations": 3.9000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 6.5955472000000000e+08,
      "workspace_megabytes": 6.2900039672851562e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 39,
      "real_time": 1.7802625655745849e+07,
      "cpu_time": 2.2363708410253845e+07,
      "time_unit": "ns",
      "items_per_second": 7.3884255043888550e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 6.5955472000000000e+08,
      "advised_time": 1.6197631835937500e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.0240000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.0240000000000000e+03,
      "num_iterations": 3.9000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 6.5955472000000000e+08,
      "workspace_megabytes": 6.2900039672851562e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__1521645649856874254<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 74,
      "real_time": 9.4406045595737733e+06,
      "cpu_time": 1.0725367324321393e+07,
      "time_unit": "ns",
      "items_per_second": 3.4831819458694312e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 9.1690883636474609e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 7.4000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 74,
      "real_time": 9.4416729627630189e+06,
      "cpu_time": 1.0716637013515722e+07,
      "time_unit": "ns",
      "items_per_second": 3.4827877950961127e+11,
      "activation_mode": 3.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 9.1791038513183594e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3; cudnnMathType_t math_type = (cudnnMathType_t)0]": 3.7948729185673933e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 7.4000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 74,
      "real_time": 9.4345980953123122e+06,
      "cpu_time": 1.0716287770273348e+07,
      "time_unit": "ns",
      "items_per_second": 3.4853994868460229e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 9.2241277694702148e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 7.4000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 74,
      "real_time": 9.4367087113897540e+06,
      "cpu_time": 1.0714063527026873e+07,
      "time_unit": "ns",
      "items_per_second": 3.4846199417293695e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 9.2098560333251953e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 7.4000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 74,
      "real_time": 9.4372427997154165e+06,
      "cpu_time": 1.0719014162162920e+07,
      "time_unit": "ns",
      "items_per_second": 3.4844227342536542e+11,
      "activation_mode": 5.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 9.1238403320312500e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.2315062669022114e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 7.4000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 103,
      "real_time": 6.7738708587887799e+06,
      "cpu_time": 7.4362066116500553e+06,
      "time_unit": "ns",
      "items_per_second": 4.8544390711752948e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.0000000000000000e+02,
      "advised_time": 6.8014078140258789e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 1.0300000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 3.0000000000000000e+02,
      "workspace_megabytes": 2.8610229492187500e-04
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 103,
      "real_time": 6.7737839202785380e+06,
      "cpu_time": 7.4356703980596056e+06,
      "time_unit": "ns",
      "items_per_second": 4.8545013757462518e+11,
      "activation_mode": 3.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.0000000000000000e+02,
      "advised_time": 6.8106241226196289e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.3805875302202324e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 1.0300000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 3.0000000000000000e+02,
      "workspace_megabytes": 2.8610229492187500e-04
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 103,
      "real_time": 6.7738012898966521e+06,
      "cpu_time": 7.4362496019441839e+06,
      "time_unit": "ns",
      "items_per_second": 4.8544889276640857e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.0000000000000000e+02,
      "advised_time": 6.8054718971252441e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 1.0300000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 3.0000000000000000e+02,
      "workspace_megabytes": 2.8610229492187500e-04
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 103,
      "real_time": 6.7758313063712949e+06,
      "cpu_time": 7.4380187766981851e+06,
      "time_unit": "ns",
      "items_per_second": 4.8530345389620142e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.0000000000000000e+02,
      "advised_time": 6.7881598472595215e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 1.0300000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 3.0000000000000000e+02,
      "workspace_megabytes": 2.8610229492187500e-04
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 103,
      "real_time": 6.7756976887077382e+06,
      "cpu_time": 7.4380788446571110e+06,
      "time_unit": "ns",
      "items_per_second": 4.8531302414514178e+11,
      "activation_mode": 5.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.0000000000000000e+02,
      "advised_time": 6.8034558296203613e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 8.1554158113550264e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 1.0300000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 3.0000000000000000e+02,
      "workspace_megabytes": 2.8610229492187500e-04
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 82,
      "real_time": 8.6099289175940715e+06,
      "cpu_time": 9.6615959756094553e+06,
      "time_unit": "ns",
      "items_per_second": 3.8192351731039380e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.3121100800000000e+08,
      "advised_time": 8.4899835586547852e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 8.2000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 2.3121100800000000e+08,
      "workspace_megabytes": 2.2050000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 82,
      "real_time": 8.5926138705051523e+06,
      "cpu_time": 9.6433497439035103e+06,
      "time_unit": "ns",
      "items_per_second": 3.8269313454052386e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.3121100800000000e+08,
      "advised_time": 8.5278720855712891e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 8.2000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 2.3121100800000000e+08,
      "workspace_megabytes": 2.2050000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 81,
      "real_time": 8.5902427219682261e+06,
      "cpu_time": 9.6555629012349527e+06,
      "time_unit": "ns",
      "items_per_second": 3.8279876860645508e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.3121100800000000e+08,
      "advised_time": 8.4736003875732422e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 8.1000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 2.3121100800000000e+08,
      "workspace_megabytes": 2.2050000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 73,
      "real_time": 9.5794139404411186e+06,
      "cpu_time": 1.0896446767118230e+07,
      "time_unit": "ns",
      "items_per_second": 3.4327093039770831e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 9.1540684800000000e+08,
      "advised_time": 9.5752000808715820e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.5608866559154586e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 7.3000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 9.1540684800000000e+08,
      "workspace_megabytes": 8.7300000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 73,
      "real_time": 9.5783237954729218e+06,
      "cpu_time": 1.0894993178082231e+07,
      "time_unit": "ns",
      "items_per_second": 3.4330999935021942e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 9.1540684800000000e+08,
      "advised_time": 9.5897598266601562e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.5608866559154586e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 7.3000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 9.1540684800000000e+08,
      "workspace_megabytes": 8.7300000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 73,
      "real_time": 9.5782588580495697e+06,
      "cpu_time": 1.0895183246574318e+07,
      "time_unit": "ns",
      "items_per_second": 3.4331232687833276e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 9.1540684800000000e+08,
      "advised_time": 9.5691843032836914e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.5608866559154586e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 7.3000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 9.1540684800000000e+08,
      "workspace_megabytes": 8.7300000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 20,
      "real_time": 3.5672411322593689e+07,
      "cpu_time": 5.3499331899993762e+07,
      "time_unit": "ns",
      "items_per_second": 9.2181442579332489e+10,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.2817259520000000e+09,
      "advised_time": 3.6016864776611328e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 2.0000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 2.2817259520000000e+09,
      "workspace_megabytes": 2.1760234375000000e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 20,
      "real_time": 3.5746314749121666e+07,
      "cpu_time": 5.3611616399999209e+07,
      "time_unit": "ns",
      "items_per_second": 9.1990862808614380e+10,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.2817259520000000e+09,
      "advised_time": 3.5948478698730469e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 2.0000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 2.2817259520000000e+09,
      "workspace_megabytes": 2.1760234375000000e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 20,
      "real_time": 3.5737216100096703e+07,
      "cpu_time": 5.3599806850004941e+07,
      "time_unit": "ns",
      "items_per_second": 9.2014283563377563e+10,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.2817259520000000e+09,
      "advised_time": 3.5942401885986328e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 2.0000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 2.2817259520000000e+09,
      "workspace_megabytes": 2.1760234375000000e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 81,
      "real_time": 8.6835481134461761e+06,
      "cpu_time": 9.7464638641951513e+06,
      "time_unit": "ns",
      "items_per_second": 3.7868556643431580e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.6216448000000000e+07,
      "advised_time": 8.7173118591308594e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)6; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.1941256922983490e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 6.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 8.1000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 2.6216448000000000e+07,
      "workspace_megabytes": 2.5001953125000000e+01
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 81,
      "real_time": 8.6852243216133416e+06,
      "cpu_time": 9.7495646913604047e+06,
      "time_unit": "ns",
      "items_per_second": 3.7861248186957233e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.6216448000000000e+07,
      "advised_time": 8.6425600051879883e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)6; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.1941256922983490e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 6.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 8.1000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 2.6216448000000000e+07,
      "workspace_megabytes": 2.5001953125000000e+01
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 81,
      "real_time": 8.6469310048369709e+06,
      "cpu_time": 9.7194572222256176e+06,
      "time_unit": "ns",
      "items_per_second": 3.8028918400766144e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.6216448000000000e+07,
      "advised_time": 8.5698556900024414e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)6; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.1941256922983490e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 6.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 8.1000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 2.6216448000000000e+07,
      "workspace_megabytes": 2.5001953125000000e+01
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 255,
      "real_time": 2.7441912184597231e+06,
      "cpu_time": 2.8564932431369447e+06,
      "time_unit": "ns",
      "items_per_second": 1.1982890674235510e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.8874368000000000e+08,
      "advised_time": 2.8170239925384521e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)7; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 2.0444073425867530e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 7.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 2.5500000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.8874368000000000e+08,
      "workspace_megabytes": 1.8000000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 255,
      "real_time": 2.7494344549874463e+06,
      "cpu_time": 2.8619535960780350e+06,
      "time_unit": "ns",
      "items_per_second": 1.1960039018333369e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.8874368000000000e+08,
      "advised_time": 2.8119039535522461e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)7; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 2.0444073425867530e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 7.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 2.5500000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.8874368000000000e+08,
      "workspace_megabytes": 1.8000000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 255,
      "real_time": 2.7450662635851139e+06,
      "cpu_time": 2.8572378705910202e+06,
      "time_unit": "ns",
      "items_per_second": 1.1979070886636326e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.8874368000000000e+08,
      "advised_time": 2.8119039535522461e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)7; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 2.0444073425867530e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 7.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 2.5500000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.8874368000000000e+08,
      "workspace_megabytes": 1.8000000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_0_FLOAT32__BatchSize_256__5384265801303902885<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:512/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 88,
      "real_time": 7.9178650207309565e+06,
      "cpu_time": 8.8219717272708789e+06,
      "time_unit": "ns",
      "items_per_second": 1.6612227298092686e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 6.4317121505737305e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 8.8000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 5.6000000000000000e+01,
      "output_size": 2.0552089600000000e+08,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 88,
      "real_time": 7.9205825353379957e+06,
      "cpu_time": 8.8249395909075066e+06,
      "time_unit": "ns",
      "items_per_second": 1.6606527721055693e+12,
      "activation_mode": 3.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 6.4174079895019531e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3; cudnnMathType_t math_type = (cudnnMathType_t)0]": 3.7948729185673933e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 8.8000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 5.6000000000000000e+01,
      "output_size": 2.0552089600000000e+08,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 88,
      "real_time": 7.9162377885288810e+06,
      "cpu_time": 8.8204545340899304e+06,
      "time_unit": "ns",
      "items_per_second": 1.6615642045341287e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 6.4208321571350098e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 8.8000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 5.6000000000000000e+01,
      "output_size": 2.0552089600000000e+08,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 88,
      "real_time": 7.9156504520638427e+06,
      "cpu_time": 8.8199594318177011e+06,
      "time_unit": "ns",
      "items_per_second": 1.6616874915908572e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 6.3934721946716309e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 8.8000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 5.6000000000000000e+01,
      "output_size": 2.0552089600000000e+08,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 88,
      "real_time": 7.9154083293608641e+06,
      "cpu_time": 8.8190200227254480e+06,
      "time_unit": "ns",
      "items_per_second": 1.6617383205879507e+12,
      "activation_mode": 5.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 6.4203200340270996e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.2315062669022114e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 8.8000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 5.6000000000000000e+01,
      "output_size": 2.0552089600000000e+08,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 151,
      "real_time": 4.6333372006994607e+06,
      "cpu_time": 4.9442443576162085e+06,
      "time_unit": "ns",
      "items_per_second": 2.8388474169361855e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.8824000000000000e+04,
      "advised_time": 4.6448640823364258e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 1.5100000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 5.6000000000000000e+01,
      "output_size": 2.0552089600000000e+08,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.8824000000000000e+04,
      "workspace_megabytes": 1.7951965332031250e-02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 151,
      "real_time": 4.6337076604662351e+06,
      "cpu_time": 4.9439862847655015e+06,
      "time_unit": "ns",
      "items_per_second": 2.8386204542468989e+12,
      "activation_mode": 3.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.8824000000000000e+04,
      "advised_time": 4.6878719329833984e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.3805875302202324e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 1.5100000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 5.6000000000000000e+01,
      "output_size": 2.0552089600000000e+08,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.8824000000000000e+04,
      "workspace_megabytes": 1.7951965332031250e-02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 151,
      "real_time": 4.6305445220247405e+06,
      "cpu_time": 4.9411567218508720e+06,
      "time_unit": "ns",
      "items_per_second": 2.8405595241417969e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.8824000000000000e+04,
      "advised_time": 4.6714878082275391e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 1.5100000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 5.6000000000000000e+01,
      "output_size": 2.0552089600000000e+08,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.8824000000000000e+04,
      "workspace_megabytes": 1.7951965332031250e-02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 151,
      "real_time": 4.6349507818208234e+06,
      "cpu_time": 4.9475210596054373e+06,
      "time_unit": "ns",
      "items_per_second": 2.8378591193653970e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.8824000000000000e+04,
      "advised_time": 4.6254081726074219e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 1.5100000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 5.6000000000000000e+01,
      "output_size": 2.0552089600000000e+08,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.8824000000000000e+04,
      "workspace_megabytes": 1.7951965332031250e-02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 151,
      "real_time": 4.6296571998160016e+06,
      "cpu_time": 4.9403898278157022e+06,
      "time_unit": "ns",
      "items_per_second": 2.8411039470746904e+12,
      "activation_mode": 5.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.8824000000000000e+04,
      "advised_time": 4.7554559707641602e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 8.1554158113550264e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 1.5100000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 5.6000000000000000e+01,
      "output_size": 2.0552089600000000e+08,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.8824000000000000e+04,
      "workspace_megabytes": 1.7951965332031250e-02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 78,
      "real_time": 9.0219622883850187e+06,
      "cpu_time": 1.0182728089747483e+07,
      "time_unit": "ns",
      "items_per_second": 1.4579242213120046e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.0552089600000000e+08,
      "advised_time": 8.2933759689331055e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 7.8000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 5.6000000000000000e+01,
      "output_size": 2.0552089600000000e+08,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 2.0552089600000000e+08,
      "workspace_megabytes": 1.9600000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 78,
      "real_time": 9.0230612609630972e+06,
      "cpu_time": 1.0183658256414091e+07,
      "time_unit": "ns",
      "items_per_second": 1.4577466520044492e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.0552089600000000e+08,
      "advised_time": 8.2329597473144531e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 7.8000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 5.6000000000000000e+01,
      "output_size": 2.0552089600000000e+08,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 2.0552089600000000e+08,
      "workspace_megabytes": 1.9600000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 78,
      "real_time": 9.0216459610905405e+06,
      "cpu_time": 1.0182035551285760e+07,
      "time_unit": "ns",
      "items_per_second": 1.4579753407226389e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.0552089600000000e+08,
      "advised_time": 8.2152318954467773e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 7.8000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 5.6000000000000000e+01,
      "output_size": 2.0552089600000000e+08,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 2.0552089600000000e+08,
      "workspace_megabytes": 1.9600000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 29,
      "real_time": 2.4102742109319258e+07,
      "cpu_time": 3.2417621620687708e+07,
      "time_unit": "ns",
      "items_per_second": 5.4571954030551147e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.2146908160000000e+09,
      "advised_time": 2.3327743530273438e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.5608866559154586e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 2.9000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 5.6000000000000000e+01,
      "output_size": 2.0552089600000000e+08,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 2.2146908160000000e+09,
      "workspace_megabytes": 2.1120937500000000e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 29,
      "real_time": 2.4102102772429071e+07,
      "cpu_time": 3.2417285517257810e+07,
      "time_unit": "ns",
      "items_per_second": 5.4573401616419934e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.2146908160000000e+09,
      "advised_time": 2.3351039886474609e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.5608866559154586e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 2.9000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 5.6000000000000000e+01,
      "output_size": 2.0552089600000000e+08,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 2.2146908160000000e+09,
      "workspace_megabytes": 2.1120937500000000e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 29,
      "real_time": 2.4102904095218100e+07,
      "cpu_time": 3.2419284172404114e+07,
      "time_unit": "ns",
      "items_per_second": 5.4571587274454449e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.2146908160000000e+09,
      "advised_time": 2.3348224639892578e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.5608866559154586e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 2.9000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 5.6000000000000000e+01,
      "output_size": 2.0552089600000000e+08,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 2.2146908160000000e+09,
      "workspace_megabytes": 2.1120937500000000e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 41,
      "real_time": 1.6986579520673286e+07,
      "cpu_time": 2.1131032682924010e+07,
      "time_unit": "ns",
      "items_per_second": 7.7433701870302429e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.2500344960000000e+09,
      "advised_time": 1.6350208282470703e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 4.1000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 5.6000000000000000e+01,
      "output_size": 2.0552089600000000e+08,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.2500344960000000e+09,
      "workspace_megabytes": 1.1921257934570312e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 41,
      "real_time": 1.6987885507505115e+07,
      "cpu_time": 2.1133471219522242e+07,
      "time_unit": "ns",
      "items_per_second": 7.7427748957861523e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.2500344960000000e+09,
      "advised_time": 1.6328704833984375e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 4.1000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 5.6000000000000000e+01,
      "output_size": 2.0552089600000000e+08,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.2500344960000000e+09,
      "workspace_megabytes": 1.1921257934570312e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 41,
      "real_time": 1.6987053995452277e+07,
      "cpu_time": 2.1131330487797063e+07,
      "time_unit": "ns",
      "items_per_second": 7.7431539026845813e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.2500344960000000e+09,
      "advised_time": 1.6305152893066406e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 4.1000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 5.6000000000000000e+01,
      "output_size": 2.0552089600000000e+08,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.2500344960000000e+09,
      "workspace_megabytes": 1.1921257934570312e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__8083863649569551973<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 107,
      "real_time": 6.5430872986597996e+06,
      "cpu_time": 7.1590035327073280e+06,
      "time_unit": "ns",
      "items_per_second": 2.0102646875419434e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 6.3157758712768555e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 1.0700000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 107,
      "real_time": 6.5365129185266029e+06,
      "cpu_time": 7.1521140093460483e+06,
      "time_unit": "ns",
      "items_per_second": 2.0122865980604377e+12,
      "activation_mode": 3.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 6.3055682182312012e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3; cudnnMathType_t math_type = (cudnnMathType_t)0]": 3.7948729185673933e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 1.0700000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 107,
      "real_time": 6.5345603485252252e+06,
      "cpu_time": 7.1499597850425253e+06,
      "time_unit": "ns",
      "items_per_second": 2.0128878826512874e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 6.3016958236694336e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 1.0700000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 107,
      "real_time": 6.5260868058235291e+06,
      "cpu_time": 7.1407058878510678e+06,
      "time_unit": "ns",
      "items_per_second": 2.0155014383600703e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 6.2913279533386230e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 1.0700000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 107,
      "real_time": 6.5380297949380968e+06,
      "cpu_time": 7.1538209999988517e+06,
      "time_unit": "ns",
      "items_per_second": 2.0118197311036479e+12,
      "activation_mode": 5.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 6.3078398704528809e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.2315062669022114e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 1.0700000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 180,
      "real_time": 3.8852320169098675e+06,
      "cpu_time": 4.1058590666687503e+06,
      "time_unit": "ns",
      "items_per_second": 3.3854702336313887e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.8824000000000000e+04,
      "advised_time": 3.9001278877258301e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 1.8000000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.8824000000000000e+04,
      "workspace_megabytes": 1.7951965332031250e-02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 180,
      "real_time": 3.8856640264081457e+06,
      "cpu_time": 4.1062187166687632e+06,
      "time_unit": "ns",
      "items_per_second": 3.3850938358555830e+12,
      "activation_mode": 3.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.8824000000000000e+04,
      "advised_time": 3.9137279987335205e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.3805875302202324e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 1.8000000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.8824000000000000e+04,
      "workspace_megabytes": 1.7951965332031250e-02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 180,
      "real_time": 3.8855904131196439e+06,
      "cpu_time": 4.1062252166676037e+06,
      "time_unit": "ns",
      "items_per_second": 3.3851579671361997e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.8824000000000000e+04,
      "advised_time": 3.9239680767059326e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 1.8000000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.8824000000000000e+04,
      "workspace_megabytes": 1.7951965332031250e-02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 180,
      "real_time": 3.8847009832453397e+06,
      "cpu_time": 4.1052676833260190e+06,
      "time_unit": "ns",
      "items_per_second": 3.3859330230898486e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.8824000000000000e+04,
      "advised_time": 3.9198720455169678e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 1.8000000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.8824000000000000e+04,
      "workspace_megabytes": 1.7951965332031250e-02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 180,
      "real_time": 3.8847242611356908e+06,
      "cpu_time": 4.1053233055612915e+06,
      "time_unit": "ns",
      "items_per_second": 3.3859127340365337e+12,
      "activation_mode": 5.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.8824000000000000e+04,
      "advised_time": 3.9229440689086914e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 8.1554158113550264e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 1.8000000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.8824000000000000e+04,
      "workspace_megabytes": 1.7951965332031250e-02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 69,
      "real_time": 1.0105712312287178e+07,
      "cpu_time": 1.1575374826095512e+07,
      "time_unit": "ns",
      "items_per_second": 1.3015744895099895e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 8.2208358400000000e+08,
      "advised_time": 9.3949117660522461e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 6.9000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 8.2208358400000000e+08,
      "workspace_megabytes": 7.8400000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 69,
      "real_time": 1.0107538622358572e+07,
      "cpu_time": 1.1576814275352500e+07,
      "time_unit": "ns",
      "items_per_second": 1.3013393107303010e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 8.2208358400000000e+08,
      "advised_time": 9.3890562057495117e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 6.9000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 8.2208358400000000e+08,
      "workspace_megabytes": 7.8400000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 69,
      "real_time": 1.0107025072194528e+07,
      "cpu_time": 1.1575933043473739e+07,
      "time_unit": "ns",
      "items_per_second": 1.3014054333540928e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 8.2208358400000000e+08,
      "advised_time": 9.3829116821289062e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 6.9000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 8.2208358400000000e+08,
      "workspace_megabytes": 7.8400000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 31,
      "real_time": 2.2814331876654778e+07,
      "cpu_time": 3.0177322741928212e+07,
      "time_unit": "ns",
      "items_per_second": 5.7653835383447791e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.7683389440000000e+09,
      "advised_time": 2.2891519546508789e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.5608866559154586e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 3.1000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 2.7683389440000000e+09,
      "workspace_megabytes": 2.6400937500000000e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 31,
      "real_time": 2.2813430596743859e+07,
      "cpu_time": 3.0176595903225783e+07,
      "time_unit": "ns",
      "items_per_second": 5.7656113087513293e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.7683389440000000e+09,
      "advised_time": 2.2904832839965820e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.5608866559154586e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 3.1000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 2.7683389440000000e+09,
      "workspace_megabytes": 2.6400937500000000e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 31,
      "real_time": 2.2815411609988060e+07,
      "cpu_time": 3.0177530645174090e+07,
      "time_unit": "ns",
      "items_per_second": 5.7651106930903552e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.7683389440000000e+09,
      "advised_time": 2.2894592285156250e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.5608866559154586e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 3.1000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 2.7683389440000000e+09,
      "workspace_megabytes": 2.6400937500000000e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 45,
      "real_time": 1.5386968913177649e+07,
      "cpu_time": 1.8808946888892327e+07,
      "time_unit": "ns",
      "items_per_second": 8.5483615507504333e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.2500344960000000e+09,
      "advised_time": 1.5418368339538574e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 4.5000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.2500344960000000e+09,
      "workspace_megabytes": 1.1921257934570312e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 46,
      "real_time": 1.5386454275120860e+07,
      "cpu_time": 1.8733094847831305e+07,
      "time_unit": "ns",
      "items_per_second": 8.5486474718664062e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.2500344960000000e+09,
      "advised_time": 1.5440896034240723e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 4.6000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.2500344960000000e+09,
      "workspace_megabytes": 1.1921257934570312e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 45,
      "real_time": 1.5383477239973016e+07,
      "cpu_time": 1.8802081222227369e+07,
      "time_unit": "ns",
      "items_per_second": 8.5503018198134460e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.2500344960000000e+09,
      "advised_time": 1.5465439796447754e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 4.5000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.2500344960000000e+09,
      "workspace_megabytes": 1.1921257934570312e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__16949443275814384545<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 153,
      "real_time": 4.5742297551162494e+06,
      "cpu_time": 4.8774378300642008e+06,
      "time_unit": "ns",
      "items_per_second": 2.8755305369801484e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 4.3776001930236816e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.0480000000000000e+03,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.0480000000000000e+03,
      "input_height": 7.0000000000000000e+00,
      "input_size": 2.5690112000000000e+07,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 1.5300000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 153,
      "real_time": 4.5741320575520489e+06,
      "cpu_time": 4.8799131895403583e+06,
      "time_unit": "ns",
      "items_per_second": 2.8755919546055493e+12,
      "activation_mode": 3.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 4.3714561462402344e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3; cudnnMathType_t math_type = (cudnnMathType_t)0]": 3.7948729185673933e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.0480000000000000e+03,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.0480000000000000e+03,
      "input_height": 7.0000000000000000e+00,
      "input_size": 2.5690112000000000e+07,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 1.5300000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 153,
      "real_time": 4.5663763501139637e+06,
      "cpu_time": 4.8697976209164718e+06,
      "time_unit": "ns",
      "items_per_second": 2.8804759694569917e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 4.4491839408874512e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.0480000000000000e+03,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.0480000000000000e+03,
      "input_height": 7.0000000000000000e+00,
      "input_size": 2.5690112000000000e+07,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 1.5300000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 153,
      "real_time": 4.5650938732756507e+06,
      "cpu_time": 4.8686151830049483e+06,
      "time_unit": "ns",
      "items_per_second": 2.8812851847363911e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 4.4183359146118164e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.0480000000000000e+03,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.0480000000000000e+03,
      "input_height": 7.0000000000000000e+00,
      "input_size": 2.5690112000000000e+07,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 1.5300000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 153,
      "real_time": 4.5664939006753992e+06,
      "cpu_time": 4.8689122418285822e+06,
      "time_unit": "ns",
      "items_per_second": 2.8804018203231538e+12,
      "activation_mode": 5.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 4.4636158943176270e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.2315062669022114e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.0480000000000000e+03,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.0480000000000000e+03,
      "input_height": 7.0000000000000000e+00,
      "input_size": 2.5690112000000000e+07,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 1.5300000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 228,
      "real_time": 3.0718125002622083e+06,
      "cpu_time": 3.2113802543847216e+06,
      "time_unit": "ns",
      "items_per_second": 4.2819466822526567e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.0000000000000000e+02,
      "advised_time": 3.0853118896484375e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.0480000000000000e+03,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.0480000000000000e+03,
      "input_height": 7.0000000000000000e+00,
      "input_size": 2.5690112000000000e+07,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 2.2800000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 3.0000000000000000e+02,
      "workspace_megabytes": 2.8610229492187500e-04
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 228,
      "real_time": 3.0722443663858268e+06,
      "cpu_time": 3.2118335701754899e+06,
      "time_unit": "ns",
      "items_per_second": 4.2813447679858618e+12,
      "activation_mode": 3.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.0000000000000000e+02,
      "advised_time": 3.1119360923767090e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.3805875302202324e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.0480000000000000e+03,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.0480000000000000e+03,
      "input_height": 7.0000000000000000e+00,
      "input_size": 2.5690112000000000e+07,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 2.2800000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 3.0000000000000000e+02,
      "workspace_megabytes": 2.8610229492187500e-04
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 228,
      "real_time": 3.0714137604247713e+06,
      "cpu_time": 3.2109247105293898e+06,
      "time_unit": "ns",
      "items_per_second": 4.2825025769829575e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.0000000000000000e+02,
      "advised_time": 3.1109120845794678e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.0480000000000000e+03,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.0480000000000000e+03,
      "input_height": 7.0000000000000000e+00,
      "input_size": 2.5690112000000000e+07,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 2.2800000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 3.0000000000000000e+02,
      "workspace_megabytes": 2.8610229492187500e-04
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 228,
      "real_time": 3.0727005040828595e+06,
      "cpu_time": 3.2122664956136164e+06,
      "time_unit": "ns",
      "items_per_second": 4.2807092088937612e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.0000000000000000e+02,
      "advised_time": 3.1129601001739502e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.0480000000000000e+03,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.0480000000000000e+03,
      "input_height": 7.0000000000000000e+00,
      "input_size": 2.5690112000000000e+07,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 2.2800000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 3.0000000000000000e+02,
      "workspace_megabytes": 2.8610229492187500e-04
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 228,
      "real_time": 3.0717640827213856e+06,
      "cpu_time": 3.2113347675459734e+06,
      "time_unit": "ns",
      "items_per_second": 4.2820141748473691e+12,
      "activation_mode": 5.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.0000000000000000e+02,
      "advised_time": 3.0853118896484375e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 8.1554158113550264e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.0480000000000000e+03,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.0480000000000000e+03,
      "input_height": 7.0000000000000000e+00,
      "input_size": 2.5690112000000000e+07,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 2.2800000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 3.0000000000000000e+02,
      "workspace_megabytes": 2.8610229492187500e-04
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 152,
      "real_time": 4.6087328626431134e+06,
      "cpu_time": 4.9142327039533034e+06,
      "time_unit": "ns",
      "items_per_second": 2.8540029843379873e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.0276044800000000e+08,
      "advised_time": 4.5157442092895508e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.0480000000000000e+03,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.0480000000000000e+03,
      "input_height": 7.0000000000000000e+00,
      "input_size": 2.5690112000000000e+07,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 1.5200000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.0276044800000000e+08,
      "workspace_megabytes": 9.8000000000000000e+01
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 152,
      "real_time": 4.6045667136480147e+06,
      "cpu_time": 4.9090154407863161e+06,
      "time_unit": "ns",
      "items_per_second": 2.8565852472097500e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.0276044800000000e+08,
      "advised_time": 4.6018562316894531e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.0480000000000000e+03,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.0480000000000000e+03,
      "input_height": 7.0000000000000000e+00,
      "input_size": 2.5690112000000000e+07,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 1.5200000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.0276044800000000e+08,
      "workspace_megabytes": 9.8000000000000000e+01
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 151,
      "real_time": 4.6148590237790383e+06,
      "cpu_time": 4.9208302715201257e+06,
      "time_unit": "ns",
      "items_per_second": 2.8502143350912007e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.0276044800000000e+08,
      "advised_time": 4.5539197921752930e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.0480000000000000e+03,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.0480000000000000e+03,
      "input_height": 7.0000000000000000e+00,
      "input_size": 2.5690112000000000e+07,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 1.5100000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.0276044800000000e+08,
      "workspace_megabytes": 9.8000000000000000e+01
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 20,
      "real_time": 3.5409203730523586e+07,
      "cpu_time": 5.3099029499986723e+07,
      "time_unit": "ns",
      "items_per_second": 3.7146662331357385e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.6280729600000000e+09,
      "advised_time": 3.5339263916015625e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.5608866559154586e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.0480000000000000e+03,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.0480000000000000e+03,
      "input_height": 7.0000000000000000e+00,
      "input_size": 2.5690112000000000e+07,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 2.0000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 3.6280729600000000e+09,
      "workspace_megabytes": 3.4600000000000000e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 20,
      "real_time": 3.5370033793151379e+07,
      "cpu_time": 5.3061614099999592e+07,
      "time_unit": "ns",
      "items_per_second": 3.7187799765537268e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.6280729600000000e+09,
      "advised_time": 3.5249153137207031e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.5608866559154586e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.0480000000000000e+03,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.0480000000000000e+03,
      "input_height": 7.0000000000000000e+00,
      "input_size": 2.5690112000000000e+07,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 2.0000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 3.6280729600000000e+09,
      "workspace_megabytes": 3.4600000000000000e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 20,
      "real_time": 3.5341388359665871e+07,
      "cpu_time": 5.2999478700007781e+07,
      "time_unit": "ns",
      "items_per_second": 3.7217941780158057e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.6280729600000000e+09,
      "advised_time": 3.5267585754394531e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.5608866559154586e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.0480000000000000e+03,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.0480000000000000e+03,
      "input_height": 7.0000000000000000e+00,
      "input_size": 2.5690112000000000e+07,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 2.0000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 3.6280729600000000e+09,
      "workspace_megabytes": 3.4600000000000000e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 24,
      "real_time": 2.9410473381479580e+07,
      "cpu_time": 4.1640444958332524e+07,
      "time_unit": "ns",
      "items_per_second": 4.4723310547877631e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 7.6650947200000000e+08,
      "advised_time": 2.9106176376342773e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.0480000000000000e+03,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.0480000000000000e+03,
      "input_height": 7.0000000000000000e+00,
      "input_size": 2.5690112000000000e+07,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 2.4000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 7.6650947200000000e+08,
      "workspace_megabytes": 7.3100039672851562e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 24,
      "real_time": 2.9423562421773870e+07,
      "cpu_time": 4.1661911874996349e+07,
      "time_unit": "ns",
      "items_per_second": 4.4703415430982404e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 7.6650947200000000e+08,
      "advised_time": 2.9079551696777344e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.0480000000000000e+03,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.0480000000000000e+03,
      "input_height": 7.0000000000000000e+00,
      "input_size": 2.5690112000000000e+07,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 2.4000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 7.6650947200000000e+08,
      "workspace_megabytes": 7.3100039672851562e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 24,
      "real_time": 2.9397797460357349e+07,
      "cpu_time": 4.1630311124999747e+07,
      "time_unit": "ns",
      "items_per_second": 4.4742594616950983e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 7.6650947200000000e+08,
      "advised_time": 2.9097984313964844e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.0480000000000000e+03,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.0480000000000000e+03,
      "input_height": 7.0000000000000000e+00,
      "input_size": 2.5690112000000000e+07,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 2.4000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 7.6650947200000000e+08,
      "workspace_megabytes": 7.3100039672851562e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17231424721094278410<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:2048/input[2]:7/input[3]:7/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 73,
      "real_time": 9.4772168524461258e+06,
      "cpu_time": 1.0769944397265609e+07,
      "time_unit": "ns",
      "items_per_second": 3.4697257509215503e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 9.1105604171752930e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 7.3000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 74,
      "real_time": 9.4694677312430497e+06,
      "cpu_time": 1.0750820513511077e+07,
      "time_unit": "ns",
      "items_per_second": 3.4725651212165259e+11,
      "activation_mode": 3.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 9.0818557739257812e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3; cudnnMathType_t math_type = (cudnnMathType_t)0]": 3.7948729185673933e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 7.4000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 74,
      "real_time": 9.4541975523571707e+06,
      "cpu_time": 1.0731089418917740e+07,
      "time_unit": "ns",
      "items_per_second": 3.4781739198797839e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 9.1506242752075195e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 7.4000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 74,
      "real_time": 9.4347408016187120e+06,
      "cpu_time": 1.0718909391896801e+07,
      "time_unit": "ns",
      "items_per_second": 3.4853467680170111e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 9.0798082351684570e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 7.4000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 74,
      "real_time": 9.4645400657444391e+06,
      "cpu_time": 1.0749536918910041e+07,
      "time_unit": "ns",
      "items_per_second": 3.4743730948972998e+11,
      "activation_mode": 5.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 9.0716161727905273e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.2315062669022114e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 7.4000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 106,
      "real_time": 6.6176721044236198e+06,
      "cpu_time": 7.2437872735793646e+06,
      "time_unit": "ns",
      "items_per_second": 4.9690197460854773e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.1840000000000000e+03,
      "advised_time": 6.6242561340332031e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 1.0600000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.1840000000000000e+03,
      "workspace_megabytes": 1.1291503906250000e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 106,
      "real_time": 6.6201626630675681e+06,
      "cpu_time": 7.2481795566020235e+06,
      "time_unit": "ns",
      "items_per_second": 4.9671503607379535e+11,
      "activation_mode": 3.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.1840000000000000e+03,
      "advised_time": 6.6478400230407715e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.3805875302202324e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 1.0600000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.1840000000000000e+03,
      "workspace_megabytes": 1.1291503906250000e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 105,
      "real_time": 6.6281989394199280e+06,
      "cpu_time": 7.2645955904720593e+06,
      "time_unit": "ns",
      "items_per_second": 4.9611280018215344e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.1840000000000000e+03,
      "advised_time": 6.6443839073181152e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 1.0500000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.1840000000000000e+03,
      "workspace_megabytes": 1.1291503906250000e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 106,
      "real_time": 6.6223477303349180e+06,
      "cpu_time": 7.2524272075482709e+06,
      "time_unit": "ns",
      "items_per_second": 4.9655114317497424e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.1840000000000000e+03,
      "advised_time": 6.6427202224731445e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 1.0600000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.1840000000000000e+03,
      "workspace_megabytes": 1.1291503906250000e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 106,
      "real_time": 6.6326396706742495e+06,
      "cpu_time": 7.2594462830172125e+06,
      "time_unit": "ns",
      "items_per_second": 4.9578063927385339e+11,
      "activation_mode": 5.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.1840000000000000e+03,
      "advised_time": 6.6355199813842773e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 8.1554158113550264e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 1.0600000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.1840000000000000e+03,
      "workspace_megabytes": 1.1291503906250000e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 75,
      "real_time": 9.3218133350213375e+06,
      "cpu_time": 1.0560651386666298e+07,
      "time_unit": "ns",
      "items_per_second": 3.5275693878636041e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.6242201600000000e+08,
      "advised_time": 9.3747196197509766e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 7.5000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.6242201600000000e+08,
      "workspace_megabytes": 4.4100000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 75,
      "real_time": 9.2951453477144241e+06,
      "cpu_time": 1.0542189293340320e+07,
      "time_unit": "ns",
      "items_per_second": 3.5376900661468042e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.6242201600000000e+08,
      "advised_time": 9.3409280776977539e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 7.5000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.6242201600000000e+08,
      "workspace_megabytes": 4.4100000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 75,
      "real_time": 9.3090953677892685e+06,
      "cpu_time": 1.0549048346668618e+07,
      "time_unit": "ns",
      "items_per_second": 3.5323887081209656e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.6242201600000000e+08,
      "advised_time": 9.3591356277465820e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 7.5000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.6242201600000000e+08,
      "workspace_megabytes": 4.4100000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 233,
      "real_time": 3.0059166984081012e+06,
      "cpu_time": 3.1379749999977490e+06,
      "time_unit": "ns",
      "items_per_second": 1.0939539135404065e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.0434918400000000e+08,
      "advised_time": 3.0156800746917725e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.5608866559154586e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 2.3300000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 3.0434918400000000e+08,
      "workspace_megabytes": 2.9025000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 233,
      "real_time": 3.0061060816545216e+06,
      "cpu_time": 3.1382890901291175e+06,
      "time_unit": "ns",
      "items_per_second": 1.0938849949666925e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.0434918400000000e+08,
      "advised_time": 3.0227520465850830e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.5608866559154586e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 2.3300000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 3.0434918400000000e+08,
      "workspace_megabytes": 2.9025000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 233,
      "real_time": 3.0056994304444655e+06,
      "cpu_time": 3.1379024635122651e+06,
      "time_unit": "ns",
      "items_per_second": 1.0940329903558387e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.0434918400000000e+08,
      "advised_time": 3.0216319561004639e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.5608866559154586e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 2.3300000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 3.0434918400000000e+08,
      "workspace_megabytes": 2.9025000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 68,
      "real_time": 1.0345283731379930e+07,
      "cpu_time": 1.1862979279393759e+07,
      "time_unit": "ns",
      "items_per_second": 3.1785830349201819e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 8.5566259200000000e+08,
      "advised_time": 1.0440704345703125e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 6.8000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 8.5566259200000000e+08,
      "workspace_megabytes": 8.1602343750000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 68,
      "real_time": 1.0351764325819472e+07,
      "cpu_time": 1.1870460397059398e+07,
      "time_unit": "ns",
      "items_per_second": 3.1765931221967682e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 8.5566259200000000e+08,
      "advised_time": 1.0402815818786621e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 6.8000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 8.5566259200000000e+08,
      "workspace_megabytes": 8.1602343750000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 68,
      "real_time": 1.0349155253018526e+07,
      "cpu_time": 1.1873567911752550e+07,
      "time_unit": "ns",
      "items_per_second": 3.1773939569037726e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 8.5566259200000000e+08,
      "advised_time": 1.0408800125122070e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 6.8000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 8.5566259200000000e+08,
      "workspace_megabytes": 8.1602343750000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 182,
      "real_time": 3.8570394678102746e+06,
      "cpu_time": 4.0707395329653602e+06,
      "time_unit": "ns",
      "items_per_second": 8.5255397655208838e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 6.5546240000000000e+06,
      "advised_time": 3.9075839519500732e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)6; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.1941256922983490e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 6.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 1.8200000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 6.5546240000000000e+06,
      "workspace_megabytes": 6.2509765625000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 182,
      "real_time": 3.8452646950935270e+06,
      "cpu_time": 4.0587925384624372e+06,
      "time_unit": "ns",
      "items_per_second": 8.5516462369829626e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 6.5546240000000000e+06,
      "advised_time": 3.8778240680694580e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)6; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.1941256922983490e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 6.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 1.8200000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 6.5546240000000000e+06,
      "workspace_megabytes": 6.2509765625000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 182,
      "real_time": 3.8510396459989334e+06,
      "cpu_time": 4.0669882912106663e+06,
      "time_unit": "ns",
      "items_per_second": 8.5388223396153296e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 6.5546240000000000e+06,
      "advised_time": 3.9249920845031738e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)6; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.1941256922983490e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 6.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 1.8200000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 6.5546240000000000e+06,
      "workspace_megabytes": 6.2509765625000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 219,
      "real_time": 3.1956358904180597e+06,
      "cpu_time": 3.3460164018267016e+06,
      "time_unit": "ns",
      "items_per_second": 1.0290078246585887e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.1142707200000000e+08,
      "advised_time": 3.3157119750976562e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)7; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 2.0444073425867530e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 7.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 2.1900000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 3.1142707200000000e+08,
      "workspace_megabytes": 2.9700000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 219,
      "real_time": 3.1947264454187187e+06,
      "cpu_time": 3.3452710639250414e+06,
      "time_unit": "ns",
      "items_per_second": 1.0293007530317709e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.1142707200000000e+08,
      "advised_time": 3.3187839984893799e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)7; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 2.0444073425867530e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 7.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 2.1900000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 3.1142707200000000e+08,
      "workspace_megabytes": 2.9700000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 219,
      "real_time": 3.1949561344434139e+06,
      "cpu_time": 3.3455649086706992e+06,
      "time_unit": "ns",
      "items_per_second": 1.0292267554317623e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.1142707200000000e+08,
      "advised_time": 3.3218560218811035e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)7; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 2.0444073425867530e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 7.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 1.4000000000000000e+01,
      "input_size": 1.2845056000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 2.1900000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 3.1142707200000000e+08,
      "workspace_megabytes": 2.9700000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__14125403594846163348<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:256/input[2]:14/input[3]:14/filter_count:256/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 49,
      "real_time": 1.4331817094768796e+07,
      "cpu_time": 1.7261327551022056e+07,
      "time_unit": "ns",
      "items_per_second": 2.2944294601696130e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 1.4252991676330566e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 4.9000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 49,
      "real_time": 1.4380950081561293e+07,
      "cpu_time": 1.7320434387765627e+07,
      "time_unit": "ns",
      "items_per_second": 2.2865904667982800e+11,
      "activation_mode": 3.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 1.4274559974670410e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3; cudnnMathType_t math_type = (cudnnMathType_t)0]": 3.7948729185673933e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 4.9000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 49,
      "real_time": 1.4351095529083086e+07,
      "cpu_time": 1.7283949795921903e+07,
      "time_unit": "ns",
      "items_per_second": 2.2913472559192816e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 1.4295040130615234e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 4.9000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 49,
      "real_time": 1.4369400845346402e+07,
      "cpu_time": 1.7306583530609842e+07,
      "time_unit": "ns",
      "items_per_second": 2.2884282868794370e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 1.4287775993347168e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 4.9000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 49,
      "real_time": 1.4363777918779120e+07,
      "cpu_time": 1.7299110163263526e+07,
      "time_unit": "ns",
      "items_per_second": 2.2893241280908768e+11,
      "activation_mode": 5.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 1.4279680252075195e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.2315062669022114e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 4.9000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 88,
      "real_time": 7.9229767540131100e+06,
      "cpu_time": 8.8272879659066144e+06,
      "time_unit": "ns",
      "items_per_second": 4.1503773620620654e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.8824000000000000e+04,
      "advised_time": 7.9595518112182617e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 8.8000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.8824000000000000e+04,
      "workspace_megabytes": 1.7951965332031250e-02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 88,
      "real_time": 7.9223083819008684e+06,
      "cpu_time": 8.8270345681758560e+06,
      "time_unit": "ns",
      "items_per_second": 4.1507275120878357e+11,
      "activation_mode": 3.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.8824000000000000e+04,
      "advised_time": 7.9615998268127441e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.3805875302202324e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 8.8000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.8824000000000000e+04,
      "workspace_megabytes": 1.7951965332031250e-02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 88,
      "real_time": 7.9208031741224909e+06,
      "cpu_time": 8.8253312840978727e+06,
      "time_unit": "ns",
      "items_per_second": 4.1515162840342883e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.8824000000000000e+04,
      "advised_time": 7.9339518547058105e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 8.8000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.8824000000000000e+04,
      "workspace_megabytes": 1.7951965332031250e-02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 88,
      "real_time": 7.9198363025418734e+06,
      "cpu_time": 8.8242575454493333e+06,
      "time_unit": "ns",
      "items_per_second": 4.1520231105592523e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.8824000000000000e+04,
      "advised_time": 7.9575037956237793e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 8.8000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.8824000000000000e+04,
      "workspace_megabytes": 1.7951965332031250e-02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 88,
      "real_time": 7.9221789386462085e+06,
      "cpu_time": 8.8269286363643315e+06,
      "time_unit": "ns",
      "items_per_second": 4.1507953322775250e+11,
      "activation_mode": 5.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.8824000000000000e+04,
      "advised_time": 7.9470081329345703e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 8.1554158113550264e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 8.8000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.8824000000000000e+04,
      "workspace_megabytes": 1.7951965332031250e-02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 40,
      "real_time": 1.7332873493432999e+07,
      "cpu_time": 2.1678208274994403e+07,
      "time_unit": "ns",
      "items_per_second": 1.8971662934284204e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.8496880640000000e+09,
      "advised_time": 1.7496063232421875e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 4.0000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.8496880640000000e+09,
      "workspace_megabytes": 1.7640000000000000e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 40,
      "real_time": 1.7338902642950416e+07,
      "cpu_time": 2.1679079175004289e+07,
      "time_unit": "ns",
      "items_per_second": 1.8965066035115887e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.8496880640000000e+09,
      "advised_time": 1.7578975677490234e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 4.0000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.8496880640000000e+09,
      "workspace_megabytes": 1.7640000000000000e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 40,
      "real_time": 1.7334848130121827e+07,
      "cpu_time": 2.1672943499990538e+07,
      "time_unit": "ns",
      "items_per_second": 1.8969501845741812e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.8496880640000000e+09,
      "advised_time": 1.7582912445068359e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 4.0000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.8496880640000000e+09,
      "workspace_megabytes": 1.7640000000000000e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 81,
      "real_time": 8.6316077475562505e+06,
      "cpu_time": 9.7010528765389454e+06,
      "time_unit": "ns",
      "items_per_second": 3.8096429218890088e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 6.9220761600000000e+08,
      "advised_time": 8.2215356826782227e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.5608866559154586e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 8.1000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 6.9220761600000000e+08,
      "workspace_megabytes": 6.6014062500000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 81,
      "real_time": 8.6327094791664016e+06,
      "cpu_time": 9.7025316913627815e+06,
      "time_unit": "ns",
      "items_per_second": 3.8091567241268158e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 6.9220761600000000e+08,
      "advised_time": 8.2257919311523438e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.5608866559154586e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 8.1000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 6.9220761600000000e+08,
      "workspace_megabytes": 6.6014062500000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 81,
      "real_time": 8.6321331284664292e+06,
      "cpu_time": 9.7015919382663276e+06,
      "time_unit": "ns",
      "items_per_second": 3.8094110540950378e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 6.9220761600000000e+08,
      "advised_time": 8.2277755737304688e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.5608866559154586e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 8.1000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 6.9220761600000000e+08,
      "workspace_megabytes": 6.6014062500000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 127,
      "real_time": 5.5164364019892821e+06,
      "cpu_time": 5.9528576063003559e+06,
      "time_unit": "ns",
      "items_per_second": 5.9609757031082495e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.0308761600000000e+08,
      "advised_time": 5.7671680450439453e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 1.2700000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 3.0308761600000000e+08,
      "workspace_megabytes": 2.8904687500000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 127,
      "real_time": 5.5159634514528466e+06,
      "cpu_time": 5.9522938582679806e+06,
      "time_unit": "ns",
      "items_per_second": 5.9614868099495605e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.0308761600000000e+08,
      "advised_time": 5.7812800407409668e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 1.2700000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 3.0308761600000000e+08,
      "workspace_megabytes": 2.8904687500000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 127,
      "real_time": 5.5170965336967176e+06,
      "cpu_time": 5.9534087322882749e+06,
      "time_unit": "ns",
      "items_per_second": 5.9602624603645630e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.0308761600000000e+08,
      "advised_time": 5.7588481903076172e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 1.2700000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 3.0308761600000000e+08,
      "workspace_megabytes": 2.8904687500000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 144,
      "real_time": 4.8729689054501550e+06,
      "cpu_time": 5.2156806736118393e+06,
      "time_unit": "ns",
      "items_per_second": 6.7481127004979932e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.0985600000000000e+05,
      "advised_time": 4.8752641677856445e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)6; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.1941256922983490e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 6.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 1.4400000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.0985600000000000e+05,
      "workspace_megabytes": 3.9086914062500000e-01
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 144,
      "real_time": 4.8734635929577053e+06,
      "cpu_time": 5.2161138055563727e+06,
      "time_unit": "ns",
      "items_per_second": 6.7474277241995557e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.0985600000000000e+05,
      "advised_time": 4.8885760307312012e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)6; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.1941256922983490e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 6.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 1.4400000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.0985600000000000e+05,
      "workspace_megabytes": 3.9086914062500000e-01
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 144,
      "real_time": 4.8732124721734887e+06,
      "cpu_time": 5.2158464027785258e+06,
      "time_unit": "ns",
      "items_per_second": 6.7477754248900598e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.0985600000000000e+05,
      "advised_time": 4.8824319839477539e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)6; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.1941256922983490e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 6.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 1.4400000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.0985600000000000e+05,
      "workspace_megabytes": 3.9086914062500000e-01
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 123,
      "real_time": 5.6991019377076048e+06,
      "cpu_time": 6.1668280325230313e+06,
      "time_unit": "ns",
      "items_per_second": 5.7699166850184351e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 9.2543385600000000e+08,
      "advised_time": 6.1696319580078125e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)7; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 2.0444073425867530e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 7.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 1.2300000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 9.2543385600000000e+08,
      "workspace_megabytes": 8.8256250000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 123,
      "real_time": 5.6987736048555467e+06,
      "cpu_time": 6.1665012520286962e+06,
      "time_unit": "ns",
      "items_per_second": 5.7702491167542224e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 9.2543385600000000e+08,
      "advised_time": 6.1583361625671387e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)7; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 2.0444073425867530e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 7.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 1.2300000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 9.2543385600000000e+08,
      "workspace_megabytes": 8.8256250000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 123,
      "real_time": 5.6996787065352369e+06,
      "cpu_time": 6.1672764959402550e+06,
      "time_unit": "ns",
      "items_per_second": 5.7693328085837622e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 9.2543385600000000e+08,
      "advised_time": 6.1644802093505859e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)7; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 2.0444073425867530e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 7.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 1.2300000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 9.2543385600000000e+08,
      "workspace_megabytes": 8.8256250000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__11391849147271539040<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 145,
      "real_time": 4.8170246408674223e+06,
      "cpu_time": 5.1534540896541178e+06,
      "time_unit": "ns",
      "items_per_second": 2.7305937429523760e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 4.4656639099121094e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.0480000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.0480000000000000e+03,
      "num_iterations": 1.4500000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 145,
      "real_time": 4.8150269796365295e+06,
      "cpu_time": 5.1533195310333930e+06,
      "time_unit": "ns",
      "items_per_second": 2.7317266132936396e+12,
      "activation_mode": 3.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 4.5476160049438477e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3; cudnnMathType_t math_type = (cudnnMathType_t)0]": 3.7948729185673933e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.0480000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.0480000000000000e+03,
      "num_iterations": 1.4500000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 145,
      "real_time": 4.8176280447635157e+06,
      "cpu_time": 5.1538112965568751e+06,
      "time_unit": "ns",
      "items_per_second": 2.7302517383625996e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 4.5072641372680664e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.0480000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.0480000000000000e+03,
      "num_iterations": 1.4500000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 145,
      "real_time": 4.8292570458403947e+06,
      "cpu_time": 5.1670793103461433e+06,
      "time_unit": "ns",
      "items_per_second": 2.7236772073106816e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 4.4939842224121094e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.0480000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.0480000000000000e+03,
      "num_iterations": 1.4500000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 145,
      "real_time": 4.8193763427693266e+06,
      "cpu_time": 5.1561239655155018e+06,
      "time_unit": "ns",
      "items_per_second": 2.7292613003204028e+12,
      "activation_mode": 5.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 4.5465598106384277e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.2315062669022114e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.0480000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.0480000000000000e+03,
      "num_iterations": 1.4500000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 224,
      "real_time": 3.1298145529165464e+06,
      "cpu_time": 3.2743409598166649e+06,
      "time_unit": "ns",
      "items_per_second": 4.2025931957351729e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.0000000000000000e+02,
      "advised_time": 3.1703040599822998e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.0480000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.0480000000000000e+03,
      "num_iterations": 2.2400000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 3.0000000000000000e+02,
      "workspace_megabytes": 2.8610229492187500e-04
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 224,
      "real_time": 3.1308942780015059e+06,
      "cpu_time": 3.2757994151784787e+06,
      "time_unit": "ns",
      "items_per_second": 4.2011438828895752e+12,
      "activation_mode": 3.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.0000000000000000e+02,
      "advised_time": 3.1692800521850586e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.3805875302202324e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.0480000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.0480000000000000e+03,
      "num_iterations": 2.2400000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 3.0000000000000000e+02,
      "workspace_megabytes": 2.8610229492187500e-04
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 224,
      "real_time": 3.1319541251702635e+06,
      "cpu_time": 3.2768191964258621e+06,
      "time_unit": "ns",
      "items_per_second": 4.1997222239916880e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.0000000000000000e+02,
      "advised_time": 3.1467521190643311e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.0480000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.0480000000000000e+03,
      "num_iterations": 2.2400000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 3.0000000000000000e+02,
      "workspace_megabytes": 2.8610229492187500e-04
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 224,
      "real_time": 3.1303125516777593e+06,
      "cpu_time": 3.2748085625051712e+06,
      "time_unit": "ns",
      "items_per_second": 4.2019246087583755e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.0000000000000000e+02,
      "advised_time": 3.1498239040374756e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.0480000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.0480000000000000e+03,
      "num_iterations": 2.2400000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 3.0000000000000000e+02,
      "workspace_megabytes": 2.8610229492187500e-04
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 224,
      "real_time": 3.1315227014212203e+06,
      "cpu_time": 3.2761764419656661e+06,
      "time_unit": "ns",
      "items_per_second": 4.2003008114967349e+12,
      "activation_mode": 5.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.0000000000000000e+02,
      "advised_time": 3.1445438861846924e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 8.1554158113550264e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.0480000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.0480000000000000e+03,
      "num_iterations": 2.2400000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 3.0000000000000000e+02,
      "workspace_megabytes": 2.8610229492187500e-04
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 164,
      "real_time": 4.2691414624952329e+06,
      "cpu_time": 4.5341022439044267e+06,
      "time_unit": "ns",
      "items_per_second": 3.0810263514463452e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.5690112000000000e+07,
      "advised_time": 4.0262718200683594e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.0480000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.0480000000000000e+03,
      "num_iterations": 1.6400000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 2.5690112000000000e+07,
      "workspace_megabytes": 2.4500000000000000e+01
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 164,
      "real_time": 4.2641069115389409e+06,
      "cpu_time": 4.5294852134116068e+06,
      "time_unit": "ns",
      "items_per_second": 3.0846640614019888e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.5690112000000000e+07,
      "advised_time": 4.0601601600646973e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.0480000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.0480000000000000e+03,
      "num_iterations": 1.6400000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 2.5690112000000000e+07,
      "workspace_megabytes": 2.4500000000000000e+01
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 164,
      "real_time": 4.2664669283733862e+06,
      "cpu_time": 4.5311335792695079e+06,
      "time_unit": "ns",
      "items_per_second": 3.0829577645442529e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.5690112000000000e+07,
      "advised_time": 4.1000962257385254e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.0480000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.0480000000000000e+03,
      "num_iterations": 1.6400000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 2.5690112000000000e+07,
      "workspace_megabytes": 2.4500000000000000e+01
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 19,
      "real_time": 3.7062912395125940e+07,
      "cpu_time": 5.6575390000010587e+07,
      "time_unit": "ns",
      "items_per_second": 3.5489216831567090e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.7221032960000000e+09,
      "advised_time": 3.6919296264648438e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.5608866559154586e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.0480000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.0480000000000000e+03,
      "num_iterations": 1.9000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 2.7221032960000000e+09,
      "workspace_megabytes": 2.5960000000000000e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 19,
      "real_time": 3.7094852642009132e+07,
      "cpu_time": 5.6626158736848041e+07,
      "time_unit": "ns",
      "items_per_second": 3.5458659105452612e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.7221032960000000e+09,
      "advised_time": 3.6898815155029297e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.5608866559154586e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.0480000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.0480000000000000e+03,
      "num_iterations": 1.9000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 2.7221032960000000e+09,
      "workspace_megabytes": 2.5960000000000000e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 19,
      "real_time": 3.7057766788884215e+07,
      "cpu_time": 5.6572891157884449e+07,
      "time_unit": "ns",
      "items_per_second": 3.5494144638919403e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.7221032960000000e+09,
      "advised_time": 3.6876319885253906e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.5608866559154586e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.0480000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.0480000000000000e+03,
      "num_iterations": 1.9000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 2.7221032960000000e+09,
      "workspace_megabytes": 2.5960000000000000e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 23,
      "real_time": 3.0371979121928629e+07,
      "cpu_time": 4.3575383782611072e+07,
      "time_unit": "ns",
      "items_per_second": 4.3307475259336206e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 7.6650947200000000e+08,
      "advised_time": 2.8696575164794922e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.0480000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.0480000000000000e+03,
      "num_iterations": 2.3000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 7.6650947200000000e+08,
      "workspace_megabytes": 7.3100039672851562e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 23,
      "real_time": 3.0382075873406038e+07,
      "cpu_time": 4.3586923956527442e+07,
      "time_unit": "ns",
      "items_per_second": 4.3293083062547894e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 7.6650947200000000e+08,
      "advised_time": 2.8713983535766602e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.0480000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.0480000000000000e+03,
      "num_iterations": 2.3000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 7.6650947200000000e+08,
      "workspace_megabytes": 7.3100039672851562e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 23,
      "real_time": 3.0405360557462860e+07,
      "cpu_time": 4.3610627391311266e+07,
      "time_unit": "ns",
      "items_per_second": 4.3259928850840656e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 7.6650947200000000e+08,
      "advised_time": 2.8711936950683594e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.0480000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 7.0000000000000000e+00,
      "input[3]": 7.0000000000000000e+00,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 7.0000000000000000e+00,
      "input_size": 6.4225280000000000e+06,
      "input_width": 7.0000000000000000e+00,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.0480000000000000e+03,
      "num_iterations": 2.3000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 7.6650947200000000e+08,
      "workspace_megabytes": 7.3100039672851562e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_1_FLOAT32__BatchSize_256__17226741106014169066<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:512/input[2]:7/input[3]:7/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 195,
      "real_time": 3.5915053736131927e+06,
      "cpu_time": 3.7805246410278832e+06,
      "time_unit": "ns",
      "items_per_second": 9.1558663956328967e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 2.0346879959106445e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 1.9500000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 195,
      "real_time": 3.5913468541529700e+06,
      "cpu_time": 3.7803701692324374e+06,
      "time_unit": "ns",
      "items_per_second": 9.1562705289727966e+11,
      "activation_mode": 3.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 2.0336639881134033e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3; cudnnMathType_t math_type = (cudnnMathType_t)0]": 3.7948729185673933e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 1.9500000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 195,
      "real_time": 3.5905934619502379e+06,
      "cpu_time": 3.7795969794854335e+06,
      "time_unit": "ns",
      "items_per_second": 9.1581917330566711e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 2.0111360549926758e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 1.9500000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 195,
      "real_time": 3.5901851737155365e+06,
      "cpu_time": 3.7791869487200994e+06,
      "time_unit": "ns",
      "items_per_second": 9.1592332341923560e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 2.0364160537719727e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 1.9500000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 195,
      "real_time": 3.5907368856267286e+06,
      "cpu_time": 3.7798280974386265e+06,
      "time_unit": "ns",
      "items_per_second": 9.1578259302785217e+11,
      "activation_mode": 5.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 2.0345599651336670e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.2315062669022114e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 1.9500000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 499,
      "real_time": 1.4016084706145739e+06,
      "cpu_time": 1.4346362785607323e+06,
      "time_unit": "ns",
      "items_per_second": 2.3461147709517900e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.8824000000000000e+04,
      "advised_time": 1.4384640455245972e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 4.9900000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.8824000000000000e+04,
      "workspace_megabytes": 1.7951965332031250e-02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 499,
      "real_time": 1.4017036400235188e+06,
      "cpu_time": 1.4347596553099304e+06,
      "time_unit": "ns",
      "items_per_second": 2.3459554802503228e+12,
      "activation_mode": 3.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.8824000000000000e+04,
      "advised_time": 1.4356479644775391e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.3805875302202324e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 4.9900000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.8824000000000000e+04,
      "workspace_megabytes": 1.7951965332031250e-02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 500,
      "real_time": 1.4017420227173714e+06,
      "cpu_time": 1.4346966339985556e+06,
      "time_unit": "ns",
      "items_per_second": 2.3458912429730415e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.8824000000000000e+04,
      "advised_time": 1.4356479644775391e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 5.0000000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.8824000000000000e+04,
      "workspace_megabytes": 1.7951965332031250e-02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 500,
      "real_time": 1.4018467911519110e+06,
      "cpu_time": 1.4348613260010553e+06,
      "time_unit": "ns",
      "items_per_second": 2.3457159204237603e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.8824000000000000e+04,
      "advised_time": 1.4366719722747803e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 5.0000000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.8824000000000000e+04,
      "workspace_megabytes": 1.7951965332031250e-02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 499,
      "real_time": 1.4016612503819452e+06,
      "cpu_time": 1.4347015030067607e+06,
      "time_unit": "ns",
      "items_per_second": 2.3460264276435884e+12,
      "activation_mode": 5.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.8824000000000000e+04,
      "advised_time": 1.4363199472427368e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 8.1554158113550264e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 4.9900000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.8824000000000000e+04,
      "workspace_megabytes": 1.7951965332031250e-02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 150,
      "real_time": 4.6736503082017107e+06,
      "cpu_time": 4.9897853333307765e+06,
      "time_unit": "ns",
      "items_per_second": 7.0359015312492627e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.0552089600000000e+08,
      "advised_time": 2.9767680168151855e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 1.5000000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 2.0552089600000000e+08,
      "workspace_megabytes": 1.9600000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 150,
      "real_time": 4.6715987070153160e+06,
      "cpu_time": 4.9875326799989734e+06,
      "time_unit": "ns",
      "items_per_second": 7.0389914507466687e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.0552089600000000e+08,
      "advised_time": 2.9962239265441895e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 1.5000000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 2.0552089600000000e+08,
      "workspace_megabytes": 1.9600000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 150,
      "real_time": 4.6727285068482161e+06,
      "cpu_time": 4.9888593333359193e+06,
      "time_unit": "ns",
      "items_per_second": 7.0372895219157544e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.0552089600000000e+08,
      "advised_time": 2.9911038875579834e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 1.5000000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 2.0552089600000000e+08,
      "workspace_megabytes": 1.9600000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 81,
      "real_time": 8.6230470534460042e+06,
      "cpu_time": 9.6917881111028362e+06,
      "time_unit": "ns",
      "items_per_second": 3.8134250174199060e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 6.9215846400000000e+08,
      "advised_time": 7.8825922012329102e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.5608866559154586e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 8.1000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 6.9215846400000000e+08,
      "workspace_megabytes": 6.6009375000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 81,
      "real_time": 8.6224584345832281e+06,
      "cpu_time": 9.6910456790154092e+06,
      "time_unit": "ns",
      "items_per_second": 3.8136853438585974e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 6.9215846400000000e+08,
      "advised_time": 7.8786559104919434e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.5608866559154586e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 8.1000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 6.9215846400000000e+08,
      "workspace_megabytes": 6.6009375000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 81,
      "real_time": 8.6231039676033426e+06,
      "cpu_time": 9.6916832592552491e+06,
      "time_unit": "ns",
      "items_per_second": 3.8133998480757520e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 6.9215846400000000e+08,
      "advised_time": 7.8766078948974609e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.5608866559154586e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 8.1000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 6.9215846400000000e+08,
      "workspace_megabytes": 6.6009375000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 125,
      "real_time": 5.6066890545189381e+06,
      "cpu_time": 6.0586744799957164e+06,
      "time_unit": "ns",
      "items_per_second": 5.8650199859926843e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.9968006400000000e+08,
      "advised_time": 5.4681601524353027e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 1.2500000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.9968006400000000e+08,
      "workspace_megabytes": 4.7653204345703125e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 125,
      "real_time": 5.6080550216138363e+06,
      "cpu_time": 6.0601789119918980e+06,
      "time_unit": "ns",
      "items_per_second": 5.8635914293396362e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.9968006400000000e+08,
      "advised_time": 5.4261760711669922e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 1.2500000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.9968006400000000e+08,
      "workspace_megabytes": 4.7653204345703125e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 125,
      "real_time": 5.6082969568669796e+06,
      "cpu_time": 6.0602018959998535e+06,
      "time_unit": "ns",
      "items_per_second": 5.8633384809869202e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.9968006400000000e+08,
      "advised_time": 5.4630398750305176e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 6.4000000000000000e+01,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 6.4000000000000000e+01,
      "input_height": 5.6000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 1.2500000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 5.6000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 5.6000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.9968006400000000e+08,
      "workspace_megabytes": 4.7653204345703125e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__8896234610552225653<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:64/input[2]:56/input[3]:56/filter_count:64/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 75,
      "real_time": 9.3899025271336231e+06,
      "cpu_time": 1.0645876213346431e+07,
      "time_unit": "ns",
      "items_per_second": 1.1206367525960002e+13,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 9.0961923599243164e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 7.5000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 75,
      "real_time": 9.3893299748500194e+06,
      "cpu_time": 1.0645870853330357e+07,
      "time_unit": "ns",
      "items_per_second": 1.1207050879440506e+13,
      "activation_mode": 3.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 9.0808315277099609e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3; cudnnMathType_t math_type = (cudnnMathType_t)0]": 3.7948729185673933e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 7.5000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 75,
      "real_time": 9.3918882186214123e+06,
      "cpu_time": 1.0647471146664126e+07,
      "time_unit": "ns",
      "items_per_second": 1.1203998205958811e+13,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 9.1023359298706055e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 7.5000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 75,
      "real_time": 9.3900301183263455e+06,
      "cpu_time": 1.0646242119996715e+07,
      "time_unit": "ns",
      "items_per_second": 1.1206215254478367e+13,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 9.1013116836547852e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 7.5000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 75,
      "real_time": 9.3896170829733219e+06,
      "cpu_time": 1.0646760266655898e+07,
      "time_unit": "ns",
      "items_per_second": 1.1206708199295264e+13,
      "activation_mode": 5.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 9.1044797897338867e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.2315062669022114e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 7.5000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 102,
      "real_time": 6.8434748670780193e+06,
      "cpu_time": 7.5182466666551502e+06,
      "time_unit": "ns",
      "items_per_second": 1.5376208840660064e+13,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.7120000000000000e+03,
      "advised_time": 6.8802561759948730e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 1.0200000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 4.7120000000000000e+03,
      "workspace_megabytes": 4.4937133789062500e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 102,
      "real_time": 6.8443924891671129e+06,
      "cpu_time": 7.5199950980315106e+06,
      "time_unit": "ns",
      "items_per_second": 1.5374147364948227e+13,
      "activation_mode": 3.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.7120000000000000e+03,
      "advised_time": 6.8638720512390137e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.3805875302202324e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 1.0200000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 4.7120000000000000e+03,
      "workspace_megabytes": 4.4937133789062500e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 102,
      "real_time": 6.8425672247494552e+06,
      "cpu_time": 7.5180165588257490e+06,
      "time_unit": "ns",
      "items_per_second": 1.5378248440350973e+13,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.7120000000000000e+03,
      "advised_time": 6.8707518577575684e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 1.0200000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 4.7120000000000000e+03,
      "workspace_megabytes": 4.4937133789062500e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 102,
      "real_time": 6.8437646864456870e+06,
      "cpu_time": 7.5190847254923619e+06,
      "time_unit": "ns",
      "items_per_second": 1.5375557689820213e+13,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.7120000000000000e+03,
      "advised_time": 6.8503360748291016e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 1.0200000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 4.7120000000000000e+03,
      "workspace_megabytes": 4.4937133789062500e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 102,
      "real_time": 6.8438205338430172e+06,
      "cpu_time": 7.5196007843153300e+06,
      "time_unit": "ns",
      "items_per_second": 1.5375432221176605e+13,
      "activation_mode": 5.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.7120000000000000e+03,
      "advised_time": 6.8771839141845703e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 8.1554158113550264e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 1.0200000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 4.7120000000000000e+03,
      "workspace_megabytes": 4.4937133789062500e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 62,
      "real_time": 1.1356534691707741e+07,
      "cpu_time": 1.3192947467743006e+07,
      "time_unit": "ns",
      "items_per_second": 9.2657400878486211e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.0552089600000000e+08,
      "advised_time": 1.1212672233581543e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 6.2000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 2.0552089600000000e+08,
      "workspace_megabytes": 1.9600000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 62,
      "real_time": 1.1360436572783416e+07,
      "cpu_time": 1.3192454580636451e+07,
      "time_unit": "ns",
      "items_per_second": 9.2625576559350879e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.0552089600000000e+08,
      "advised_time": 1.1196415901184082e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 6.2000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 2.0552089600000000e+08,
      "workspace_megabytes": 1.9600000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 62,
      "real_time": 1.1360062481535058e+07,
      "cpu_time": 1.3194951790335234e+07,
      "time_unit": "ns",
      "items_per_second": 9.2628626755388203e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.0552089600000000e+08,
      "advised_time": 1.1247615814208984e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 2.5600000000000000e+02,
      "input[2]": 5.6000000000000000e+01,
      "input[3]": 5.6000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 2.5600000000000000e+02,
      "input_height": 5.6000000000000000e+01,
      "input_size": 2.0552089600000000e+08,
      "input_width": 5.6000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 6.2000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 2.0552089600000000e+08,
      "workspace_megabytes": 1.9600000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__10171953429566747<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:256/input[2]:56/input[3]:56/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 263,
      "real_time": 2.6682762087397138e+06,
      "cpu_time": 2.7744771977222278e+06,
      "time_unit": "ns",
      "items_per_second": 9.8590522981971309e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 2.5958399772644043e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 2.6300000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 263,
      "real_time": 2.6665260535470094e+06,
      "cpu_time": 2.7722058935337081e+06,
      "time_unit": "ns",
      "items_per_second": 9.8655232162486836e+12,
      "activation_mode": 3.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 2.6327040195465088e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3; cudnnMathType_t math_type = (cudnnMathType_t)0]": 3.7948729185673933e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 2.6300000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 263,
      "real_time": 2.6621182064947179e+06,
      "cpu_time": 2.7693698403072110e+06,
      "time_unit": "ns",
      "items_per_second": 9.8818582224561328e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 2.6296319961547852e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 2.6300000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 263,
      "real_time": 2.6628466662968518e+06,
      "cpu_time": 2.7690979125460917e+06,
      "time_unit": "ns",
      "items_per_second": 9.8791548987625234e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 2.6220800876617432e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 2.6300000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 263,
      "real_time": 2.6619200065321228e+06,
      "cpu_time": 2.7684806615903107e+06,
      "time_unit": "ns",
      "items_per_second": 9.8825940011141133e+12,
      "activation_mode": 5.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 2.5466880798339844e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.2315062669022114e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 2.6300000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 407,
      "real_time": 1.7192249271574747e+06,
      "cpu_time": 1.7662619090917038e+06,
      "time_unit": "ns",
      "items_per_second": 1.5301473514285779e+13,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.1840000000000000e+03,
      "advised_time": 1.7795200347900391e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 4.0700000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 1.1840000000000000e+03,
      "workspace_megabytes": 1.1291503906250000e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 407,
      "real_time": 1.7204012221791747e+06,
      "cpu_time": 1.7679292678148262e+06,
      "time_unit": "ns",
      "items_per_second": 1.5291011392492627e+13,
      "activation_mode": 3.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.1840000000000000e+03,
      "advised_time": 1.7981439828872681e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.3805875302202324e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 4.0700000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 1.1840000000000000e+03,
      "workspace_megabytes": 1.1291503906250000e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 408,
      "real_time": 1.7191970201012879e+06,
      "cpu_time": 1.7662712524542278e+06,
      "time_unit": "ns",
      "items_per_second": 1.5301721897150637e+13,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.1840000000000000e+03,
      "advised_time": 1.7582080364227295e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 4.0800000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 1.1840000000000000e+03,
      "workspace_megabytes": 1.1291503906250000e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 407,
      "real_time": 1.7201768257826820e+06,
      "cpu_time": 1.7675793783807268e+06,
      "time_unit": "ns",
      "items_per_second": 1.5293006098969178e+13,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.1840000000000000e+03,
      "advised_time": 1.7582080364227295e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 4.0700000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 1.1840000000000000e+03,
      "workspace_megabytes": 1.1291503906250000e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 407,
      "real_time": 1.7191998475215212e+06,
      "cpu_time": 1.7664251081095396e+06,
      "time_unit": "ns",
      "items_per_second": 1.5301696731723732e+13,
      "activation_mode": 5.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.1840000000000000e+03,
      "advised_time": 1.7223680019378662e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 8.1554158113550264e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 4.0700000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 1.1840000000000000e+03,
      "workspace_megabytes": 1.1291503906250000e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 216,
      "real_time": 3.2380157143860641e+06,
      "cpu_time": 3.3927311851816406e+06,
      "time_unit": "ns",
      "items_per_second": 8.1243196477160430e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.0276044800000000e+08,
      "advised_time": 3.4240961074829102e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 2.1600000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 1.0276044800000000e+08,
      "workspace_megabytes": 9.8000000000000000e+01
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 216,
      "real_time": 3.2373736299902061e+06,
      "cpu_time": 3.3921477962927064e+06,
      "time_unit": "ns",
      "items_per_second": 8.1259309843947744e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.0276044800000000e+08,
      "advised_time": 3.3451519012451172e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 2.1600000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 1.0276044800000000e+08,
      "workspace_megabytes": 9.8000000000000000e+01
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 216,
      "real_time": 3.2383911085263309e+06,
      "cpu_time": 3.3930249907406829e+06,
      "time_unit": "ns",
      "items_per_second": 8.1233778769764385e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.0276044800000000e+08,
      "advised_time": 3.3464319705963135e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 2.1600000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 1.0276044800000000e+08,
      "workspace_megabytes": 9.8000000000000000e+01
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__14289716976641025511<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 73,
      "real_time": 9.5319561405132897e+06,
      "cpu_time": 1.0839924547933886e+07,
      "time_unit": "ns",
      "items_per_second": 1.1039360358023387e+13,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 9.3153276443481445e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.0480000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.0480000000000000e+03,
      "num_iterations": 7.3000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 73,
      "real_time": 9.5406882337307278e+06,
      "cpu_time": 1.0845336410957066e+07,
      "time_unit": "ns",
      "items_per_second": 1.1029256608551062e+13,
      "activation_mode": 3.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 9.3736963272094727e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3; cudnnMathType_t math_type = (cudnnMathType_t)0]": 3.7948729185673933e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.0480000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.0480000000000000e+03,
      "num_iterations": 7.3000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 74,
      "real_time": 9.5415347180253752e+06,
      "cpu_time": 1.0836529621622356e+07,
      "time_unit": "ns",
      "items_per_second": 1.1028278139910883e+13,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 9.3777923583984375e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.0480000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.0480000000000000e+03,
      "num_iterations": 7.4000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 74,
      "real_time": 9.5468881993076280e+06,
      "cpu_time": 1.0837845567558311e+07,
      "time_unit": "ns",
      "items_per_second": 1.1022093959331312e+13,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 9.3941116333007812e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.0480000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.0480000000000000e+03,
      "num_iterations": 7.4000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 73,
      "real_time": 9.5392903950932901e+06,
      "cpu_time": 1.0848328849312428e+07,
      "time_unit": "ns",
      "items_per_second": 1.1030872779187568e+13,
      "activation_mode": 5.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 9.3990716934204102e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.2315062669022114e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.0480000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.0480000000000000e+03,
      "num_iterations": 7.3000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 99,
      "real_time": 7.0941249247301705e+06,
      "cpu_time": 7.8148796363676051e+06,
      "time_unit": "ns",
      "items_per_second": 1.4832935685299107e+13,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.0000000000000000e+02,
      "advised_time": 7.1299200057983398e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.0480000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.0480000000000000e+03,
      "num_iterations": 9.9000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 3.0000000000000000e+02,
      "workspace_megabytes": 2.8610229492187500e-04
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 99,
      "real_time": 7.0898637476593554e+06,
      "cpu_time": 7.8106747878727494e+06,
      "time_unit": "ns",
      "items_per_second": 1.4841850633129514e+13,
      "activation_mode": 3.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.0000000000000000e+02,
      "advised_time": 7.1526398658752441e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.3805875302202324e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.0480000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.0480000000000000e+03,
      "num_iterations": 9.9000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 3.0000000000000000e+02,
      "workspace_megabytes": 2.8610229492187500e-04
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 99,
      "real_time": 7.0917979541300526e+06,
      "cpu_time": 7.8127752020335346e+06,
      "time_unit": "ns",
      "items_per_second": 1.4837802688769369e+13,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.0000000000000000e+02,
      "advised_time": 7.0533761978149414e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.0480000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.0480000000000000e+03,
      "num_iterations": 9.9000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 3.0000000000000000e+02,
      "workspace_megabytes": 2.8610229492187500e-04
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 99,
      "real_time": 7.0910496740705436e+06,
      "cpu_time": 7.8117924646462733e+06,
      "time_unit": "ns",
      "items_per_second": 1.4839368441709943e+13,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.0000000000000000e+02,
      "advised_time": 7.1369280815124512e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.0480000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.0480000000000000e+03,
      "num_iterations": 9.9000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 3.0000000000000000e+02,
      "workspace_megabytes": 2.8610229492187500e-04
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 99,
      "real_time": 7.0925119963258207e+06,
      "cpu_time": 7.8128838787841853e+06,
      "time_unit": "ns",
      "items_per_second": 1.4836308885555818e+13,
      "activation_mode": 5.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.0000000000000000e+02,
      "advised_time": 7.1114239692687988e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 8.1554158113550264e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.0480000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.0480000000000000e+03,
      "num_iterations": 9.9000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 3.0000000000000000e+02,
      "workspace_megabytes": 2.8610229492187500e-04
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 91,
      "real_time": 7.7301802185292430e+06,
      "cpu_time": 8.5838081538490728e+06,
      "time_unit": "ns",
      "items_per_second": 1.3612450910235131e+13,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 5.1380224000000000e+07,
      "advised_time": 7.6062722206115723e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.0480000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.0480000000000000e+03,
      "num_iterations": 9.1000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 5.1380224000000000e+07,
      "workspace_megabytes": 4.9000000000000000e+01
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 91,
      "real_time": 7.7341010609825887e+06,
      "cpu_time": 8.5915435824073013e+06,
      "time_unit": "ns",
      "items_per_second": 1.3605550007983906e+13,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 5.1380224000000000e+07,
      "advised_time": 7.5663361549377441e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.0480000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.0480000000000000e+03,
      "num_iterations": 9.1000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 5.1380224000000000e+07,
      "workspace_megabytes": 4.9000000000000000e+01
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 91,
      "real_time": 7.7328952899264107e+06,
      "cpu_time": 8.5861396153826974e+06,
      "time_unit": "ns",
      "items_per_second": 1.3607671487428271e+13,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 5.1380224000000000e+07,
      "advised_time": 7.5438079833984375e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.0480000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.0480000000000000e+03,
      "num_iterations": 9.1000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.0480000000000000e+03,
      "output_height": 7.0000000000000000e+00,
      "output_size": 2.5690112000000000e+07,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 5.1380224000000000e+07,
      "workspace_megabytes": 4.9000000000000000e+01
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_2_FLOAT32__BatchSize_256__503305952495076182<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:2048/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 152,
      "real_time": 4.6128347381263189e+06,
      "cpu_time": 4.9214809671157487e+06,
      "time_unit": "ns",
      "items_per_second": 2.8514651165115742e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 4.5011839866638184e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 1.5200000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 152,
      "real_time": 4.6109277663115216e+06,
      "cpu_time": 4.9190472499934621e+06,
      "time_unit": "ns",
      "items_per_second": 2.8526444157510449e+12,
      "activation_mode": 3.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 4.5854721069335938e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3; cudnnMathType_t math_type = (cudnnMathType_t)0]": 3.7948729185673933e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 1.5200000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 152,
      "real_time": 4.6133421097980123e+06,
      "cpu_time": 4.9217159670942426e+06,
      "time_unit": "ns",
      "items_per_second": 2.8511515146610049e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 4.4666881561279297e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 1.5200000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 152,
      "real_time": 4.6160623008434316e+06,
      "cpu_time": 4.9232910197435776e+06,
      "time_unit": "ns",
      "items_per_second": 2.8494713647163438e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 4.5015039443969727e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 1.5200000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 152,
      "real_time": 4.6141671635651663e+06,
      "cpu_time": 4.9226962302623773e+06,
      "time_unit": "ns",
      "items_per_second": 2.8506417036345488e+12,
      "activation_mode": 5.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 4.4482560157775879e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.2315062669022114e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 1.5200000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 231,
      "real_time": 3.0368899118187493e+06,
      "cpu_time": 3.1732484718605671e+06,
      "time_unit": "ns",
      "items_per_second": 4.3311867489205942e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.1840000000000000e+03,
      "advised_time": 3.0760960578918457e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 2.3100000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.1840000000000000e+03,
      "workspace_megabytes": 1.1291503906250000e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 231,
      "real_time": 3.0362411972651365e+06,
      "cpu_time": 3.1724755237993710e+06,
      "time_unit": "ns",
      "items_per_second": 4.3321121378129434e+12,
      "activation_mode": 3.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.1840000000000000e+03,
      "advised_time": 3.0760960578918457e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.3805875302202324e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 2.3100000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.1840000000000000e+03,
      "workspace_megabytes": 1.1291503906250000e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 231,
      "real_time": 3.0363869260657919e+06,
      "cpu_time": 3.1726862467637435e+06,
      "time_unit": "ns",
      "items_per_second": 4.3319042217859282e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.1840000000000000e+03,
      "advised_time": 3.0750720500946045e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 2.3100000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.1840000000000000e+03,
      "workspace_megabytes": 1.1291503906250000e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 231,
      "real_time": 3.0365876532945926e+06,
      "cpu_time": 3.1728589653492281e+06,
      "time_unit": "ns",
      "items_per_second": 4.3316178703845693e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.1840000000000000e+03,
      "advised_time": 3.0792000293731689e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 2.3100000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.1840000000000000e+03,
      "workspace_megabytes": 1.1291503906250000e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 231,
      "real_time": 3.0363698901197352e+06,
      "cpu_time": 3.1726434588738484e+06,
      "time_unit": "ns",
      "items_per_second": 4.3319285264949438e+12,
      "activation_mode": 5.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.1840000000000000e+03,
      "advised_time": 3.0740480422973633e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 8.1554158113550264e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 2.3100000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.1840000000000000e+03,
      "workspace_megabytes": 1.1291503906250000e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 140,
      "real_time": 4.9955940306452764e+06,
      "cpu_time": 5.3579442785787378e+06,
      "time_unit": "ns",
      "items_per_second": 2.6329876413718496e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.0552089600000000e+08,
      "advised_time": 4.9397759437561035e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 1.4000000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 2.0552089600000000e+08,
      "workspace_megabytes": 1.9600000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 100,
      "real_time": 5.0013011088594794e+06,
      "cpu_time": 5.5035297399876975e+06,
      "time_unit": "ns",
      "items_per_second": 2.6299830899402793e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.0552089600000000e+08,
      "advised_time": 4.9477119445800781e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 1.0000000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 2.0552089600000000e+08,
      "workspace_megabytes": 1.9600000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 140,
      "real_time": 4.9911608952762829e+06,
      "cpu_time": 5.3521819928537952e+06,
      "time_unit": "ns",
      "items_per_second": 2.6353262537476074e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 2.0552089600000000e+08,
      "advised_time": 4.8783359527587891e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 1.4000000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 2.0552089600000000e+08,
      "workspace_megabytes": 1.9600000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 68,
      "real_time": 1.0270115755060140e+07,
      "cpu_time": 1.1784537485301800e+07,
      "time_unit": "ns",
      "items_per_second": 1.2807389573500457e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.2090081280000000e+09,
      "advised_time": 1.0277695655822754e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.5608866559154586e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 6.8000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.2090081280000000e+09,
      "workspace_megabytes": 1.1530000000000000e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 68,
      "real_time": 1.0269211276489146e+07,
      "cpu_time": 1.1783974220564045e+07,
      "time_unit": "ns",
      "items_per_second": 1.2808517606521465e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.2090081280000000e+09,
      "advised_time": 1.0265536308288574e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.5608866559154586e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 6.8000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.2090081280000000e+09,
      "workspace_megabytes": 1.1530000000000000e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 68,
      "real_time": 1.0251637644079678e+07,
      "cpu_time": 1.1766451426477514e+07,
      "time_unit": "ns",
      "items_per_second": 1.2830474311190713e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.2090081280000000e+09,
      "advised_time": 1.0228608131408691e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.5608866559154586e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 6.8000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.2090081280000000e+09,
      "workspace_megabytes": 1.1530000000000000e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 41,
      "real_time": 1.7048319301954132e+07,
      "cpu_time": 2.1205005585382357e+07,
      "time_unit": "ns",
      "items_per_second": 7.7153278930506213e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 6.5955472000000000e+08,
      "advised_time": 1.6795648574829102e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 4.1000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 6.5955472000000000e+08,
      "workspace_megabytes": 6.2900039672851562e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 41,
      "real_time": 1.7050991698009212e+07,
      "cpu_time": 2.1207734390228745e+07,
      "time_unit": "ns",
      "items_per_second": 7.7141186723677295e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 6.5955472000000000e+08,
      "advised_time": 1.6823295593261719e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 4.1000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 6.5955472000000000e+08,
      "workspace_megabytes": 6.2900039672851562e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 41,
      "real_time": 1.7049030150945593e+07,
      "cpu_time": 2.1205905414634377e+07,
      "time_unit": "ns",
      "items_per_second": 7.7150062071246167e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 6.5955472000000000e+08,
      "advised_time": 1.6815103530883789e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 2.5600000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 2.5600000000000000e+02,
      "num_iterations": 4.1000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 2.5600000000000000e+02,
      "output_height": 1.4000000000000000e+01,
      "output_size": 1.2845056000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 6.5955472000000000e+08,
      "workspace_megabytes": 6.2900039672851562e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12437968246862863965<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:256/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 132,
      "real_time": 5.3090441188859669e+06,
      "cpu_time": 5.7160951591075705e+06,
      "time_unit": "ns",
      "items_per_second": 2.4775340060199111e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 5.0882558822631836e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 1.3200000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 132,
      "real_time": 5.3103944519534707e+06,
      "cpu_time": 5.7173530757622886e+06,
      "time_unit": "ns",
      "items_per_second": 2.4769040158893359e+12,
      "activation_mode": 3.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 5.1025919914245605e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3; cudnnMathType_t math_type = (cudnnMathType_t)0]": 3.7948729185673933e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 1.3200000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 132,
      "real_time": 5.3101713825581651e+06,
      "cpu_time": 5.7171006060602022e+06,
      "time_unit": "ns",
      "items_per_second": 2.4770080655407026e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 5.0739197731018066e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 1.3200000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 132,
      "real_time": 5.3085742983967066e+06,
      "cpu_time": 5.7154049621179625e+06,
      "time_unit": "ns",
      "items_per_second": 2.4777532732230132e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 5.0841598510742188e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 1.3200000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 132,
      "real_time": 5.3089199778200546e+06,
      "cpu_time": 5.7154882348327469e+06,
      "time_unit": "ns",
      "items_per_second": 2.4775919394062920e+12,
      "activation_mode": 5.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 5.0840320587158203e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.2315062669022114e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 1.3200000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 189,
      "real_time": 3.7037714670298905e+06,
      "cpu_time": 3.9045292698367564e+06,
      "time_unit": "ns",
      "items_per_second": 3.5513361072862998e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.7120000000000000e+03,
      "advised_time": 3.7396481037139893e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 1.8900000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.7120000000000000e+03,
      "workspace_megabytes": 4.4937133789062500e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 189,
      "real_time": 3.7028575094328038e+06,
      "cpu_time": 3.9035783015955165e+06,
      "time_unit": "ns",
      "items_per_second": 3.5522126656217998e+12,
      "activation_mode": 3.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.7120000000000000e+03,
      "advised_time": 3.7386240959167480e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.3805875302202324e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 1.8900000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.7120000000000000e+03,
      "workspace_megabytes": 4.4937133789062500e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 189,
      "real_time": 3.7039050637749294e+06,
      "cpu_time": 3.9046484497253271e+06,
      "time_unit": "ns",
      "items_per_second": 3.5512080135753911e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.7120000000000000e+03,
      "advised_time": 3.7437438964843750e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 1.8900000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.7120000000000000e+03,
      "workspace_megabytes": 4.4937133789062500e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 189,
      "real_time": 3.7024921193107883e+06,
      "cpu_time": 3.9031619417939857e+06,
      "time_unit": "ns",
      "items_per_second": 3.5525632250227363e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.7120000000000000e+03,
      "advised_time": 3.7416958808898926e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 1.8900000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.7120000000000000e+03,
      "workspace_megabytes": 4.4937133789062500e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 189,
      "real_time": 3.7031482175859823e+06,
      "cpu_time": 3.9038432486866144e+06,
      "time_unit": "ns",
      "items_per_second": 3.5519338063585342e+12,
      "activation_mode": 5.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.7120000000000000e+03,
      "advised_time": 3.7181119918823242e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 8.1554158113550264e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 1.8900000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.7120000000000000e+03,
      "workspace_megabytes": 4.4937133789062500e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 125,
      "real_time": 5.5984355881810188e+06,
      "cpu_time": 6.0514397679889956e+06,
      "time_unit": "ns",
      "items_per_second": 2.3494665852311138e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.0276044800000000e+08,
      "advised_time": 5.8490881919860840e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 1.2500000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.0276044800000000e+08,
      "workspace_megabytes": 9.8000000000000000e+01
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 125,
      "real_time": 5.6004705540835857e+06,
      "cpu_time": 6.0532304079897581e+06,
      "time_unit": "ns",
      "items_per_second": 2.3486128918951709e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.0276044800000000e+08,
      "advised_time": 5.8142399787902832e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 1.2500000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.0276044800000000e+08,
      "workspace_megabytes": 9.8000000000000000e+01
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 125,
      "real_time": 5.6037465743720531e+06,
      "cpu_time": 6.0565657279985314e+06,
      "time_unit": "ns",
      "items_per_second": 2.3472398634433145e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.0276044800000000e+08,
      "advised_time": 5.8979840278625488e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 1.2500000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.0276044800000000e+08,
      "workspace_megabytes": 9.8000000000000000e+01
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 59,
      "real_time": 1.1814287784746138e+07,
      "cpu_time": 1.3821111762724085e+07,
      "time_unit": "ns",
      "items_per_second": 1.1133415389612192e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.1411128320000000e+09,
      "advised_time": 1.1827199935913086e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.5608866559154586e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 5.9000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.1411128320000000e+09,
      "workspace_megabytes": 1.0882500000000000e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 59,
      "real_time": 1.1815976635631869e+07,
      "cpu_time": 1.3822661559334708e+07,
      "time_unit": "ns",
      "items_per_second": 1.1131824096820935e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.1411128320000000e+09,
      "advised_time": 1.1837439537048340e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.5608866559154586e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 5.9000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.1411128320000000e+09,
      "workspace_megabytes": 1.0882500000000000e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 59,
      "real_time": 1.1815843851131909e+07,
      "cpu_time": 1.3822913508458780e+07,
      "time_unit": "ns",
      "items_per_second": 1.1131949194419971e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.1411128320000000e+09,
      "advised_time": 1.1845567703247070e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.5608866559154586e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 5.9000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.1411128320000000e+09,
      "workspace_megabytes": 1.0882500000000000e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 60,
      "real_time": 1.1576336529105902e+07,
      "cpu_time": 1.3507818999990679e+07,
      "time_unit": "ns",
      "items_per_second": 1.1362262414304482e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 6.3281603200000000e+08,
      "advised_time": 1.0459136009216309e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 6.0000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 6.3281603200000000e+08,
      "workspace_megabytes": 6.0350039672851562e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 60,
      "real_time": 1.1575291771441698e+07,
      "cpu_time": 1.3506608733321931e+07,
      "time_unit": "ns",
      "items_per_second": 1.1363287944457366e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 6.3281603200000000e+08,
      "advised_time": 1.0458111763000488e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 6.0000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 6.3281603200000000e+08,
      "workspace_megabytes": 6.0350039672851562e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 60,
      "real_time": 1.1578872039293250e+07,
      "cpu_time": 1.3510861416655947e+07,
      "time_unit": "ns",
      "items_per_second": 1.1359774336708926e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 6.3281603200000000e+08,
      "advised_time": 1.0460127830505371e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 6.0000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 1.0276044800000000e+08,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 6.3281603200000000e+08,
      "workspace_megabytes": 6.0350039672851562e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__12924237800495388912<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 37,
      "real_time": 1.9364125472871032e+07,
      "cpu_time": 2.4598808945944943e+07,
      "time_unit": "ns",
      "items_per_second": 1.2736184525633214e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 1.5984607696533203e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 7.0000000000000000e+00,
      "filter_width": 7.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 3.0000000000000000e+00,
      "input[2]": 2.2400000000000000e+02,
      "input[3]": 2.2400000000000000e+02,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 3.0000000000000000e+00,
      "input_height": 2.2400000000000000e+02,
      "input_size": 3.8535168000000000e+07,
      "input_width": 2.2400000000000000e+02,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 3.7000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 1.1200000000000000e+02,
      "output_size": 2.0552089600000000e+08,
      "output_width": 1.1200000000000000e+02,
      "pad_height": 3.0000000000000000e+00,
      "pad_width": 3.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 36,
      "real_time": 1.9461066513839696e+07,
      "cpu_time": 2.4867663361103751e+07,
      "time_unit": "ns",
      "items_per_second": 1.2672742011575424e+11,
      "activation_mode": 3.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 1.5976448059082031e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3; cudnnMathType_t math_type = (cudnnMathType_t)0]": 3.7948729185673933e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 7.0000000000000000e+00,
      "filter_width": 7.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 3.0000000000000000e+00,
      "input[2]": 2.2400000000000000e+02,
      "input[3]": 2.2400000000000000e+02,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 3.0000000000000000e+00,
      "input_height": 2.2400000000000000e+02,
      "input_size": 3.8535168000000000e+07,
      "input_width": 2.2400000000000000e+02,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 3.6000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 1.1200000000000000e+02,
      "output_size": 2.0552089600000000e+08,
      "output_width": 1.1200000000000000e+02,
      "pad_height": 3.0000000000000000e+00,
      "pad_width": 3.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 37,
      "real_time": 1.9464825103814539e+07,
      "cpu_time": 2.4728348594604738e+07,
      "time_unit": "ns",
      "items_per_second": 1.2670294949203972e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 1.5983424186706543e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 7.0000000000000000e+00,
      "filter_width": 7.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 3.0000000000000000e+00,
      "input[2]": 2.2400000000000000e+02,
      "input[3]": 2.2400000000000000e+02,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 3.0000000000000000e+00,
      "input_height": 2.2400000000000000e+02,
      "input_size": 3.8535168000000000e+07,
      "input_width": 2.2400000000000000e+02,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 3.7000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 1.1200000000000000e+02,
      "output_size": 2.0552089600000000e+08,
      "output_width": 1.1200000000000000e+02,
      "pad_height": 3.0000000000000000e+00,
      "pad_width": 3.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 36,
      "real_time": 1.9249440946926672e+07,
      "cpu_time": 2.4602146916663513e+07,
      "time_unit": "ns",
      "items_per_second": 1.2812064302541507e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 1.5994879722595215e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 7.0000000000000000e+00,
      "filter_width": 7.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 3.0000000000000000e+00,
      "input[2]": 2.2400000000000000e+02,
      "input[3]": 2.2400000000000000e+02,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 3.0000000000000000e+00,
      "input_height": 2.2400000000000000e+02,
      "input_size": 3.8535168000000000e+07,
      "input_width": 2.2400000000000000e+02,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 3.6000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 1.1200000000000000e+02,
      "output_size": 2.0552089600000000e+08,
      "output_width": 1.1200000000000000e+02,
      "pad_height": 3.0000000000000000e+00,
      "pad_width": 3.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 36,
      "real_time": 1.9157765381452110e+07,
      "cpu_time": 2.4483714972215392e+07,
      "time_unit": "ns",
      "items_per_second": 1.2873373814190976e+11,
      "activation_mode": 5.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 1.6002048492431641e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.2315062669022114e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 7.0000000000000000e+00,
      "filter_width": 7.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 3.0000000000000000e+00,
      "input[2]": 2.2400000000000000e+02,
      "input[3]": 2.2400000000000000e+02,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 3.0000000000000000e+00,
      "input_height": 2.2400000000000000e+02,
      "input_size": 3.8535168000000000e+07,
      "input_width": 2.2400000000000000e+02,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 3.6000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 1.1200000000000000e+02,
      "output_size": 2.0552089600000000e+08,
      "output_width": 1.1200000000000000e+02,
      "pad_height": 3.0000000000000000e+00,
      "pad_width": 3.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 74,
      "real_time": 9.4918996548733190e+06,
      "cpu_time": 1.0777770986495655e+07,
      "time_unit": "ns",
      "items_per_second": 2.5982688836515256e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 7.5272000000000000e+04,
      "advised_time": 9.5546236038208008e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 7.0000000000000000e+00,
      "filter_width": 7.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 3.0000000000000000e+00,
      "input[2]": 2.2400000000000000e+02,
      "input[3]": 2.2400000000000000e+02,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 3.0000000000000000e+00,
      "input_height": 2.2400000000000000e+02,
      "input_size": 3.8535168000000000e+07,
      "input_width": 2.2400000000000000e+02,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 7.4000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 1.1200000000000000e+02,
      "output_size": 2.0552089600000000e+08,
      "output_width": 1.1200000000000000e+02,
      "pad_height": 3.0000000000000000e+00,
      "pad_width": 3.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 7.5272000000000000e+04,
      "workspace_megabytes": 7.1784973144531250e-02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 74,
      "real_time": 9.4913331464537084e+06,
      "cpu_time": 1.0777115797306603e+07,
      "time_unit": "ns",
      "items_per_second": 2.5984239663123370e+11,
      "activation_mode": 3.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 7.5272000000000000e+04,
      "advised_time": 9.5590400695800781e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.3805875302202324e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 7.0000000000000000e+00,
      "filter_width": 7.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 3.0000000000000000e+00,
      "input[2]": 2.2400000000000000e+02,
      "input[3]": 2.2400000000000000e+02,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 3.0000000000000000e+00,
      "input_height": 2.2400000000000000e+02,
      "input_size": 3.8535168000000000e+07,
      "input_width": 2.2400000000000000e+02,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 7.4000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 1.1200000000000000e+02,
      "output_size": 2.0552089600000000e+08,
      "output_width": 1.1200000000000000e+02,
      "pad_height": 3.0000000000000000e+00,
      "pad_width": 3.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 7.5272000000000000e+04,
      "workspace_megabytes": 7.1784973144531250e-02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 74,
      "real_time": 9.4921314535108767e+06,
      "cpu_time": 1.0777878635136599e+07,
      "time_unit": "ns",
      "items_per_second": 2.5982054337098355e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 7.5272000000000000e+04,
      "advised_time": 9.5569915771484375e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 7.0000000000000000e+00,
      "filter_width": 7.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 3.0000000000000000e+00,
      "input[2]": 2.2400000000000000e+02,
      "input[3]": 2.2400000000000000e+02,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 3.0000000000000000e+00,
      "input_height": 2.2400000000000000e+02,
      "input_size": 3.8535168000000000e+07,
      "input_width": 2.2400000000000000e+02,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 7.4000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 1.1200000000000000e+02,
      "output_size": 2.0552089600000000e+08,
      "output_width": 1.1200000000000000e+02,
      "pad_height": 3.0000000000000000e+00,
      "pad_width": 3.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 7.5272000000000000e+04,
      "workspace_megabytes": 7.1784973144531250e-02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 74,
      "real_time": 9.4908293512826022e+06,
      "cpu_time": 1.0776449067570005e+07,
      "time_unit": "ns",
      "items_per_second": 2.5985618966657614e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 7.5272000000000000e+04,
      "advised_time": 9.5610876083374023e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 7.0000000000000000e+00,
      "filter_width": 7.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 3.0000000000000000e+00,
      "input[2]": 2.2400000000000000e+02,
      "input[3]": 2.2400000000000000e+02,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 3.0000000000000000e+00,
      "input_height": 2.2400000000000000e+02,
      "input_size": 3.8535168000000000e+07,
      "input_width": 2.2400000000000000e+02,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 7.4000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 1.1200000000000000e+02,
      "output_size": 2.0552089600000000e+08,
      "output_width": 1.1200000000000000e+02,
      "pad_height": 3.0000000000000000e+00,
      "pad_width": 3.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 7.5272000000000000e+04,
      "workspace_megabytes": 7.1784973144531250e-02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 74,
      "real_time": 9.4935977957337294e+06,
      "cpu_time": 1.0779283702710336e+07,
      "time_unit": "ns",
      "items_per_second": 2.5978041255426828e+11,
      "activation_mode": 5.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 7.5272000000000000e+04,
      "advised_time": 9.5252799987792969e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 8.1554158113550264e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 7.0000000000000000e+00,
      "filter_width": 7.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 3.0000000000000000e+00,
      "input[2]": 2.2400000000000000e+02,
      "input[3]": 2.2400000000000000e+02,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 3.0000000000000000e+00,
      "input_height": 2.2400000000000000e+02,
      "input_size": 3.8535168000000000e+07,
      "input_width": 2.2400000000000000e+02,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 7.4000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 1.1200000000000000e+02,
      "output_size": 2.0552089600000000e+08,
      "output_width": 1.1200000000000000e+02,
      "pad_height": 3.0000000000000000e+00,
      "pad_width": 3.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 7.5272000000000000e+04,
      "workspace_megabytes": 7.1784973144531250e-02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 27,
      "real_time": 2.6055196231162105e+07,
      "cpu_time": 3.5701500074116699e+07,
      "time_unit": "ns",
      "items_per_second": 9.4654852341904663e+10,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.8882232320000000e+09,
      "advised_time": 1.8908159255981445e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 7.0000000000000000e+00,
      "filter_width": 7.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 3.0000000000000000e+00,
      "input[2]": 2.2400000000000000e+02,
      "input[3]": 2.2400000000000000e+02,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 3.0000000000000000e+00,
      "input_height": 2.2400000000000000e+02,
      "input_size": 3.8535168000000000e+07,
      "input_width": 2.2400000000000000e+02,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 2.7000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 1.1200000000000000e+02,
      "output_size": 2.0552089600000000e+08,
      "output_width": 1.1200000000000000e+02,
      "pad_height": 3.0000000000000000e+00,
      "pad_width": 3.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 1.8882232320000000e+09,
      "workspace_megabytes": 1.8007500000000000e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 27,
      "real_time": 2.6026893062172111e+07,
      "cpu_time": 3.5678379740737133e+07,
      "time_unit": "ns",
      "items_per_second": 9.4757785576200287e+10,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.8882232320000000e+09,
      "advised_time": 1.8897567749023438e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 7.0000000000000000e+00,
      "filter_width": 7.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 3.0000000000000000e+00,
      "input[2]": 2.2400000000000000e+02,
      "input[3]": 2.2400000000000000e+02,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 3.0000000000000000e+00,
      "input_height": 2.2400000000000000e+02,
      "input_size": 3.8535168000000000e+07,
      "input_width": 2.2400000000000000e+02,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 2.7000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 1.1200000000000000e+02,
      "output_size": 2.0552089600000000e+08,
      "output_width": 1.1200000000000000e+02,
      "pad_height": 3.0000000000000000e+00,
      "pad_width": 3.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 1.8882232320000000e+09,
      "workspace_megabytes": 1.8007500000000000e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 27,
      "real_time": 2.6048759688381795e+07,
      "cpu_time": 3.5700466296264969e+07,
      "time_unit": "ns",
      "items_per_second": 9.4678241171689682e+10,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.8882232320000000e+09,
      "advised_time": 1.8912031173706055e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 7.0000000000000000e+00,
      "filter_width": 7.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 3.0000000000000000e+00,
      "input[2]": 2.2400000000000000e+02,
      "input[3]": 2.2400000000000000e+02,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 3.0000000000000000e+00,
      "input_height": 2.2400000000000000e+02,
      "input_size": 3.8535168000000000e+07,
      "input_width": 2.2400000000000000e+02,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 2.7000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 1.1200000000000000e+02,
      "output_size": 2.0552089600000000e+08,
      "output_width": 1.1200000000000000e+02,
      "pad_height": 3.0000000000000000e+00,
      "pad_width": 3.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 1.8882232320000000e+09,
      "workspace_megabytes": 1.8007500000000000e+03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 15,
      "real_time": 4.6140977243582405e+07,
      "cpu_time": 7.1680059199979961e+07,
      "time_unit": "ns",
      "items_per_second": 5.3450336324270691e+10,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.5017574400000000e+08,
      "advised_time": 4.8273406982421875e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 7.0000000000000000e+00,
      "filter_width": 7.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 3.0000000000000000e+00,
      "input[2]": 2.2400000000000000e+02,
      "input[3]": 2.2400000000000000e+02,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 3.0000000000000000e+00,
      "input_height": 2.2400000000000000e+02,
      "input_size": 3.8535168000000000e+07,
      "input_width": 2.2400000000000000e+02,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 1.5000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 1.1200000000000000e+02,
      "output_size": 2.0552089600000000e+08,
      "output_width": 1.1200000000000000e+02,
      "pad_height": 3.0000000000000000e+00,
      "pad_width": 3.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 1.5017574400000000e+08,
      "workspace_megabytes": 1.4321875000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 15,
      "real_time": 4.6130632609128952e+07,
      "cpu_time": 7.1662543333377466e+07,
      "time_unit": "ns",
      "items_per_second": 5.3462322376908943e+10,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.5017574400000000e+08,
      "advised_time": 4.8241886138916016e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 7.0000000000000000e+00,
      "filter_width": 7.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 3.0000000000000000e+00,
      "input[2]": 2.2400000000000000e+02,
      "input[3]": 2.2400000000000000e+02,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 3.0000000000000000e+00,
      "input_height": 2.2400000000000000e+02,
      "input_size": 3.8535168000000000e+07,
      "input_width": 2.2400000000000000e+02,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 1.5000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 1.1200000000000000e+02,
      "output_size": 2.0552089600000000e+08,
      "output_width": 1.1200000000000000e+02,
      "pad_height": 3.0000000000000000e+00,
      "pad_width": 3.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 1.5017574400000000e+08,
      "workspace_megabytes": 1.4321875000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 15,
      "real_time": 4.6142115940650307e+07,
      "cpu_time": 7.1667964799977809e+07,
      "time_unit": "ns",
      "items_per_second": 5.3449017274634369e+10,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.5017574400000000e+08,
      "advised_time": 4.8448513031005859e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 6.4000000000000000e+01,
      "filter_height": 7.0000000000000000e+00,
      "filter_width": 7.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 3.0000000000000000e+00,
      "input[2]": 2.2400000000000000e+02,
      "input[3]": 2.2400000000000000e+02,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 3.0000000000000000e+00,
      "input_height": 2.2400000000000000e+02,
      "input_size": 3.8535168000000000e+07,
      "input_width": 2.2400000000000000e+02,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 6.4000000000000000e+01,
      "num_iterations": 1.5000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 6.4000000000000000e+01,
      "output_height": 1.1200000000000000e+02,
      "output_size": 2.0552089600000000e+08,
      "output_width": 1.1200000000000000e+02,
      "pad_height": 3.0000000000000000e+00,
      "pad_width": 3.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 1.5017574400000000e+08,
      "workspace_megabytes": 1.4321875000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5617847787106706506<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:3/input[2]:224/input[3]:224/filter_count:64/filter_height:7/filter_width:7/pad_height:3/pad_width:3/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 68,
      "real_time": 1.0235281866591644e+07,
      "cpu_time": 1.1746553500004480e+07,
      "time_unit": "ns",
      "items_per_second": 3.2127442886875940e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 1.0178560256958008e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 6.8000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 68,
      "real_time": 1.0235116269220324e+07,
      "cpu_time": 1.1744250735278804e+07,
      "time_unit": "ns",
      "items_per_second": 3.2127962687525916e+11,
      "activation_mode": 3.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 1.0223263740539551e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3; cudnnMathType_t math_type = (cudnnMathType_t)0]": 3.7948729185673933e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 6.8000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 68,
      "real_time": 1.0233841370791197e+07,
      "cpu_time": 1.1743248897065476e+07,
      "time_unit": "ns",
      "items_per_second": 3.2131965083857587e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 1.0215423583984375e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 6.8000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 68,
      "real_time": 1.0232885642086759e+07,
      "cpu_time": 1.1743901705870030e+07,
      "time_unit": "ns",
      "items_per_second": 3.2134966137757214e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 1.0184703826904297e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 6.8000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 68,
      "real_time": 1.0240552892141482e+07,
      "cpu_time": 1.1751829132335575e+07,
      "time_unit": "ns",
      "items_per_second": 3.2110906223856732e+11,
      "activation_mode": 5.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 1.0243071556091309e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.2315062669022114e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 6.8000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 106,
      "real_time": 6.5759692631029291e+06,
      "cpu_time": 7.2004376603702456e+06,
      "time_unit": "ns",
      "items_per_second": 5.0005317914888947e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.7120000000000000e+03,
      "advised_time": 6.6140799522399902e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 1.0600000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.7120000000000000e+03,
      "workspace_megabytes": 4.4937133789062500e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 106,
      "real_time": 6.5766780435125222e+06,
      "cpu_time": 7.2015284622723833e+06,
      "time_unit": "ns",
      "items_per_second": 4.9999928751928711e+11,
      "activation_mode": 3.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.7120000000000000e+03,
      "advised_time": 6.6262722015380859e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.3805875302202324e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 1.0600000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.7120000000000000e+03,
      "workspace_megabytes": 4.4937133789062500e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 106,
      "real_time": 6.5749050776983770e+06,
      "cpu_time": 7.1995883113381844e+06,
      "time_unit": "ns",
      "items_per_second": 5.0013411557131104e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.7120000000000000e+03,
      "advised_time": 6.5942077636718750e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 1.0600000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.7120000000000000e+03,
      "workspace_megabytes": 4.4937133789062500e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 106,
      "real_time": 6.5773286074751392e+06,
      "cpu_time": 7.2027953679433372e+06,
      "time_unit": "ns",
      "items_per_second": 4.9994983256010736e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.7120000000000000e+03,
      "advised_time": 6.6273279190063477e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 1.0600000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.7120000000000000e+03,
      "workspace_megabytes": 4.4937133789062500e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 106,
      "real_time": 6.5756157119670566e+06,
      "cpu_time": 7.2001124528302206e+06,
      "time_unit": "ns",
      "items_per_second": 5.0008006550862048e+11,
      "activation_mode": 5.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.7120000000000000e+03,
      "advised_time": 6.5884160995483398e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 8.1554158113550264e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 1.0600000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.7120000000000000e+03,
      "workspace_megabytes": 4.4937133789062500e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 58,
      "real_time": 1.2101321352709984e+07,
      "cpu_time": 1.4192448224125186e+07,
      "time_unit": "ns",
      "items_per_second": 2.7173349423231427e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 9.2484403200000000e+08,
      "advised_time": 1.1973631858825684e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 5.8000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 9.2484403200000000e+08,
      "workspace_megabytes": 8.8200000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 58,
      "real_time": 1.2106072912313815e+07,
      "cpu_time": 1.4197415189657696e+07,
      "time_unit": "ns",
      "items_per_second": 2.7162684049715555e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 9.2484403200000000e+08,
      "advised_time": 1.1975680351257324e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 5.8000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 9.2484403200000000e+08,
      "workspace_megabytes": 8.8200000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 58,
      "real_time": 1.2105740141123533e+07,
      "cpu_time": 1.4197015827570450e+07,
      "time_unit": "ns",
      "items_per_second": 2.7163430716883121e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 9.2484403200000000e+08,
      "advised_time": 1.1979776382446289e+01,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 5.8000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 9.2484403200000000e+08,
      "workspace_megabytes": 8.8200000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 198,
      "real_time": 3.5395638034161595e+06,
      "cpu_time": 3.7228557272684532e+06,
      "time_unit": "ns",
      "items_per_second": 9.2902247808792456e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.2840883200000000e+08,
      "advised_time": 3.6679680347442627e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.5608866559154586e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 1.9800000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.2840883200000000e+08,
      "workspace_megabytes": 4.0856250000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 198,
      "real_time": 3.5396124603170338e+06,
      "cpu_time": 3.7230687525246623e+06,
      "time_unit": "ns",
      "items_per_second": 9.2900970738064148e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.2840883200000000e+08,
      "advised_time": 3.6610240936279297e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.5608866559154586e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 1.9800000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.2840883200000000e+08,
      "workspace_megabytes": 4.0856250000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 198,
      "real_time": 3.5394800949202040e+06,
      "cpu_time": 3.7229595757574802e+06,
      "time_unit": "ns",
      "items_per_second": 9.2904444941486072e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.2840883200000000e+08,
      "advised_time": 3.6648960113525391e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)4; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.5608866559154586e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 4.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 1.9800000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.2840883200000000e+08,
      "workspace_megabytes": 4.0856250000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 197,
      "real_time": 3.5490945210860013e+06,
      "cpu_time": 3.7330510304638143e+06,
      "time_unit": "ns",
      "items_per_second": 9.2652768655870850e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.5654041600000000e+08,
      "advised_time": 3.6565439701080322e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 1.9700000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 3.5654041600000000e+08,
      "workspace_megabytes": 3.4002343750000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 197,
      "real_time": 3.5492145612382041e+06,
      "cpu_time": 3.7334120507638622e+06,
      "time_unit": "ns",
      "items_per_second": 9.2649634990024622e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.5654041600000000e+08,
      "advised_time": 3.6618559360504150e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 1.9700000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 3.5654041600000000e+08,
      "workspace_megabytes": 3.4002343750000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 197,
      "real_time": 3.5495837739087301e+06,
      "cpu_time": 3.7336629593794569e+06,
      "time_unit": "ns",
      "items_per_second": 9.2639997967394153e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.5654041600000000e+08,
      "advised_time": 3.6607999801635742e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)5; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4482905588559884e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 5.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 1.9700000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 3.5654041600000000e+08,
      "workspace_megabytes": 3.4002343750000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 171,
      "real_time": 4.0918130738039813e+06,
      "cpu_time": 4.3351371052522790e+06,
      "time_unit": "ns",
      "items_per_second": 8.0363747724745850e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.6389120000000000e+06,
      "advised_time": 4.1564159393310547e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)6; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.1941256922983490e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 6.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 1.7100000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.6389120000000000e+06,
      "workspace_megabytes": 1.5629882812500000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 171,
      "real_time": 4.0921504821693688e+06,
      "cpu_time": 4.3358389532173611e+06,
      "time_unit": "ns",
      "items_per_second": 8.0357121526399915e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.6389120000000000e+06,
      "advised_time": 4.1615362167358398e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)6; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.1941256922983490e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 6.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 1.7100000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.6389120000000000e+06,
      "workspace_megabytes": 1.5629882812500000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 171,
      "real_time": 4.0909702078117961e+06,
      "cpu_time": 4.3346263918005349e+06,
      "time_unit": "ns",
      "items_per_second": 8.0380305134485071e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.6389120000000000e+06,
      "advised_time": 4.1564159393310547e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)6; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.1941256922983490e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 6.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 1.7100000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 1.6389120000000000e+06,
      "workspace_megabytes": 1.5629882812500000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 199,
      "real_time": 3.5156229471123246e+06,
      "cpu_time": 3.6965625879463400e+06,
      "time_unit": "ns",
      "items_per_second": 9.3534897953177368e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.6478131200000000e+08,
      "advised_time": 3.5553278923034668e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)7; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 2.0444073425867530e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 7.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 1.9900000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.6478131200000000e+08,
      "workspace_megabytes": 4.4325000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 199,
      "real_time": 3.5156901602881337e+06,
      "cpu_time": 3.6969256482405574e+06,
      "time_unit": "ns",
      "items_per_second": 9.3533109747376013e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.6478131200000000e+08,
      "advised_time": 3.6063680648803711e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)7; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 2.0444073425867530e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 7.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 1.9900000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.6478131200000000e+08,
      "workspace_megabytes": 4.4325000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 199,
      "real_time": 3.5154344794813115e+06,
      "cpu_time": 3.6966585125639141e+06,
      "time_unit": "ns",
      "items_per_second": 9.3539912497108496e+11,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 6.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 4.6478131200000000e+08,
      "advised_time": 3.5573759078979492e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)7; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 2.0444073425867530e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 7.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.2800000000000000e+02,
      "filter_height": 3.0000000000000000e+00,
      "filter_width": 3.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.2800000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.2800000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 2.5690112000000000e+07,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.2800000000000000e+02,
      "num_iterations": 1.9900000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.2800000000000000e+02,
      "output_height": 2.8000000000000000e+01,
      "output_size": 2.5690112000000000e+07,
      "output_width": 2.8000000000000000e+01,
      "pad_height": 1.0000000000000000e+00,
      "pad_width": 1.0000000000000000e+00,
      "stride_height": 1.0000000000000000e+00,
      "stride_width": 1.0000000000000000e+00,
      "workspace_bytes": 4.6478131200000000e+08,
      "workspace_megabytes": 4.4325000000000000e+02
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__16358280275817588998<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:128/input[2]:28/input[3]:28/filter_count:128/filter_height:3/filter_width:3/pad_height:1/pad_width:1/stride_height:1/stride_width:1/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 72,
      "real_time": 9.6820599218416549e+06,
      "cpu_time": 1.1031171874985073e+07,
      "time_unit": "ns",
      "items_per_second": 1.0868213954617264e+13,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 9.5447044372558594e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.0240000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.0240000000000000e+03,
      "num_iterations": 7.2000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 72,
      "real_time": 9.6908297855407000e+06,
      "cpu_time": 1.1040761097218718e+07,
      "time_unit": "ns",
      "items_per_second": 1.0858378599220115e+13,
      "activation_mode": 3.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 9.4699516296386719e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3; cudnnMathType_t math_type = (cudnnMathType_t)0]": 3.7948729185673933e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.0240000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.0240000000000000e+03,
      "num_iterations": 7.2000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 72,
      "real_time": 9.6787626001363005e+06,
      "cpu_time": 1.1025125999980092e+07,
      "time_unit": "ns",
      "items_per_second": 1.0871916493800371e+13,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 9.4349441528320312e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.0240000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.0240000000000000e+03,
      "num_iterations": 7.2000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 72,
      "real_time": 9.6784986995367538e+06,
      "cpu_time": 1.1026075680547364e+07,
      "time_unit": "ns",
      "items_per_second": 1.0872212934949975e+13,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 9.5047683715820312e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.0240000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.0240000000000000e+03,
      "num_iterations": 7.2000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 72,
      "real_time": 9.6733657669069041e+06,
      "cpu_time": 1.1021148444456229e+07,
      "time_unit": "ns",
      "items_per_second": 1.0877982006220225e+13,
      "activation_mode": 5.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 9.5559682846069336e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.2315062669022114e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.0240000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.0240000000000000e+03,
      "num_iterations": 7.2000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 107,
      "real_time": 6.5377575963317789e+06,
      "cpu_time": 7.1531621401888262e+06,
      "time_unit": "ns",
      "items_per_second": 1.6095227943452791e+13,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.1840000000000000e+03,
      "advised_time": 6.5740799903869629e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.0240000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.0240000000000000e+03,
      "num_iterations": 1.0700000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 1.1840000000000000e+03,
      "workspace_megabytes": 1.1291503906250000e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 107,
      "real_time": 6.5338943049172377e+06,
      "cpu_time": 7.1491748878402896e+06,
      "time_unit": "ns",
      "items_per_second": 1.6104744558357662e+13,
      "activation_mode": 3.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.1840000000000000e+03,
      "advised_time": 6.5727682113647461e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.3805875302202324e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.0240000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.0240000000000000e+03,
      "num_iterations": 1.0700000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 1.1840000000000000e+03,
      "workspace_megabytes": 1.1291503906250000e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 107,
      "real_time": 6.5357515013942095e+06,
      "cpu_time": 7.1506734673008714e+06,
      "time_unit": "ns",
      "items_per_second": 1.6100168240722279e+13,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.1840000000000000e+03,
      "advised_time": 6.5739197731018066e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.0240000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.0240000000000000e+03,
      "num_iterations": 1.0700000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 1.1840000000000000e+03,
      "workspace_megabytes": 1.1291503906250000e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 107,
      "real_time": 6.5354341771627702e+06,
      "cpu_time": 7.1502222242989466e+06,
      "time_unit": "ns",
      "items_per_second": 1.6100949975091342e+13,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.1840000000000000e+03,
      "advised_time": 6.5781760215759277e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.0240000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.0240000000000000e+03,
      "num_iterations": 1.0700000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 1.1840000000000000e+03,
      "workspace_megabytes": 1.1291503906250000e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 107,
      "real_time": 6.5348844792450145e+06,
      "cpu_time": 7.1503014766175142e+06,
      "time_unit": "ns",
      "items_per_second": 1.6102304346190525e+13,
      "activation_mode": 5.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.1840000000000000e+03,
      "advised_time": 6.5719361305236816e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 8.1554158113550264e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.0240000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.0240000000000000e+03,
      "num_iterations": 1.0700000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 1.1840000000000000e+03,
      "workspace_megabytes": 1.1291503906250000e-03
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 81,
      "real_time": 8.5933005644215476e+06,
      "cpu_time": 9.6600502469171286e+06,
      "time_unit": "ns",
      "items_per_second": 1.2245201708370975e+13,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.0276044800000000e+08,
      "advised_time": 8.4234237670898438e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.0240000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.0240000000000000e+03,
      "num_iterations": 8.1000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 1.0276044800000000e+08,
      "workspace_megabytes": 9.8000000000000000e+01
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 81,
      "real_time": 8.6002864035559297e+06,
      "cpu_time": 9.6612678641975671e+06,
      "time_unit": "ns",
      "items_per_second": 1.2235255178070850e+13,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.0276044800000000e+08,
      "advised_time": 8.4183044433593750e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.0240000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.0240000000000000e+03,
      "num_iterations": 8.1000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 1.0276044800000000e+08,
      "workspace_megabytes": 9.8000000000000000e+01
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 82,
      "real_time": 8.5909463374353033e+06,
      "cpu_time": 9.6389164268380348e+06,
      "time_unit": "ns",
      "items_per_second": 1.2248557332208156e+13,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 1.0276044800000000e+08,
      "advised_time": 8.3947515487670898e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 1.0240000000000000e+03,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 5.1200000000000000e+02,
      "input[2]": 2.8000000000000000e+01,
      "input[3]": 2.8000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 5.1200000000000000e+02,
      "input_height": 2.8000000000000000e+01,
      "input_size": 1.0276044800000000e+08,
      "input_width": 2.8000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 1.0240000000000000e+03,
      "num_iterations": 8.2000000000000000e+01,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 1.0240000000000000e+03,
      "output_height": 1.4000000000000000e+01,
      "output_size": 5.1380224000000000e+07,
      "output_width": 1.4000000000000000e+01,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 1.0276044800000000e+08,
      "workspace_megabytes": 9.8000000000000000e+01
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__17931650036999580819<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:512/input[2]:28/input[3]:28/filter_count:1024/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 266,
      "real_time": 2.6330439613683890e+06,
      "cpu_time": 2.7366549548878195e+06,
      "time_unit": "ns",
      "items_per_second": 9.9909743528658965e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 2.5088000297546387e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 2.6600000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 265,
      "real_time": 2.6376658189549763e+06,
      "cpu_time": 2.7417634188683257e+06,
      "time_unit": "ns",
      "items_per_second": 9.9734676390591855e+12,
      "activation_mode": 3.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 2.4893438816070557e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3; cudnnMathType_t math_type = (cudnnMathType_t)0]": 3.7948729185673933e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 2.6500000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 265,
      "real_time": 2.6414973547084713e+06,
      "cpu_time": 2.7463586301907850e+06,
      "time_unit": "ns",
      "items_per_second": 9.9590009587207539e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 2.5118079185485840e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 2.6500000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 266,
      "real_time": 2.6406847514716308e+06,
      "cpu_time": 2.7448544511328763e+06,
      "time_unit": "ns",
      "items_per_second": 9.9620655867155371e+12,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 2.4790079593658447e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4991874974232711e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 2.6600000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 266,
      "real_time": 2.6365234297735239e+06,
      "cpu_time": 2.7407723609085362e+06,
      "time_unit": "ns",
      "items_per_second": 9.9777890804709180e+12,
      "activation_mode": 5.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 0.0000000000000000e+00,
      "advised_time": 2.4780158996582031e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)0; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.2315062669022114e+19,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 0.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 2.6600000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 0.0000000000000000e+00,
      "workspace_megabytes": 0.0000000000000000e+00
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 388,
      "real_time": 1.8061402100186374e+06,
      "cpu_time": 1.8575245592801273e+06,
      "time_unit": "ns",
      "items_per_second": 1.4565134280316225e+13,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.0000000000000000e+02,
      "advised_time": 1.8411519527435303e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 3.8800000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 3.0000000000000000e+02,
      "workspace_megabytes": 2.8610229492187500e-04
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 388,
      "real_time": 1.8060385230352583e+06,
      "cpu_time": 1.8575988917529320e+06,
      "time_unit": "ns",
      "items_per_second": 1.4565954353946207e+13,
      "activation_mode": 3.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.0000000000000000e+02,
      "advised_time": 1.8391040563583374e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)3; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.3805875302202324e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 3.8800000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 3.0000000000000000e+02,
      "workspace_megabytes": 2.8610229492187500e-04
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 388,
      "real_time": 1.8064304346161098e+06,
      "cpu_time": 1.8578907293855532e+06,
      "time_unit": "ns",
      "items_per_second": 1.4562794217752711e+13,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.0000000000000000e+02,
      "advised_time": 1.8483200073242188e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 3.8800000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 3.0000000000000000e+02,
      "workspace_megabytes": 2.8610229492187500e-04
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 387,
      "real_time": 1.8069618628227971e+06,
      "cpu_time": 1.8586756589070847e+06,
      "time_unit": "ns",
      "items_per_second": 1.4558511294147777e+13,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.0000000000000000e+02,
      "advised_time": 1.8434560298919678e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 7.3632461532209244e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 3.8700000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 3.0000000000000000e+02,
      "workspace_megabytes": 2.8610229492187500e-04
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 388,
      "real_time": 1.8071783958579001e+06,
      "cpu_time": 1.8586688634023939e+06,
      "time_unit": "ns",
      "items_per_second": 1.4556766918139119e+13,
      "activation_mode": 5.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 3.0000000000000000e+02,
      "advised_time": 1.8339200019836426e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)1; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)5; cudnnMathType_t math_type = (cudnnMathType_t)0]": 8.1554158113550264e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 1.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 3.8800000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 3.0000000000000000e+02,
      "workspace_megabytes": 2.8610229492187500e-04
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 267,
      "real_time": 2.6115430126573597e+06,
      "cpu_time": 2.7138064082488399e+06,
      "time_unit": "ns",
      "items_per_second": 1.0073230485004268e+13,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 5.1380224000000000e+07,
      "advised_time": 2.4676160812377930e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 2.6700000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 5.1380224000000000e+07,
      "workspace_megabytes": 4.9000000000000000e+01
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 268,
      "real_time": 2.6139800502821356e+06,
      "cpu_time": 2.7170045746205305e+06,
      "time_unit": "ns",
      "items_per_second": 1.0063839119644631e+13,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 5.1380224000000000e+07,
      "advised_time": 2.5056960582733154e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 2.6800000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 5.1380224000000000e+07,
      "workspace_megabytes": 4.9000000000000000e+01
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "iterations": 268,
      "real_time": 2.6204069310090323e+06,
      "cpu_time": 2.7228468544815620e+06,
      "time_unit": "ns",
      "items_per_second": 1.0039156276338410e+13,
      "activation_mode": 1.0000000000000000e+00,
      "advised_convolution_algorithm": 1.0000000000000000e+00,
      "advised_determinism": 1.0000000000000000e+00,
      "advised_memory": 5.1380224000000000e+07,
      "advised_time": 2.5139200687408447e+00,
      "batch_size": 2.5600000000000000e+02,
      "benchmark_file:/home/abduld/scope/scopes/cudnn_scope/src/cudnn_conv_bias_activation_fwd.inc": 3.7241290715759565e+17,
      "benchmark_func:void iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl(benchmark::State&) [with T = float; cudnnConvolutionFwdAlgo_t convolution_algorithm = (cudnnConvolutionFwdAlgo_t)2; cudnnActivationMode_t activation_mode = (cudnnActivationMode_t)1; cudnnMathType_t math_type = (cudnnMathType_t)0]": 1.4433935009319416e+18,
      "compute_capability:6.1": 4.4519655652347223e+18,
      "conv_bwd_type": 0.0000000000000000e+00,
      "conv_fwd_type": 2.0000000000000000e+00,
      "convolution_algorithm": 2.0000000000000000e+00,
      "cublas_version:10.1": 9.6389230149667533e+17,
      "cuda_driver_version:10.10": 3.1909591731558548e+18,
      "cuda_runtime_version:10.10": 3.1909591731558548e+18,
      "cudnn_version:7.6.3": 1.5847091547736381e+19,
      "cupti_enabled": 0.0000000000000000e+00,
      "demangled_benchmark_func:iLAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_Impl": 3.7501726223256182e+18,
      "dilation_height": 1.0000000000000000e+00,
      "dilation_width": 1.0000000000000000e+00,
      "filter_count": 5.1200000000000000e+02,
      "filter_height": 1.0000000000000000e+00,
      "filter_width": 1.0000000000000000e+00,
      "gpu_name:TITAN Xp": 7.2235257218963241e+18,
      "group": 1.0000000000000000e+00,
      "host_name:whatever": 1.2416275329644214e+19,
      "input[0]": 2.5600000000000000e+02,
      "input[1]": 1.0240000000000000e+03,
      "input[2]": 1.4000000000000000e+01,
      "input[3]": 1.4000000000000000e+01,
      "input_batch_size": 2.5600000000000000e+02,
      "input_channels": 1.0240000000000000e+03,
      "input_height": 1.4000000000000000e+01,
      "input_size": 5.1380224000000000e+07,
      "input_width": 1.4000000000000000e+01,
      "math_type": 0.0000000000000000e+00,
      "num_filters": 5.1200000000000000e+02,
      "num_iterations": 2.6800000000000000e+02,
      "output_batch_size": 2.5600000000000000e+02,
      "output_channels": 5.1200000000000000e+02,
      "output_height": 7.0000000000000000e+00,
      "output_size": 6.4225280000000000e+06,
      "output_width": 7.0000000000000000e+00,
      "pad_height": 0.0000000000000000e+00,
      "pad_width": 0.0000000000000000e+00,
      "stride_height": 2.0000000000000000e+00,
      "stride_width": 2.0000000000000000e+00,
      "workspace_bytes": 5.1380224000000000e+07,
      "workspace_megabytes": 4.9000000000000000e+01
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_GEMM, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_DIRECT, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_FFT, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_SIGMOID>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_TANH>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_CLIPPED_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_ELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_RELU>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    },
    {
      "name": "LAYER_CUDNN_CONV_BIAS_ACTIVATION_FWD_4_FLOAT32__BatchSize_256__5537822956401177079<CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED, CUDNN_ACTIVATION_IDENTITY>/input[0]:256/input[1]:1024/input[2]:14/input[3]:14/filter_count:512/filter_height:1/filter_width:1/pad_height:0/pad_width:0/stride_height:2/stride_width:2/dilation_height:1/dilation_width:1/group:1/batch_size:256/conv_fwd_type:2/conv_bwd_type:0/manual_time",
      "error_occurred": true,
      "error_message": "CUDNN/CONV_BIAS_ACTIVATION_FWD failed to launch kernel because of CUDNN_STATUS_NOT_SUPPORTED",
      "iterations": 1,
      "real_time": 0.0000000000000000e+00,
      "cpu_time": 0.0000000000000000e+00,
      "time_unit": "ns"
    }
  ]
}
